{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "class SequentialRecommendationDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        self.max_seq_length = 0\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                user_id = int(parts[0])\n",
    "                sequence = list(map(int, parts[1:]))\n",
    "                self.data.append((user_id, sequence))\n",
    "                self.max_seq_length = max(self.max_seq_length, len(sequence))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_id, sequence = self.data[idx]\n",
    "        input_seq = torch.tensor(sequence[:-1], dtype=torch.long)\n",
    "        target_seq = torch.tensor(sequence[1:], dtype=torch.long)\n",
    "        return user_id, input_seq, target_seq\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    user_ids, input_seqs, target_seqs = zip(*batch)\n",
    "    input_seqs = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
    "    target_seqs = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
    "    return user_ids, input_seqs, target_seqs\n",
    "    \n",
    "dataset = SequentialRecommendationDataset('sequential_recommendation_data.txt')\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730, 226, 440, 537, 1810, 945, 580, 1263)\n",
      "8\n",
      "tensor([[54700, 66808, 54823,  ...,     0,     0,     0],\n",
      "        [ 8759,  8759, 54068,  ...,     0,     0,     0],\n",
      "        [56970, 56970, 56970,  ..., 27708, 41881, 41881],\n",
      "        ...,\n",
      "        [68402, 68402, 68402,  ...,     0,     0,     0],\n",
      "        [37283, 37283, 37283,  ...,     0,     0,     0],\n",
      "        [53198, 53198, 53198,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# print the first batch\n",
    "for user_ids, input_seqs, target_seqs in dataloader:\n",
    "    print(user_ids)\n",
    "    print(len(input_seqs))\n",
    "    print(target_seqs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 69224\n",
      "Epoch 1, Loss: 6.730216026306152\n",
      "Epoch 2, Loss: 2.4250097274780273\n",
      "Epoch 3, Loss: 2.1128721237182617\n",
      "Epoch 4, Loss: 1.32228422164917\n",
      "Epoch 5, Loss: 1.2418441772460938\n"
     ]
    }
   ],
   "source": [
    "# define the rnn model\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "# Hyperparameters\n",
    "# input size should be the number of items in the dataset + 1 (for padding) , so calculate it from the .txt file\n",
    "input_size = 0\n",
    "with open('sequential_recommendation_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        sequence = list(map(int, parts[1:]))\n",
    "        input_size = max(input_size, max(sequence))\n",
    "input_size += 1\n",
    "\n",
    "print(f'vocab size: {input_size}')\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = input_size\n",
    "num_epochs = 5\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Initialize the model, loss function, and optimizer and train the model on gpu\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (user_ids, input_seqs, target_seqs) in enumerate(dataloader):\n",
    "        \n",
    "        model.train()\n",
    "        input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "        outputs = model(input_seqs)\n",
    "        # print(outputs.shape, target_seqs.shape)\n",
    "        loss = criterion(outputs.view(-1, num_classes), target_seqs.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if (i+1) % 10 == 0:\n",
    "        #     print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        #            .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    \n",
    "    if loss.item() < 0.8:\n",
    "        break\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'rnnmodel.ckpt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this section, we will perform inference on our trained model. The goal is to predict the next sequence of items based on a given input item. This is a common scenario in recommendation systems where we want to predict what items a user might interact with next, based on their past interactions.\n",
    "\n",
    "The process will work as follows:\n",
    "\n",
    "1. We start by feeding the model an input item.\n",
    "2. The model will generate a prediction for the next item.\n",
    "3. We then take the model's prediction and use it as the new input, repeating the process.\n",
    "4. This is done iteratively, up to 5 times, to generate a sequence of recommended items.\n",
    "\n",
    "This method of using the model's own predictions as input for subsequent predictions is known as autoregression.\n",
    "\n",
    "Let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting item: 10289\n",
      "Generated sequence: [10289, 10289, 62569, 62569, 62569, 62569]\n"
     ]
    }
   ],
   "source": [
    "# Now lets do inference where i will give the model one item and it will predict the next sequence upto 5 items(feed the output of the model as input to the model again)\n",
    "\n",
    "# Load the model checkpoint\n",
    "model = RNNModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('rnnmodel.ckpt'))\n",
    "model.eval()\n",
    "\n",
    "# Inference\n",
    "\n",
    "# Choose a random item from the dataset\n",
    "import random\n",
    "item = random.randint(1, input_size-1)\n",
    "print('Starting item:', item)\n",
    "\n",
    "# Initialize the input sequence with the chosen item\n",
    "input_seq = torch.tensor([[item]]).to(device)\n",
    "\n",
    "# Generate the next 5 items in the sequence\n",
    "with torch.no_grad():\n",
    "    for _ in range(5):\n",
    "        output = model(input_seq)\n",
    "        _, predicted = torch.max(output[:, -1, :], 1)\n",
    "        input_seq = torch.cat((input_seq, predicted.unsqueeze(1)), dim=1)\n",
    "        \n",
    "print('Generated sequence:', input_seq.squeeze().tolist())\n",
    "\n",
    "torch.save(model.state_dict(), 'rnnmodel.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now create the transformer decoder architecture and train it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.878972053527832\n",
      "Epoch 2, Loss: 2.6818923950195312\n",
      "Epoch 3, Loss: 1.9506988525390625\n",
      "Epoch 4, Loss: 1.3681130409240723\n",
      "Epoch 5, Loss: 1.2575690746307373\n"
     ]
    }
   ],
   "source": [
    "# create the transformer deocder model which takes the inout sequence one by one and predicts the next item in the sequence and train the model on gpu use teacher forcing technique\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_items, embed_size, num_layers, num_heads, hidden_dim):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.item_embedding = nn.Embedding(num_items, embed_size)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(embed_size, num_heads, hidden_dim),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, num_items)\n",
    "        \n",
    "    def forward(self, input_seqs):\n",
    "        embeddings = self.item_embedding(input_seqs)\n",
    "        output = self.transformer_decoder(embeddings, embeddings)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "# Hyperparameters\n",
    "num_items = 0\n",
    "with open('sequential_recommendation_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        sequence = list(map(int, parts[1:]))\n",
    "        num_items = max(num_items, max(sequence))\n",
    "        \n",
    "num_items += 1\n",
    "embed_size = 128\n",
    "num_layers = 1\n",
    "num_heads = 2\n",
    "hidden_dim = 256\n",
    "num_epochs = 5\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Initialize the model, loss function, and optimizer and train the model on gpu\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerDecoder(num_items, embed_size, num_layers, num_heads, hidden_dim)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(dataloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (user_ids, input_seqs, target_seqs) in enumerate(dataloader):\n",
    "        \n",
    "        model.train()\n",
    "        input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "        outputs = model(input_seqs)\n",
    "        loss = criterion(outputs.view(-1, num_items), target_seqs.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    \n",
    "    if loss.item() < 0.8:\n",
    "        break\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'transformermodel.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting item: 48010\n",
      "Generated sequence: [48010, 48010, 48010, 48010, 48010, 48010, 48010, 48010, 48010, 48010, 48010]\n"
     ]
    }
   ],
   "source": [
    "# Now lets do inference where i will give the model one item and it will predict the next sequence upto 5 items(feed the output of the model as input to the model again)\n",
    "\n",
    "# Load the model checkpoint\n",
    "model = TransformerDecoder(num_items, embed_size, num_layers, num_heads, hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load('transformermodel.ckpt'))\n",
    "model.eval()\n",
    "\n",
    "# Inference\n",
    "\n",
    "# Choose a random item from the dataset\n",
    "import random\n",
    "item = random.randint(1, num_items-1)\n",
    "print('Starting item:', item)\n",
    "\n",
    "# Initialize the input sequence with the chosen item\n",
    "input_seq = torch.tensor([[item]]).to(device)\n",
    "\n",
    "# Generate the next 5 items in the sequence\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        output = model(input_seq)\n",
    "        _, predicted = torch.max(output[:, -1, :], 1)\n",
    "        input_seq = torch.cat((input_seq, predicted.unsqueeze(1)), dim=1)\n",
    "        \n",
    "print('Generated sequence:', input_seq.squeeze().tolist())\n",
    "\n",
    "#save the model checkpoint\n",
    "torch.save(model.state_dict(), 'transformermodel.ckpt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: NDCG Score Calculation\n",
    "\n",
    "In this section, we will evaluate the performance of our trained models - the Transformer model and the Recurrent Neural Network (RNN) model. The evaluation metric we will use is the Normalized Discounted Cumulative Gain (NDCG) score.\n",
    "\n",
    "NDCG is a popular metric for evaluating recommendation systems, as it takes into account both the relevance of the recommended items and their ranking. Higher NDCG scores indicate better performance of the model.\n",
    "\n",
    "## Transformer Model Evaluation\n",
    "\n",
    "First, we will calculate the NDCG score for the Transformer model. We will use the test dataset to generate recommendations and then compare these recommendations with the actual items the users interacted with.\n",
    "\n",
    "## RNN Model Evaluation\n",
    "\n",
    "Next, we will calculate the NDCG score for the RNN model, following the same process as with the Transformer model. \n",
    "\n",
    "By comparing the NDCG scores of the two models, we can determine which model performs better at recommending items that are relevant to the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG Score Calculation\n",
    "\n",
    "Normalized Discounted Cumulative Gain (NDCG) is a popular metric used for evaluating the performance of recommendation systems. It measures the performance of a recommendation system based on the relevance of recommended items and their ranking.\n",
    "\n",
    "The formula for DCG (Discounted Cumulative Gain) is:\n",
    "\n",
    "DCG@k = Î£ (2^relevance[i] - 1) / log2(i + 1) for i in range(1, k+1)\n",
    "\n",
    "Where:\n",
    "- `relevance[i]` is the relevance of the item at position `i` in the recommended list. In the context of recommendation systems, this could be the rating of the item.\n",
    "- `log2(i + 1)` is a discount factor that reduces the contribution of items as their position in the list increases. The `+1` in the log and range functions is to account for the fact that list positions start at 1, not 0.\n",
    "\n",
    "The formula for NDCG is:\n",
    "\n",
    "NDCG@k = DCG@k / IDCG@k\n",
    "\n",
    "Where:\n",
    "- `DCG@k` is the DCG score for the recommended list.\n",
    "- `IDCG@k` is the DCG score for the ideal list (a perfectly ranked list).\n",
    "\n",
    "The NDCG score is a value between 0 and 1. A score of 1 means that the recommended list is perfectly ranked, while a score of 0 means the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(69224, 128)\n",
       "  (rnn): RNN(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=69224, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load rnn and transformer model\n",
    "\n",
    "transformer_model = TransformerDecoder(num_items, embed_size, num_layers, num_heads, hidden_dim).to(device)\n",
    "transformer_model.load_state_dict(torch.load('transformermodel.ckpt'))\n",
    "transformer_model.eval()\n",
    "\n",
    "rnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "rnn_model.load_state_dict(torch.load('rnnmodel.ckpt'))\n",
    "rnn_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for RNN model: 0.9612536197577365\n",
      "NDCG@5 for Transformer model: 0.9610426060369133\n",
      "NDCG@10 for RNN model: 0.9915906071219627\n",
      "NDCG@10 for Transformer model: 0.9908851721113027\n"
     ]
    }
   ],
   "source": [
    "#Load both the models and compare the results of ndcg@5 and ndcg@10 for both the models by calculating dcg and idcg and then calculating ndcg\n",
    "\n",
    "# Load the dataset\n",
    "dataset = SequentialRecommendationDataset('sequential_recommendation_data.txt')\n",
    "\n",
    "# calculate NDCG for the RNN model and Transformer model\n",
    "\n",
    "def calculate_ndcg(model, dataset, device, k):\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    for user_ids, input_seqs, target_seqs in dataloader:\n",
    "        input_seqs = input_seqs.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seqs)\n",
    "            for i in range(len(user_ids)):\n",
    "                user_id = user_ids[i]\n",
    "                target_seq = target_seqs[i]\n",
    "                predicted_seq = output[i].argmax(dim=1)\n",
    "                dcg += calculate_dcg(target_seq, predicted_seq, k)\n",
    "                idcg += calculate_dcg(target_seq, target_seq, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "def calculate_dcg(target_seq, predicted_seq, k):\n",
    "    target_seq = target_seq.cpu().numpy()\n",
    "    predicted_seq = predicted_seq.cpu().numpy()\n",
    "    dcg = 0\n",
    "    for i in range(min(k, len(target_seq))):\n",
    "        item = predicted_seq[i]\n",
    "        if item in target_seq:\n",
    "            rank = np.where(target_seq == item)[0][0]\n",
    "            dcg += 1 / np.log2(rank + 2)\n",
    "    return dcg\n",
    "\n",
    "import numpy as np\n",
    "k = 5\n",
    "ndcg_rnn = calculate_ndcg(rnn_model, dataset, device, k)\n",
    "ndcg_transformer = calculate_ndcg(transformer_model, dataset, device, k)\n",
    "\n",
    "print(f'NDCG@{k} for RNN model: {ndcg_rnn}')\n",
    "print(f'NDCG@{k} for Transformer model: {ndcg_transformer}')\n",
    "\n",
    "k = 10\n",
    "ndcg_rnn = calculate_ndcg(rnn_model, dataset, device, k)\n",
    "ndcg_transformer = calculate_ndcg(transformer_model, dataset, device, k)\n",
    "\n",
    "print(f'NDCG@{k} for RNN model: {ndcg_rnn}')\n",
    "print(f'NDCG@{k} for Transformer model: {ndcg_transformer}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

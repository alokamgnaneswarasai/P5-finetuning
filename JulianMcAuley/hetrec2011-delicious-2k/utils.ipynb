{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of users: 1868\n",
      "No of tags: 40898\n",
      "No of interactions: 437594\n",
      "No of bookmarks: 69224\n"
     ]
    }
   ],
   "source": [
    "# read the user_taggedbookmarks.dat and find unique noof users and tags and noof interations\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('user_taggedbookmarks.dat'):\n",
    "    print('user_taggedbookmarks.dat not found')\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "with open('user_taggedbookmarks.dat', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "users = set()\n",
    "tags = set()\n",
    "interactions = 0\n",
    "bookmarks = set()\n",
    "for line in lines:\n",
    "    interactions += 1\n",
    "    # userID\tbookmarkID\ttagID\tday\tmonth\tyear\thour\tminute\tsecond is given\n",
    "    # so we are interested in userID and bookmarkID\n",
    "    parts = line.split('\\t')\n",
    "    user = parts[0]\n",
    "\n",
    "    bookmark = parts[1]\n",
    "    tag = parts[2]\n",
    "    users.add(user)\n",
    "    tags.add(tag)\n",
    "    bookmarks.add(bookmark)\n",
    "\n",
    "print('No of users:', len(users))\n",
    "print('No of tags:', len(tags))\n",
    "print('No of interactions:', interactions)\n",
    "print('No of bookmarks:', len(bookmarks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a dictionary map which assigns each user to a unique number and each bookmark_id to a unique number\n",
    "\n",
    "user_map = {}\n",
    "bookmark_map = {}\n",
    "\n",
    "\n",
    "for i, user in enumerate(users):\n",
    "    user_map[user] = i\n",
    "\n",
    "\n",
    "\n",
    "for i, bookmark in enumerate(bookmarks):\n",
    "    bookmark_map[bookmark] = i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  bookmark\n",
      "0  1602     25743\n",
      "1   693     37866\n",
      "2   693     25270\n",
      "3   693     47976\n",
      "4   693     47976\n"
     ]
    }
   ],
   "source": [
    "# make a dataframe with mapped userid and mapped bookmarkid\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    parts = line.split('\\t')\n",
    "    user = parts[0]\n",
    "    bookmark = parts[1]\n",
    "    data.append([user_map[user], bookmark_map[bookmark]])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['user', 'bookmark'])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "0    [65020, 65020, 65020, 65020, 65020, 65020, 440...\n",
      "1    [845, 845, 845, 845, 60711, 60711, 60711, 6071...\n",
      "2    [6404, 6404, 57122, 57122, 57122, 34242, 34242...\n",
      "3    [34125, 34125, 34125, 51952, 51952, 51952, 386...\n",
      "4    [60691, 60691, 60691, 64263, 64263, 64263, 367...\n",
      "Name: bookmark, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Now group by user so that each user has a list of bookmarks\n",
    "grouped = df.groupby('user')['bookmark'].apply(list)\n",
    "\n",
    "print(grouped.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential_data.txt created\n"
     ]
    }
   ],
   "source": [
    "# Now create a sequntial_data.txt file which has the data in the format where each line is a user and the bookmarks are separated by space\n",
    "with open('sequential_data.txt', 'w') as f:\n",
    "    for user, bookmarks in grouped.items():\n",
    "        f.write(str(user) + ' ' + ' '.join(map(str, bookmarks)) + '\\n')\n",
    "\n",
    "print('sequential_data.txt created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 248 fields in line 2, saw 327\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the seqential_recommendation_data.txt file where the first column is user and the rest of the columns are the bookmarks(and all of them are separated by space), find min and max no of bookmarks per user \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequential_recommendation_data.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 248 fields in line 2, saw 327\n"
     ]
    }
   ],
   "source": [
    "# Load the seqential_recommendation_data.txt file where the first column is user and the rest of the columns are the bookmarks(and all of them are separated by space), find min and max no of bookmarks per user \n",
    "\n",
    "df = pd.read_csv('sequential_recommendation_data.txt', sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of bookmarks per user: 1\n",
      "Max number of bookmarks per user: 1675\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store the data\n",
    "users = []\n",
    "bookmarks = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open('sequential_recommendation_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')  # Split each line by space\n",
    "        users.append(parts[0])  # First part is the user\n",
    "        bookmarks.append(parts[1:])  # The rest are bookmarks\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'User': users, 'Bookmarks': bookmarks})\n",
    "\n",
    "# To find min and max number of bookmarks per user\n",
    "df['Bookmark_Count'] = df['Bookmarks'].apply(len)\n",
    "min_bookmarks = df['Bookmark_Count'].min()\n",
    "max_bookmarks = df['Bookmark_Count'].max()\n",
    "\n",
    "print(f\"Min number of bookmarks per user: {min_bookmarks}\")\n",
    "print(f\"Max number of bookmarks per user: {max_bookmarks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store the data\n",
    "users = []\n",
    "bookmarks = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open('sequential_recommendation_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')  # Split each line by space\n",
    "        if len(parts[1:]) >= 6:  # Check if there are at least 6 bookmarks\n",
    "            users.append(parts[0])  # First part is the user\n",
    "            bookmarks.append(parts[1:])  # The rest are bookmarks\n",
    "\n",
    "# Write the filtered data to a new file\n",
    "with open('filtered_recommendation_data.txt', 'w') as file:\n",
    "    for user, user_bookmarks in zip(users, bookmarks):\n",
    "        file.write(f\"{user} {' '.join(user_bookmarks)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

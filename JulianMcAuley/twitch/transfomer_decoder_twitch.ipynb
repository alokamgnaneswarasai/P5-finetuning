{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "class SequentialRecommendationDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        self.max_seq_length = 0\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                user_id = int(parts[0])\n",
    "                sequence = list(map(int, parts[1:]))\n",
    "                self.data.append((user_id, sequence))\n",
    "                self.max_seq_length = max(self.max_seq_length, len(sequence))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_id, sequence = self.data[idx]\n",
    "        input_seq = torch.tensor(sequence[:-1], dtype=torch.long)\n",
    "        target_seq = torch.tensor(sequence[1:], dtype=torch.long)\n",
    "        return user_id, input_seq, target_seq\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    user_ids, input_seqs, target_seqs = zip(*batch)\n",
    "    input_seqs = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
    "    target_seqs = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
    "    return user_ids, input_seqs, target_seqs\n",
    "    \n",
    "dataset = SequentialRecommendationDataset('sequential_recommendation_data.txt')\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80564, 75501, 65752, 36940, 86890, 62034, 50987, 43238, 56751, 57888, 18677, 43690, 30710, 23901, 49763, 4091)\n",
      "16\n",
      "tensor([[143274, 143273,   6843,  ...,      0,      0,      0],\n",
      "        [    55,     55,     55,  ...,      0,      0,      0],\n",
      "        [  5287,   2139,    603,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   342,    326,    330,  ...,      0,      0,      0],\n",
      "        [  1295,   1600,  11026,  ...,      0,      0,      0],\n",
      "        [   105,  19952,  19953,  ...,      0,      0,      0]])\n"
     ]
    }
   ],
   "source": [
    "# print the first batch\n",
    "for user_ids, input_seqs, target_seqs in dataloader:\n",
    "    print(user_ids)\n",
    "    print(len(input_seqs))\n",
    "    print(target_seqs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 162625\n",
      "Epoch [1/5], Step [10/6250], Loss: 1.8450\n",
      "Epoch [1/5], Step [20/6250], Loss: 2.9251\n",
      "Epoch [1/5], Step [30/6250], Loss: 2.2566\n",
      "Epoch [1/5], Step [40/6250], Loss: 2.8118\n",
      "Epoch [1/5], Step [50/6250], Loss: 2.9440\n",
      "Epoch [1/5], Step [60/6250], Loss: 3.5444\n",
      "Epoch [1/5], Step [70/6250], Loss: 2.5486\n",
      "Epoch [1/5], Step [80/6250], Loss: 2.9275\n",
      "Epoch [1/5], Step [90/6250], Loss: 2.8369\n",
      "Epoch [1/5], Step [100/6250], Loss: 2.1149\n",
      "Epoch [1/5], Step [110/6250], Loss: 1.8854\n",
      "Epoch [1/5], Step [120/6250], Loss: 2.0392\n",
      "Epoch [1/5], Step [130/6250], Loss: 2.8217\n",
      "Epoch [1/5], Step [140/6250], Loss: 2.4260\n",
      "Epoch [1/5], Step [150/6250], Loss: 2.3919\n",
      "Epoch [1/5], Step [160/6250], Loss: 2.8445\n",
      "Epoch [1/5], Step [170/6250], Loss: 4.0471\n",
      "Epoch [1/5], Step [180/6250], Loss: 2.7138\n",
      "Epoch [1/5], Step [190/6250], Loss: 1.5724\n",
      "Epoch [1/5], Step [200/6250], Loss: 2.1003\n",
      "Epoch [1/5], Step [210/6250], Loss: 2.3696\n",
      "Epoch [1/5], Step [220/6250], Loss: 2.2020\n",
      "Epoch [1/5], Step [230/6250], Loss: 2.4168\n",
      "Epoch [1/5], Step [240/6250], Loss: 2.4974\n",
      "Epoch [1/5], Step [250/6250], Loss: 2.8378\n",
      "Epoch [1/5], Step [260/6250], Loss: 2.9671\n",
      "Epoch [1/5], Step [270/6250], Loss: 3.2688\n",
      "Epoch [1/5], Step [280/6250], Loss: 2.1607\n",
      "Epoch [1/5], Step [290/6250], Loss: 1.6200\n",
      "Epoch [1/5], Step [300/6250], Loss: 2.4253\n",
      "Epoch [1/5], Step [310/6250], Loss: 1.9422\n",
      "Epoch [1/5], Step [320/6250], Loss: 3.4253\n",
      "Epoch [1/5], Step [330/6250], Loss: 2.9115\n",
      "Epoch [1/5], Step [340/6250], Loss: 2.2562\n",
      "Epoch [1/5], Step [350/6250], Loss: 2.6307\n",
      "Epoch [1/5], Step [360/6250], Loss: 2.7891\n",
      "Epoch [1/5], Step [370/6250], Loss: 1.6218\n",
      "Epoch [1/5], Step [380/6250], Loss: 1.9123\n",
      "Epoch [1/5], Step [390/6250], Loss: 1.8293\n",
      "Epoch [1/5], Step [400/6250], Loss: 2.2876\n",
      "Epoch [1/5], Step [410/6250], Loss: 1.9639\n",
      "Epoch [1/5], Step [420/6250], Loss: 1.7202\n",
      "Epoch [1/5], Step [430/6250], Loss: 1.7309\n",
      "Epoch [1/5], Step [440/6250], Loss: 1.2542\n",
      "Epoch [1/5], Step [450/6250], Loss: 1.2912\n",
      "Epoch [1/5], Step [460/6250], Loss: 1.9718\n",
      "Epoch [1/5], Step [470/6250], Loss: 3.4701\n",
      "Epoch [1/5], Step [480/6250], Loss: 2.1339\n",
      "Epoch [1/5], Step [490/6250], Loss: 4.0117\n",
      "Epoch [1/5], Step [500/6250], Loss: 1.8364\n",
      "Epoch [1/5], Step [510/6250], Loss: 2.2763\n",
      "Epoch [1/5], Step [520/6250], Loss: 2.5552\n",
      "Epoch [1/5], Step [530/6250], Loss: 1.8107\n",
      "Epoch [1/5], Step [540/6250], Loss: 3.0886\n",
      "Epoch [1/5], Step [550/6250], Loss: 1.4110\n",
      "Epoch [1/5], Step [560/6250], Loss: 2.0531\n",
      "Epoch [1/5], Step [570/6250], Loss: 3.5775\n",
      "Epoch [1/5], Step [580/6250], Loss: 1.4311\n",
      "Epoch [1/5], Step [590/6250], Loss: 1.6542\n",
      "Epoch [1/5], Step [600/6250], Loss: 2.5462\n",
      "Epoch [1/5], Step [610/6250], Loss: 1.8385\n",
      "Epoch [1/5], Step [620/6250], Loss: 2.1204\n",
      "Epoch [1/5], Step [630/6250], Loss: 1.8458\n",
      "Epoch [1/5], Step [640/6250], Loss: 1.5597\n",
      "Epoch [1/5], Step [650/6250], Loss: 2.0633\n",
      "Epoch [1/5], Step [660/6250], Loss: 1.7196\n",
      "Epoch [1/5], Step [670/6250], Loss: 3.3680\n",
      "Epoch [1/5], Step [680/6250], Loss: 2.1904\n",
      "Epoch [1/5], Step [690/6250], Loss: 2.3026\n",
      "Epoch [1/5], Step [700/6250], Loss: 2.4551\n",
      "Epoch [1/5], Step [710/6250], Loss: 2.5789\n",
      "Epoch [1/5], Step [720/6250], Loss: 1.6229\n",
      "Epoch [1/5], Step [730/6250], Loss: 1.6465\n",
      "Epoch [1/5], Step [740/6250], Loss: 2.2524\n",
      "Epoch [1/5], Step [750/6250], Loss: 1.6193\n",
      "Epoch [1/5], Step [760/6250], Loss: 1.4971\n",
      "Epoch [1/5], Step [770/6250], Loss: 2.7099\n",
      "Epoch [1/5], Step [780/6250], Loss: 2.0765\n",
      "Epoch [1/5], Step [790/6250], Loss: 3.8945\n",
      "Epoch [1/5], Step [800/6250], Loss: 2.0020\n",
      "Epoch [1/5], Step [810/6250], Loss: 2.5049\n",
      "Epoch [1/5], Step [820/6250], Loss: 2.2468\n",
      "Epoch [1/5], Step [830/6250], Loss: 2.2037\n",
      "Epoch [1/5], Step [840/6250], Loss: 1.5164\n",
      "Epoch [1/5], Step [850/6250], Loss: 2.1730\n",
      "Epoch [1/5], Step [860/6250], Loss: 2.3829\n",
      "Epoch [1/5], Step [870/6250], Loss: 2.6126\n",
      "Epoch [1/5], Step [880/6250], Loss: 1.6430\n",
      "Epoch [1/5], Step [890/6250], Loss: 2.1342\n",
      "Epoch [1/5], Step [900/6250], Loss: 2.2340\n",
      "Epoch [1/5], Step [910/6250], Loss: 2.2755\n",
      "Epoch [1/5], Step [920/6250], Loss: 2.8189\n",
      "Epoch [1/5], Step [930/6250], Loss: 2.0816\n",
      "Epoch [1/5], Step [940/6250], Loss: 1.2609\n",
      "Epoch [1/5], Step [950/6250], Loss: 2.5645\n",
      "Epoch [1/5], Step [960/6250], Loss: 1.8119\n",
      "Epoch [1/5], Step [970/6250], Loss: 2.5325\n",
      "Epoch [1/5], Step [980/6250], Loss: 2.0225\n",
      "Epoch [1/5], Step [990/6250], Loss: 2.0202\n",
      "Epoch [1/5], Step [1000/6250], Loss: 1.6979\n",
      "Epoch [1/5], Step [1010/6250], Loss: 2.6355\n",
      "Epoch [1/5], Step [1020/6250], Loss: 1.0825\n",
      "Epoch [1/5], Step [1030/6250], Loss: 1.9094\n",
      "Epoch [1/5], Step [1040/6250], Loss: 2.3233\n",
      "Epoch [1/5], Step [1050/6250], Loss: 2.9208\n",
      "Epoch [1/5], Step [1060/6250], Loss: 3.8794\n",
      "Epoch [1/5], Step [1070/6250], Loss: 2.2718\n",
      "Epoch [1/5], Step [1080/6250], Loss: 2.0366\n",
      "Epoch [1/5], Step [1090/6250], Loss: 1.6020\n",
      "Epoch [1/5], Step [1100/6250], Loss: 2.5161\n",
      "Epoch [1/5], Step [1110/6250], Loss: 1.6759\n",
      "Epoch [1/5], Step [1120/6250], Loss: 2.9882\n",
      "Epoch [1/5], Step [1130/6250], Loss: 2.5868\n",
      "Epoch [1/5], Step [1140/6250], Loss: 2.5615\n",
      "Epoch [1/5], Step [1150/6250], Loss: 1.9892\n",
      "Epoch [1/5], Step [1160/6250], Loss: 1.7739\n",
      "Epoch [1/5], Step [1170/6250], Loss: 1.7690\n",
      "Epoch [1/5], Step [1180/6250], Loss: 2.7538\n",
      "Epoch [1/5], Step [1190/6250], Loss: 1.9190\n",
      "Epoch [1/5], Step [1200/6250], Loss: 1.9269\n",
      "Epoch [1/5], Step [1210/6250], Loss: 2.6754\n",
      "Epoch [1/5], Step [1220/6250], Loss: 1.9297\n",
      "Epoch [1/5], Step [1230/6250], Loss: 1.7086\n",
      "Epoch [1/5], Step [1240/6250], Loss: 1.2773\n",
      "Epoch [1/5], Step [1250/6250], Loss: 2.4109\n",
      "Epoch [1/5], Step [1260/6250], Loss: 3.2057\n",
      "Epoch [1/5], Step [1270/6250], Loss: 1.7821\n",
      "Epoch [1/5], Step [1280/6250], Loss: 2.7524\n",
      "Epoch [1/5], Step [1290/6250], Loss: 1.8490\n",
      "Epoch [1/5], Step [1300/6250], Loss: 2.6690\n",
      "Epoch [1/5], Step [1310/6250], Loss: 1.3897\n",
      "Epoch [1/5], Step [1320/6250], Loss: 1.6657\n",
      "Epoch [1/5], Step [1330/6250], Loss: 3.4850\n",
      "Epoch [1/5], Step [1340/6250], Loss: 2.0062\n",
      "Epoch [1/5], Step [1350/6250], Loss: 2.0424\n",
      "Epoch [1/5], Step [1360/6250], Loss: 1.1228\n",
      "Epoch [1/5], Step [1370/6250], Loss: 1.5696\n",
      "Epoch [1/5], Step [1380/6250], Loss: 2.4974\n",
      "Epoch [1/5], Step [1390/6250], Loss: 2.6554\n",
      "Epoch [1/5], Step [1400/6250], Loss: 3.0632\n",
      "Epoch [1/5], Step [1410/6250], Loss: 2.3436\n",
      "Epoch [1/5], Step [1420/6250], Loss: 1.7432\n",
      "Epoch [1/5], Step [1430/6250], Loss: 1.1174\n",
      "Epoch [1/5], Step [1440/6250], Loss: 2.4492\n",
      "Epoch [1/5], Step [1450/6250], Loss: 2.8082\n",
      "Epoch [1/5], Step [1460/6250], Loss: 2.8959\n",
      "Epoch [1/5], Step [1470/6250], Loss: 1.9661\n",
      "Epoch [1/5], Step [1480/6250], Loss: 1.9129\n",
      "Epoch [1/5], Step [1490/6250], Loss: 2.1767\n",
      "Epoch [1/5], Step [1500/6250], Loss: 1.9852\n",
      "Epoch [1/5], Step [1510/6250], Loss: 2.0651\n",
      "Epoch [1/5], Step [1520/6250], Loss: 1.9557\n",
      "Epoch [1/5], Step [1530/6250], Loss: 2.6795\n",
      "Epoch [1/5], Step [1540/6250], Loss: 2.6141\n",
      "Epoch [1/5], Step [1550/6250], Loss: 1.0766\n",
      "Epoch [1/5], Step [1560/6250], Loss: 1.8521\n",
      "Epoch [1/5], Step [1570/6250], Loss: 1.9141\n",
      "Epoch [1/5], Step [1580/6250], Loss: 2.2333\n",
      "Epoch [1/5], Step [1590/6250], Loss: 2.3815\n",
      "Epoch [1/5], Step [1600/6250], Loss: 1.5352\n",
      "Epoch [1/5], Step [1610/6250], Loss: 1.7246\n",
      "Epoch [1/5], Step [1620/6250], Loss: 2.0105\n",
      "Epoch [1/5], Step [1630/6250], Loss: 2.3656\n",
      "Epoch [1/5], Step [1640/6250], Loss: 1.7181\n",
      "Epoch [1/5], Step [1650/6250], Loss: 2.5921\n",
      "Epoch [1/5], Step [1660/6250], Loss: 2.6703\n",
      "Epoch [1/5], Step [1670/6250], Loss: 2.3449\n",
      "Epoch [1/5], Step [1680/6250], Loss: 2.7571\n",
      "Epoch [1/5], Step [1690/6250], Loss: 2.1419\n",
      "Epoch [1/5], Step [1700/6250], Loss: 2.2051\n",
      "Epoch [1/5], Step [1710/6250], Loss: 2.0571\n",
      "Epoch [1/5], Step [1720/6250], Loss: 2.1276\n",
      "Epoch [1/5], Step [1730/6250], Loss: 1.3601\n",
      "Epoch [1/5], Step [1740/6250], Loss: 2.5166\n",
      "Epoch [1/5], Step [1750/6250], Loss: 1.8008\n",
      "Epoch [1/5], Step [1760/6250], Loss: 2.1305\n",
      "Epoch [1/5], Step [1770/6250], Loss: 2.1913\n",
      "Epoch [1/5], Step [1780/6250], Loss: 1.8014\n",
      "Epoch [1/5], Step [1790/6250], Loss: 1.6905\n",
      "Epoch [1/5], Step [1800/6250], Loss: 1.1018\n",
      "Epoch [1/5], Step [1810/6250], Loss: 1.4286\n",
      "Epoch [1/5], Step [1820/6250], Loss: 2.1441\n",
      "Epoch [1/5], Step [1830/6250], Loss: 2.9098\n",
      "Epoch [1/5], Step [1840/6250], Loss: 1.5595\n",
      "Epoch [1/5], Step [1850/6250], Loss: 1.8622\n",
      "Epoch [1/5], Step [1860/6250], Loss: 2.3258\n",
      "Epoch [1/5], Step [1870/6250], Loss: 2.5308\n",
      "Epoch [1/5], Step [1880/6250], Loss: 1.9132\n",
      "Epoch [1/5], Step [1890/6250], Loss: 1.9792\n",
      "Epoch [1/5], Step [1900/6250], Loss: 2.3949\n",
      "Epoch [1/5], Step [1910/6250], Loss: 2.3302\n",
      "Epoch [1/5], Step [1920/6250], Loss: 2.3793\n",
      "Epoch [1/5], Step [1930/6250], Loss: 3.2940\n",
      "Epoch [1/5], Step [1940/6250], Loss: 2.2813\n",
      "Epoch [1/5], Step [1950/6250], Loss: 2.6649\n",
      "Epoch [1/5], Step [1960/6250], Loss: 2.5530\n",
      "Epoch [1/5], Step [1970/6250], Loss: 2.3573\n",
      "Epoch [1/5], Step [1980/6250], Loss: 1.3359\n",
      "Epoch [1/5], Step [1990/6250], Loss: 2.3665\n",
      "Epoch [1/5], Step [2000/6250], Loss: 4.1893\n",
      "Epoch [1/5], Step [2010/6250], Loss: 2.8308\n",
      "Epoch [1/5], Step [2020/6250], Loss: 1.0784\n",
      "Epoch [1/5], Step [2030/6250], Loss: 1.9935\n",
      "Epoch [1/5], Step [2040/6250], Loss: 2.0829\n",
      "Epoch [1/5], Step [2050/6250], Loss: 1.9026\n",
      "Epoch [1/5], Step [2060/6250], Loss: 1.6414\n",
      "Epoch [1/5], Step [2070/6250], Loss: 2.0166\n",
      "Epoch [1/5], Step [2080/6250], Loss: 2.2806\n",
      "Epoch [1/5], Step [2090/6250], Loss: 2.3103\n",
      "Epoch [1/5], Step [2100/6250], Loss: 2.7004\n",
      "Epoch [1/5], Step [2110/6250], Loss: 2.0032\n",
      "Epoch [1/5], Step [2120/6250], Loss: 1.6631\n",
      "Epoch [1/5], Step [2130/6250], Loss: 1.9383\n",
      "Epoch [1/5], Step [2140/6250], Loss: 1.6968\n",
      "Epoch [1/5], Step [2150/6250], Loss: 3.0324\n",
      "Epoch [1/5], Step [2160/6250], Loss: 1.5850\n",
      "Epoch [1/5], Step [2170/6250], Loss: 1.6348\n",
      "Epoch [1/5], Step [2180/6250], Loss: 3.3605\n",
      "Epoch [1/5], Step [2190/6250], Loss: 2.4263\n",
      "Epoch [1/5], Step [2200/6250], Loss: 2.0460\n",
      "Epoch [1/5], Step [2210/6250], Loss: 1.1319\n",
      "Epoch [1/5], Step [2220/6250], Loss: 1.9806\n",
      "Epoch [1/5], Step [2230/6250], Loss: 1.7918\n",
      "Epoch [1/5], Step [2240/6250], Loss: 1.9164\n",
      "Epoch [1/5], Step [2250/6250], Loss: 2.7676\n",
      "Epoch [1/5], Step [2260/6250], Loss: 2.0119\n",
      "Epoch [1/5], Step [2270/6250], Loss: 2.9466\n",
      "Epoch [1/5], Step [2280/6250], Loss: 1.9208\n",
      "Epoch [1/5], Step [2290/6250], Loss: 2.3220\n",
      "Epoch [1/5], Step [2300/6250], Loss: 2.3252\n",
      "Epoch [1/5], Step [2310/6250], Loss: 2.1600\n",
      "Epoch [1/5], Step [2320/6250], Loss: 2.3248\n",
      "Epoch [1/5], Step [2330/6250], Loss: 3.2161\n",
      "Epoch [1/5], Step [2340/6250], Loss: 2.7449\n",
      "Epoch [1/5], Step [2350/6250], Loss: 1.0360\n",
      "Epoch [1/5], Step [2360/6250], Loss: 1.9573\n",
      "Epoch [1/5], Step [2370/6250], Loss: 2.1694\n",
      "Epoch [1/5], Step [2380/6250], Loss: 2.9283\n",
      "Epoch [1/5], Step [2390/6250], Loss: 2.1328\n",
      "Epoch [1/5], Step [2400/6250], Loss: 2.9935\n",
      "Epoch [1/5], Step [2410/6250], Loss: 1.6404\n",
      "Epoch [1/5], Step [2420/6250], Loss: 2.1209\n",
      "Epoch [1/5], Step [2430/6250], Loss: 1.5858\n",
      "Epoch [1/5], Step [2440/6250], Loss: 1.8657\n",
      "Epoch [1/5], Step [2450/6250], Loss: 2.3046\n",
      "Epoch [1/5], Step [2460/6250], Loss: 1.5925\n",
      "Epoch [1/5], Step [2470/6250], Loss: 3.8301\n",
      "Epoch [1/5], Step [2480/6250], Loss: 1.5886\n",
      "Epoch [1/5], Step [2490/6250], Loss: 1.6577\n",
      "Epoch [1/5], Step [2500/6250], Loss: 1.4331\n",
      "Epoch [1/5], Step [2510/6250], Loss: 2.4220\n",
      "Epoch [1/5], Step [2520/6250], Loss: 1.4396\n",
      "Epoch [1/5], Step [2530/6250], Loss: 1.5023\n",
      "Epoch [1/5], Step [2540/6250], Loss: 3.6652\n",
      "Epoch [1/5], Step [2550/6250], Loss: 3.0427\n",
      "Epoch [1/5], Step [2560/6250], Loss: 1.8572\n",
      "Epoch [1/5], Step [2570/6250], Loss: 2.1657\n",
      "Epoch [1/5], Step [2580/6250], Loss: 1.9738\n",
      "Epoch [1/5], Step [2590/6250], Loss: 1.9718\n",
      "Epoch [1/5], Step [2600/6250], Loss: 1.4608\n",
      "Epoch [1/5], Step [2610/6250], Loss: 2.0272\n",
      "Epoch [1/5], Step [2620/6250], Loss: 3.1643\n",
      "Epoch [1/5], Step [2630/6250], Loss: 1.7213\n",
      "Epoch [1/5], Step [2640/6250], Loss: 3.4850\n",
      "Epoch [1/5], Step [2650/6250], Loss: 2.3549\n",
      "Epoch [1/5], Step [2660/6250], Loss: 2.2931\n",
      "Epoch [1/5], Step [2670/6250], Loss: 3.0478\n",
      "Epoch [1/5], Step [2680/6250], Loss: 1.9402\n",
      "Epoch [1/5], Step [2690/6250], Loss: 2.8826\n",
      "Epoch [1/5], Step [2700/6250], Loss: 3.0118\n",
      "Epoch [1/5], Step [2710/6250], Loss: 1.2279\n",
      "Epoch [1/5], Step [2720/6250], Loss: 2.7870\n",
      "Epoch [1/5], Step [2730/6250], Loss: 2.8020\n",
      "Epoch [1/5], Step [2740/6250], Loss: 1.9836\n",
      "Epoch [1/5], Step [2750/6250], Loss: 2.6454\n",
      "Epoch [1/5], Step [2760/6250], Loss: 2.5739\n",
      "Epoch [1/5], Step [2770/6250], Loss: 2.9721\n",
      "Epoch [1/5], Step [2780/6250], Loss: 2.2366\n",
      "Epoch [1/5], Step [2790/6250], Loss: 3.3228\n",
      "Epoch [1/5], Step [2800/6250], Loss: 1.7583\n",
      "Epoch [1/5], Step [2810/6250], Loss: 2.2259\n",
      "Epoch [1/5], Step [2820/6250], Loss: 2.5473\n",
      "Epoch [1/5], Step [2830/6250], Loss: 1.6919\n",
      "Epoch [1/5], Step [2840/6250], Loss: 2.7420\n",
      "Epoch [1/5], Step [2850/6250], Loss: 1.9278\n",
      "Epoch [1/5], Step [2860/6250], Loss: 1.9594\n",
      "Epoch [1/5], Step [2870/6250], Loss: 3.0231\n",
      "Epoch [1/5], Step [2880/6250], Loss: 2.3768\n",
      "Epoch [1/5], Step [2890/6250], Loss: 3.0278\n",
      "Epoch [1/5], Step [2900/6250], Loss: 2.6939\n",
      "Epoch [1/5], Step [2910/6250], Loss: 1.7566\n",
      "Epoch [1/5], Step [2920/6250], Loss: 1.5194\n",
      "Epoch [1/5], Step [2930/6250], Loss: 2.6946\n",
      "Epoch [1/5], Step [2940/6250], Loss: 2.1436\n",
      "Epoch [1/5], Step [2950/6250], Loss: 2.4865\n",
      "Epoch [1/5], Step [2960/6250], Loss: 2.3774\n",
      "Epoch [1/5], Step [2970/6250], Loss: 2.2942\n",
      "Epoch [1/5], Step [2980/6250], Loss: 2.4795\n",
      "Epoch [1/5], Step [2990/6250], Loss: 2.9891\n",
      "Epoch [1/5], Step [3000/6250], Loss: 2.0155\n",
      "Epoch [1/5], Step [3010/6250], Loss: 2.0430\n",
      "Epoch [1/5], Step [3020/6250], Loss: 3.7472\n",
      "Epoch [1/5], Step [3030/6250], Loss: 1.5862\n",
      "Epoch [1/5], Step [3040/6250], Loss: 2.2126\n",
      "Epoch [1/5], Step [3050/6250], Loss: 2.4827\n",
      "Epoch [1/5], Step [3060/6250], Loss: 1.4419\n",
      "Epoch [1/5], Step [3070/6250], Loss: 1.9327\n",
      "Epoch [1/5], Step [3080/6250], Loss: 1.5366\n",
      "Epoch [1/5], Step [3090/6250], Loss: 1.4770\n",
      "Epoch [1/5], Step [3100/6250], Loss: 3.5192\n",
      "Epoch [1/5], Step [3110/6250], Loss: 2.3147\n",
      "Epoch [1/5], Step [3120/6250], Loss: 2.4044\n",
      "Epoch [1/5], Step [3130/6250], Loss: 1.6457\n",
      "Epoch [1/5], Step [3140/6250], Loss: 3.0657\n",
      "Epoch [1/5], Step [3150/6250], Loss: 3.8689\n",
      "Epoch [1/5], Step [3160/6250], Loss: 2.7180\n",
      "Epoch [1/5], Step [3170/6250], Loss: 2.5949\n",
      "Epoch [1/5], Step [3180/6250], Loss: 4.0393\n",
      "Epoch [1/5], Step [3190/6250], Loss: 1.9095\n",
      "Epoch [1/5], Step [3200/6250], Loss: 3.1671\n",
      "Epoch [1/5], Step [3210/6250], Loss: 2.1785\n",
      "Epoch [1/5], Step [3220/6250], Loss: 2.1011\n",
      "Epoch [1/5], Step [3230/6250], Loss: 2.5538\n",
      "Epoch [1/5], Step [3240/6250], Loss: 2.6783\n",
      "Epoch [1/5], Step [3250/6250], Loss: 4.2349\n",
      "Epoch [1/5], Step [3260/6250], Loss: 2.4755\n",
      "Epoch [1/5], Step [3270/6250], Loss: 2.2513\n",
      "Epoch [1/5], Step [3280/6250], Loss: 2.0991\n",
      "Epoch [1/5], Step [3290/6250], Loss: 3.0579\n",
      "Epoch [1/5], Step [3300/6250], Loss: 3.2179\n",
      "Epoch [1/5], Step [3310/6250], Loss: 2.3390\n",
      "Epoch [1/5], Step [3320/6250], Loss: 1.5093\n",
      "Epoch [1/5], Step [3330/6250], Loss: 2.0116\n",
      "Epoch [1/5], Step [3340/6250], Loss: 2.7749\n",
      "Epoch [1/5], Step [3350/6250], Loss: 2.6903\n",
      "Epoch [1/5], Step [3360/6250], Loss: 4.1869\n",
      "Epoch [1/5], Step [3370/6250], Loss: 2.2993\n",
      "Epoch [1/5], Step [3380/6250], Loss: 3.1287\n",
      "Epoch [1/5], Step [3390/6250], Loss: 2.3729\n",
      "Epoch [1/5], Step [3400/6250], Loss: 2.0516\n",
      "Epoch [1/5], Step [3410/6250], Loss: 2.1043\n",
      "Epoch [1/5], Step [3420/6250], Loss: 1.6123\n",
      "Epoch [1/5], Step [3430/6250], Loss: 1.9959\n",
      "Epoch [1/5], Step [3440/6250], Loss: 2.2697\n",
      "Epoch [1/5], Step [3450/6250], Loss: 2.0349\n",
      "Epoch [1/5], Step [3460/6250], Loss: 2.7910\n",
      "Epoch [1/5], Step [3470/6250], Loss: 2.3441\n",
      "Epoch [1/5], Step [3480/6250], Loss: 2.4943\n",
      "Epoch [1/5], Step [3490/6250], Loss: 3.6319\n",
      "Epoch [1/5], Step [3500/6250], Loss: 2.1082\n",
      "Epoch [1/5], Step [3510/6250], Loss: 2.7125\n",
      "Epoch [1/5], Step [3520/6250], Loss: 1.6135\n",
      "Epoch [1/5], Step [3530/6250], Loss: 2.6438\n",
      "Epoch [1/5], Step [3540/6250], Loss: 2.8990\n",
      "Epoch [1/5], Step [3550/6250], Loss: 1.4279\n",
      "Epoch [1/5], Step [3560/6250], Loss: 2.4921\n",
      "Epoch [1/5], Step [3570/6250], Loss: 2.8330\n",
      "Epoch [1/5], Step [3580/6250], Loss: 3.1793\n",
      "Epoch [1/5], Step [3590/6250], Loss: 2.7398\n",
      "Epoch [1/5], Step [3600/6250], Loss: 2.6571\n",
      "Epoch [1/5], Step [3610/6250], Loss: 2.0837\n",
      "Epoch [1/5], Step [3620/6250], Loss: 2.7059\n",
      "Epoch [1/5], Step [3630/6250], Loss: 1.6497\n",
      "Epoch [1/5], Step [3640/6250], Loss: 2.7469\n",
      "Epoch [1/5], Step [3650/6250], Loss: 2.6435\n",
      "Epoch [1/5], Step [3660/6250], Loss: 2.1020\n",
      "Epoch [1/5], Step [3670/6250], Loss: 2.4245\n",
      "Epoch [1/5], Step [3680/6250], Loss: 2.4560\n",
      "Epoch [1/5], Step [3690/6250], Loss: 2.2158\n",
      "Epoch [1/5], Step [3700/6250], Loss: 1.8356\n",
      "Epoch [1/5], Step [3710/6250], Loss: 2.8971\n",
      "Epoch [1/5], Step [3720/6250], Loss: 1.6073\n",
      "Epoch [1/5], Step [3730/6250], Loss: 2.5078\n",
      "Epoch [1/5], Step [3740/6250], Loss: 2.9470\n",
      "Epoch [1/5], Step [3750/6250], Loss: 2.0565\n",
      "Epoch [1/5], Step [3760/6250], Loss: 1.8383\n",
      "Epoch [1/5], Step [3770/6250], Loss: 2.4708\n",
      "Epoch [1/5], Step [3780/6250], Loss: 1.8172\n",
      "Epoch [1/5], Step [3790/6250], Loss: 2.6703\n",
      "Epoch [1/5], Step [3800/6250], Loss: 2.6140\n",
      "Epoch [1/5], Step [3810/6250], Loss: 2.0642\n",
      "Epoch [1/5], Step [3820/6250], Loss: 1.8363\n",
      "Epoch [1/5], Step [3830/6250], Loss: 2.7123\n",
      "Epoch [1/5], Step [3840/6250], Loss: 2.4792\n",
      "Epoch [1/5], Step [3850/6250], Loss: 2.3975\n",
      "Epoch [1/5], Step [3860/6250], Loss: 2.8983\n",
      "Epoch [1/5], Step [3870/6250], Loss: 2.5304\n",
      "Epoch [1/5], Step [3880/6250], Loss: 3.2343\n",
      "Epoch [1/5], Step [3890/6250], Loss: 2.1450\n",
      "Epoch [1/5], Step [3900/6250], Loss: 2.5460\n",
      "Epoch [1/5], Step [3910/6250], Loss: 1.9711\n",
      "Epoch [1/5], Step [3920/6250], Loss: 1.7345\n",
      "Epoch [1/5], Step [3930/6250], Loss: 1.8103\n",
      "Epoch [1/5], Step [3940/6250], Loss: 2.0614\n",
      "Epoch [1/5], Step [3950/6250], Loss: 2.9670\n",
      "Epoch [1/5], Step [3960/6250], Loss: 1.7071\n",
      "Epoch [1/5], Step [3970/6250], Loss: 2.6938\n",
      "Epoch [1/5], Step [3980/6250], Loss: 4.3284\n",
      "Epoch [1/5], Step [3990/6250], Loss: 2.1363\n",
      "Epoch [1/5], Step [4000/6250], Loss: 1.7408\n",
      "Epoch [1/5], Step [4010/6250], Loss: 2.1171\n",
      "Epoch [1/5], Step [4020/6250], Loss: 2.2297\n",
      "Epoch [1/5], Step [4030/6250], Loss: 2.8211\n",
      "Epoch [1/5], Step [4040/6250], Loss: 2.3229\n",
      "Epoch [1/5], Step [4050/6250], Loss: 2.6745\n",
      "Epoch [1/5], Step [4060/6250], Loss: 2.8227\n",
      "Epoch [1/5], Step [4070/6250], Loss: 1.8976\n",
      "Epoch [1/5], Step [4080/6250], Loss: 2.1913\n",
      "Epoch [1/5], Step [4090/6250], Loss: 2.0917\n",
      "Epoch [1/5], Step [4100/6250], Loss: 2.4134\n",
      "Epoch [1/5], Step [4110/6250], Loss: 1.5704\n",
      "Epoch [1/5], Step [4120/6250], Loss: 2.3977\n",
      "Epoch [1/5], Step [4130/6250], Loss: 1.9360\n",
      "Epoch [1/5], Step [4140/6250], Loss: 3.9679\n",
      "Epoch [1/5], Step [4150/6250], Loss: 2.3845\n",
      "Epoch [1/5], Step [4160/6250], Loss: 1.5431\n",
      "Epoch [1/5], Step [4170/6250], Loss: 1.9538\n",
      "Epoch [1/5], Step [4180/6250], Loss: 2.6541\n",
      "Epoch [1/5], Step [4190/6250], Loss: 3.0611\n",
      "Epoch [1/5], Step [4200/6250], Loss: 2.1911\n",
      "Epoch [1/5], Step [4210/6250], Loss: 2.1958\n",
      "Epoch [1/5], Step [4220/6250], Loss: 2.1282\n",
      "Epoch [1/5], Step [4230/6250], Loss: 2.7078\n",
      "Epoch [1/5], Step [4240/6250], Loss: 1.9635\n",
      "Epoch [1/5], Step [4250/6250], Loss: 1.7209\n",
      "Epoch [1/5], Step [4260/6250], Loss: 3.9755\n",
      "Epoch [1/5], Step [4270/6250], Loss: 2.5883\n",
      "Epoch [1/5], Step [4280/6250], Loss: 2.2544\n",
      "Epoch [1/5], Step [4290/6250], Loss: 4.9295\n",
      "Epoch [1/5], Step [4300/6250], Loss: 1.1015\n",
      "Epoch [1/5], Step [4310/6250], Loss: 2.5295\n",
      "Epoch [1/5], Step [4320/6250], Loss: 1.3252\n",
      "Epoch [1/5], Step [4330/6250], Loss: 1.6279\n",
      "Epoch [1/5], Step [4340/6250], Loss: 2.5706\n",
      "Epoch [1/5], Step [4350/6250], Loss: 2.2633\n",
      "Epoch [1/5], Step [4360/6250], Loss: 2.8431\n",
      "Epoch [1/5], Step [4370/6250], Loss: 1.9803\n",
      "Epoch [1/5], Step [4380/6250], Loss: 1.6318\n",
      "Epoch [1/5], Step [4390/6250], Loss: 2.2346\n",
      "Epoch [1/5], Step [4400/6250], Loss: 2.4266\n",
      "Epoch [1/5], Step [4410/6250], Loss: 2.6983\n",
      "Epoch [1/5], Step [4420/6250], Loss: 2.3767\n",
      "Epoch [1/5], Step [4430/6250], Loss: 2.5260\n",
      "Epoch [1/5], Step [4440/6250], Loss: 2.8144\n",
      "Epoch [1/5], Step [4450/6250], Loss: 1.6137\n",
      "Epoch [1/5], Step [4460/6250], Loss: 2.6612\n",
      "Epoch [1/5], Step [4470/6250], Loss: 2.2143\n",
      "Epoch [1/5], Step [4480/6250], Loss: 2.0127\n",
      "Epoch [1/5], Step [4490/6250], Loss: 3.7712\n",
      "Epoch [1/5], Step [4500/6250], Loss: 1.3877\n",
      "Epoch [1/5], Step [4510/6250], Loss: 2.8170\n",
      "Epoch [1/5], Step [4520/6250], Loss: 2.0941\n",
      "Epoch [1/5], Step [4530/6250], Loss: 1.9754\n",
      "Epoch [1/5], Step [4540/6250], Loss: 1.5592\n",
      "Epoch [1/5], Step [4550/6250], Loss: 2.3037\n",
      "Epoch [1/5], Step [4560/6250], Loss: 2.2980\n",
      "Epoch [1/5], Step [4570/6250], Loss: 1.8344\n",
      "Epoch [1/5], Step [4580/6250], Loss: 1.6147\n",
      "Epoch [1/5], Step [4590/6250], Loss: 1.6807\n",
      "Epoch [1/5], Step [4600/6250], Loss: 3.1025\n",
      "Epoch [1/5], Step [4610/6250], Loss: 2.2987\n",
      "Epoch [1/5], Step [4620/6250], Loss: 2.0838\n",
      "Epoch [1/5], Step [4630/6250], Loss: 2.8397\n",
      "Epoch [1/5], Step [4640/6250], Loss: 2.3013\n",
      "Epoch [1/5], Step [4650/6250], Loss: 1.7566\n",
      "Epoch [1/5], Step [4660/6250], Loss: 2.1705\n",
      "Epoch [1/5], Step [4670/6250], Loss: 2.6263\n",
      "Epoch [1/5], Step [4680/6250], Loss: 2.5707\n",
      "Epoch [1/5], Step [4690/6250], Loss: 1.9576\n",
      "Epoch [1/5], Step [4700/6250], Loss: 2.1006\n",
      "Epoch [1/5], Step [4710/6250], Loss: 3.6998\n",
      "Epoch [1/5], Step [4720/6250], Loss: 2.2122\n",
      "Epoch [1/5], Step [4730/6250], Loss: 2.2521\n",
      "Epoch [1/5], Step [4740/6250], Loss: 2.2911\n",
      "Epoch [1/5], Step [4750/6250], Loss: 2.4677\n",
      "Epoch [1/5], Step [4760/6250], Loss: 1.7648\n",
      "Epoch [1/5], Step [4770/6250], Loss: 1.6923\n",
      "Epoch [1/5], Step [4780/6250], Loss: 2.7659\n",
      "Epoch [1/5], Step [4790/6250], Loss: 2.1731\n",
      "Epoch [1/5], Step [4800/6250], Loss: 3.1198\n",
      "Epoch [1/5], Step [4810/6250], Loss: 2.1019\n",
      "Epoch [1/5], Step [4820/6250], Loss: 2.0897\n",
      "Epoch [1/5], Step [4830/6250], Loss: 2.5793\n",
      "Epoch [1/5], Step [4840/6250], Loss: 2.6358\n",
      "Epoch [1/5], Step [4850/6250], Loss: 2.6385\n",
      "Epoch [1/5], Step [4860/6250], Loss: 2.3584\n",
      "Epoch [1/5], Step [4870/6250], Loss: 2.7610\n",
      "Epoch [1/5], Step [4880/6250], Loss: 2.7577\n",
      "Epoch [1/5], Step [4890/6250], Loss: 3.5656\n",
      "Epoch [1/5], Step [4900/6250], Loss: 2.7928\n",
      "Epoch [1/5], Step [4910/6250], Loss: 2.8982\n",
      "Epoch [1/5], Step [4920/6250], Loss: 1.3104\n",
      "Epoch [1/5], Step [4930/6250], Loss: 2.2339\n",
      "Epoch [1/5], Step [4940/6250], Loss: 1.9219\n",
      "Epoch [1/5], Step [4950/6250], Loss: 3.6012\n",
      "Epoch [1/5], Step [4960/6250], Loss: 1.7552\n",
      "Epoch [1/5], Step [4970/6250], Loss: 1.3895\n",
      "Epoch [1/5], Step [4980/6250], Loss: 3.1152\n",
      "Epoch [1/5], Step [4990/6250], Loss: 2.5908\n",
      "Epoch [1/5], Step [5000/6250], Loss: 1.7232\n",
      "Epoch [1/5], Step [5010/6250], Loss: 1.8567\n",
      "Epoch [1/5], Step [5020/6250], Loss: 2.6577\n",
      "Epoch [1/5], Step [5030/6250], Loss: 1.8516\n",
      "Epoch [1/5], Step [5040/6250], Loss: 2.2195\n",
      "Epoch [1/5], Step [5050/6250], Loss: 1.5360\n",
      "Epoch [1/5], Step [5060/6250], Loss: 2.1874\n",
      "Epoch [1/5], Step [5070/6250], Loss: 1.6609\n",
      "Epoch [1/5], Step [5080/6250], Loss: 1.9380\n",
      "Epoch [1/5], Step [5090/6250], Loss: 2.5094\n",
      "Epoch [1/5], Step [5100/6250], Loss: 1.5357\n",
      "Epoch [1/5], Step [5110/6250], Loss: 1.3802\n",
      "Epoch [1/5], Step [5120/6250], Loss: 3.8007\n",
      "Epoch [1/5], Step [5130/6250], Loss: 1.3743\n",
      "Epoch [1/5], Step [5140/6250], Loss: 2.5400\n",
      "Epoch [1/5], Step [5150/6250], Loss: 2.1648\n",
      "Epoch [1/5], Step [5160/6250], Loss: 2.6075\n",
      "Epoch [1/5], Step [5170/6250], Loss: 2.3452\n",
      "Epoch [1/5], Step [5180/6250], Loss: 2.7382\n",
      "Epoch [1/5], Step [5190/6250], Loss: 2.0953\n",
      "Epoch [1/5], Step [5200/6250], Loss: 3.0433\n",
      "Epoch [1/5], Step [5210/6250], Loss: 3.1785\n",
      "Epoch [1/5], Step [5220/6250], Loss: 1.8133\n",
      "Epoch [1/5], Step [5230/6250], Loss: 2.0002\n",
      "Epoch [1/5], Step [5240/6250], Loss: 1.5783\n",
      "Epoch [1/5], Step [5250/6250], Loss: 2.6018\n",
      "Epoch [1/5], Step [5260/6250], Loss: 1.2724\n",
      "Epoch [1/5], Step [5270/6250], Loss: 1.4295\n",
      "Epoch [1/5], Step [5280/6250], Loss: 2.1271\n",
      "Epoch [1/5], Step [5290/6250], Loss: 2.8606\n",
      "Epoch [1/5], Step [5300/6250], Loss: 2.1031\n",
      "Epoch [1/5], Step [5310/6250], Loss: 2.6289\n",
      "Epoch [1/5], Step [5320/6250], Loss: 3.4698\n",
      "Epoch [1/5], Step [5330/6250], Loss: 2.5987\n",
      "Epoch [1/5], Step [5340/6250], Loss: 3.7916\n",
      "Epoch [1/5], Step [5350/6250], Loss: 3.0079\n",
      "Epoch [1/5], Step [5360/6250], Loss: 1.9827\n",
      "Epoch [1/5], Step [5370/6250], Loss: 1.8773\n",
      "Epoch [1/5], Step [5380/6250], Loss: 2.3014\n",
      "Epoch [1/5], Step [5390/6250], Loss: 3.2557\n",
      "Epoch [1/5], Step [5400/6250], Loss: 2.0847\n",
      "Epoch [1/5], Step [5410/6250], Loss: 3.1416\n",
      "Epoch [1/5], Step [5420/6250], Loss: 3.3698\n",
      "Epoch [1/5], Step [5430/6250], Loss: 3.5117\n",
      "Epoch [1/5], Step [5440/6250], Loss: 3.9786\n",
      "Epoch [1/5], Step [5450/6250], Loss: 1.7386\n",
      "Epoch [1/5], Step [5460/6250], Loss: 3.3607\n",
      "Epoch [1/5], Step [5470/6250], Loss: 2.0366\n",
      "Epoch [1/5], Step [5480/6250], Loss: 2.8372\n",
      "Epoch [1/5], Step [5490/6250], Loss: 1.0352\n",
      "Epoch [1/5], Step [5500/6250], Loss: 2.7756\n",
      "Epoch [1/5], Step [5510/6250], Loss: 2.7446\n",
      "Epoch [1/5], Step [5520/6250], Loss: 2.0341\n",
      "Epoch [1/5], Step [5530/6250], Loss: 1.9794\n",
      "Epoch [1/5], Step [5540/6250], Loss: 2.2892\n",
      "Epoch [1/5], Step [5550/6250], Loss: 2.5018\n",
      "Epoch [1/5], Step [5560/6250], Loss: 2.1278\n",
      "Epoch [1/5], Step [5570/6250], Loss: 2.2737\n",
      "Epoch [1/5], Step [5580/6250], Loss: 1.9478\n",
      "Epoch [1/5], Step [5590/6250], Loss: 1.9174\n",
      "Epoch [1/5], Step [5600/6250], Loss: 3.0389\n",
      "Epoch [1/5], Step [5610/6250], Loss: 1.7256\n",
      "Epoch [1/5], Step [5620/6250], Loss: 1.6407\n",
      "Epoch [1/5], Step [5630/6250], Loss: 1.8896\n",
      "Epoch [1/5], Step [5640/6250], Loss: 1.7130\n",
      "Epoch [1/5], Step [5650/6250], Loss: 1.8701\n",
      "Epoch [1/5], Step [5660/6250], Loss: 2.0437\n",
      "Epoch [1/5], Step [5670/6250], Loss: 3.4175\n",
      "Epoch [1/5], Step [5680/6250], Loss: 2.5210\n",
      "Epoch [1/5], Step [5690/6250], Loss: 2.6562\n",
      "Epoch [1/5], Step [5700/6250], Loss: 2.4506\n",
      "Epoch [1/5], Step [5710/6250], Loss: 3.8232\n",
      "Epoch [1/5], Step [5720/6250], Loss: 2.0711\n",
      "Epoch [1/5], Step [5730/6250], Loss: 2.3821\n",
      "Epoch [1/5], Step [5740/6250], Loss: 1.2746\n",
      "Epoch [1/5], Step [5750/6250], Loss: 3.1879\n",
      "Epoch [1/5], Step [5760/6250], Loss: 3.7268\n",
      "Epoch [1/5], Step [5770/6250], Loss: 3.7327\n",
      "Epoch [1/5], Step [5780/6250], Loss: 2.4768\n",
      "Epoch [1/5], Step [5790/6250], Loss: 2.6049\n",
      "Epoch [1/5], Step [5800/6250], Loss: 2.7879\n",
      "Epoch [1/5], Step [5810/6250], Loss: 2.5399\n",
      "Epoch [1/5], Step [5820/6250], Loss: 3.7990\n",
      "Epoch [1/5], Step [5830/6250], Loss: 2.3974\n",
      "Epoch [1/5], Step [5840/6250], Loss: 2.0629\n",
      "Epoch [1/5], Step [5850/6250], Loss: 3.0897\n",
      "Epoch [1/5], Step [5860/6250], Loss: 2.9936\n",
      "Epoch [1/5], Step [5870/6250], Loss: 2.8915\n",
      "Epoch [1/5], Step [5880/6250], Loss: 1.9920\n",
      "Epoch [1/5], Step [5890/6250], Loss: 2.7983\n",
      "Epoch [1/5], Step [5900/6250], Loss: 2.5317\n",
      "Epoch [1/5], Step [5910/6250], Loss: 2.9475\n",
      "Epoch [1/5], Step [5920/6250], Loss: 1.9069\n",
      "Epoch [1/5], Step [5930/6250], Loss: 1.6614\n",
      "Epoch [1/5], Step [5940/6250], Loss: 3.4079\n",
      "Epoch [1/5], Step [5950/6250], Loss: 2.8182\n",
      "Epoch [1/5], Step [5960/6250], Loss: 4.0454\n",
      "Epoch [1/5], Step [5970/6250], Loss: 1.7486\n",
      "Epoch [1/5], Step [5980/6250], Loss: 2.3447\n",
      "Epoch [1/5], Step [5990/6250], Loss: 1.6287\n",
      "Epoch [1/5], Step [6000/6250], Loss: 2.2040\n",
      "Epoch [1/5], Step [6010/6250], Loss: 3.1045\n",
      "Epoch [1/5], Step [6020/6250], Loss: 3.1404\n",
      "Epoch [1/5], Step [6030/6250], Loss: 3.2190\n",
      "Epoch [1/5], Step [6040/6250], Loss: 2.8437\n",
      "Epoch [1/5], Step [6050/6250], Loss: 3.0431\n",
      "Epoch [1/5], Step [6060/6250], Loss: 2.8314\n",
      "Epoch [1/5], Step [6070/6250], Loss: 2.4627\n",
      "Epoch [1/5], Step [6080/6250], Loss: 2.0086\n",
      "Epoch [1/5], Step [6090/6250], Loss: 2.3622\n",
      "Epoch [1/5], Step [6100/6250], Loss: 1.7449\n",
      "Epoch [1/5], Step [6110/6250], Loss: 1.7583\n",
      "Epoch [1/5], Step [6120/6250], Loss: 4.5029\n",
      "Epoch [1/5], Step [6130/6250], Loss: 4.4392\n",
      "Epoch [1/5], Step [6140/6250], Loss: 2.5764\n",
      "Epoch [1/5], Step [6150/6250], Loss: 2.2533\n",
      "Epoch [1/5], Step [6160/6250], Loss: 2.1927\n",
      "Epoch [1/5], Step [6170/6250], Loss: 2.8308\n",
      "Epoch [1/5], Step [6180/6250], Loss: 1.2575\n",
      "Epoch [1/5], Step [6190/6250], Loss: 2.5134\n",
      "Epoch [1/5], Step [6200/6250], Loss: 1.8657\n",
      "Epoch [1/5], Step [6210/6250], Loss: 2.5524\n",
      "Epoch [1/5], Step [6220/6250], Loss: 2.0571\n",
      "Epoch [1/5], Step [6230/6250], Loss: 2.7470\n",
      "Epoch [1/5], Step [6240/6250], Loss: 2.3964\n",
      "Epoch [1/5], Step [6250/6250], Loss: 2.2413\n",
      "Epoch 1, Loss: 2.2413065433502197\n",
      "Epoch [2/5], Step [10/6250], Loss: 1.7103\n",
      "Epoch [2/5], Step [20/6250], Loss: 2.1310\n",
      "Epoch [2/5], Step [30/6250], Loss: 2.4786\n",
      "Epoch [2/5], Step [40/6250], Loss: 1.3675\n",
      "Epoch [2/5], Step [50/6250], Loss: 2.1661\n",
      "Epoch [2/5], Step [60/6250], Loss: 2.2102\n",
      "Epoch [2/5], Step [70/6250], Loss: 2.1046\n",
      "Epoch [2/5], Step [80/6250], Loss: 2.9691\n",
      "Epoch [2/5], Step [90/6250], Loss: 1.4907\n",
      "Epoch [2/5], Step [100/6250], Loss: 1.4819\n",
      "Epoch [2/5], Step [110/6250], Loss: 1.8869\n",
      "Epoch [2/5], Step [120/6250], Loss: 2.3290\n",
      "Epoch [2/5], Step [130/6250], Loss: 3.3926\n",
      "Epoch [2/5], Step [140/6250], Loss: 1.9633\n",
      "Epoch [2/5], Step [150/6250], Loss: 2.5175\n",
      "Epoch [2/5], Step [160/6250], Loss: 2.4561\n",
      "Epoch [2/5], Step [170/6250], Loss: 3.1155\n",
      "Epoch [2/5], Step [180/6250], Loss: 4.0603\n",
      "Epoch [2/5], Step [190/6250], Loss: 3.3141\n",
      "Epoch [2/5], Step [200/6250], Loss: 1.5327\n",
      "Epoch [2/5], Step [210/6250], Loss: 1.6226\n",
      "Epoch [2/5], Step [220/6250], Loss: 3.2951\n",
      "Epoch [2/5], Step [230/6250], Loss: 1.4283\n",
      "Epoch [2/5], Step [240/6250], Loss: 2.4330\n",
      "Epoch [2/5], Step [250/6250], Loss: 1.7443\n",
      "Epoch [2/5], Step [260/6250], Loss: 2.0741\n",
      "Epoch [2/5], Step [270/6250], Loss: 2.2163\n",
      "Epoch [2/5], Step [280/6250], Loss: 1.7129\n",
      "Epoch [2/5], Step [290/6250], Loss: 1.8399\n",
      "Epoch [2/5], Step [300/6250], Loss: 2.5963\n",
      "Epoch [2/5], Step [310/6250], Loss: 2.2394\n",
      "Epoch [2/5], Step [320/6250], Loss: 2.2640\n",
      "Epoch [2/5], Step [330/6250], Loss: 1.4024\n",
      "Epoch [2/5], Step [340/6250], Loss: 2.2749\n",
      "Epoch [2/5], Step [350/6250], Loss: 2.2290\n",
      "Epoch [2/5], Step [360/6250], Loss: 1.3556\n",
      "Epoch [2/5], Step [370/6250], Loss: 1.9146\n",
      "Epoch [2/5], Step [380/6250], Loss: 2.1774\n",
      "Epoch [2/5], Step [390/6250], Loss: 1.5078\n",
      "Epoch [2/5], Step [400/6250], Loss: 1.1401\n",
      "Epoch [2/5], Step [410/6250], Loss: 2.1304\n",
      "Epoch [2/5], Step [420/6250], Loss: 2.4002\n",
      "Epoch [2/5], Step [430/6250], Loss: 1.3878\n",
      "Epoch [2/5], Step [440/6250], Loss: 2.9641\n",
      "Epoch [2/5], Step [450/6250], Loss: 1.3127\n",
      "Epoch [2/5], Step [460/6250], Loss: 2.4116\n",
      "Epoch [2/5], Step [470/6250], Loss: 2.6057\n",
      "Epoch [2/5], Step [480/6250], Loss: 2.0372\n",
      "Epoch [2/5], Step [490/6250], Loss: 1.7273\n",
      "Epoch [2/5], Step [500/6250], Loss: 2.4554\n",
      "Epoch [2/5], Step [510/6250], Loss: 2.8613\n",
      "Epoch [2/5], Step [520/6250], Loss: 3.4410\n",
      "Epoch [2/5], Step [530/6250], Loss: 1.9493\n",
      "Epoch [2/5], Step [540/6250], Loss: 2.9171\n",
      "Epoch [2/5], Step [550/6250], Loss: 1.5833\n",
      "Epoch [2/5], Step [560/6250], Loss: 1.8172\n",
      "Epoch [2/5], Step [570/6250], Loss: 3.0570\n",
      "Epoch [2/5], Step [580/6250], Loss: 2.7459\n",
      "Epoch [2/5], Step [590/6250], Loss: 2.0383\n",
      "Epoch [2/5], Step [600/6250], Loss: 1.9152\n",
      "Epoch [2/5], Step [610/6250], Loss: 2.4659\n",
      "Epoch [2/5], Step [620/6250], Loss: 2.2255\n",
      "Epoch [2/5], Step [630/6250], Loss: 2.9521\n",
      "Epoch [2/5], Step [640/6250], Loss: 2.4597\n",
      "Epoch [2/5], Step [650/6250], Loss: 2.2511\n",
      "Epoch [2/5], Step [660/6250], Loss: 1.3912\n",
      "Epoch [2/5], Step [670/6250], Loss: 2.2809\n",
      "Epoch [2/5], Step [680/6250], Loss: 1.7239\n",
      "Epoch [2/5], Step [690/6250], Loss: 1.6123\n",
      "Epoch [2/5], Step [700/6250], Loss: 1.6971\n",
      "Epoch [2/5], Step [710/6250], Loss: 2.5230\n",
      "Epoch [2/5], Step [720/6250], Loss: 2.1197\n",
      "Epoch [2/5], Step [730/6250], Loss: 1.3990\n",
      "Epoch [2/5], Step [740/6250], Loss: 2.1254\n",
      "Epoch [2/5], Step [750/6250], Loss: 2.5430\n",
      "Epoch [2/5], Step [760/6250], Loss: 1.5870\n",
      "Epoch [2/5], Step [770/6250], Loss: 1.6368\n",
      "Epoch [2/5], Step [780/6250], Loss: 2.3887\n",
      "Epoch [2/5], Step [790/6250], Loss: 3.5745\n",
      "Epoch [2/5], Step [800/6250], Loss: 2.7989\n",
      "Epoch [2/5], Step [810/6250], Loss: 1.9710\n",
      "Epoch [2/5], Step [820/6250], Loss: 3.3353\n",
      "Epoch [2/5], Step [830/6250], Loss: 1.3033\n",
      "Epoch [2/5], Step [840/6250], Loss: 2.5165\n",
      "Epoch [2/5], Step [850/6250], Loss: 2.2952\n",
      "Epoch [2/5], Step [860/6250], Loss: 2.5654\n",
      "Epoch [2/5], Step [870/6250], Loss: 2.4166\n",
      "Epoch [2/5], Step [880/6250], Loss: 2.0794\n",
      "Epoch [2/5], Step [890/6250], Loss: 2.4188\n",
      "Epoch [2/5], Step [900/6250], Loss: 3.8653\n",
      "Epoch [2/5], Step [910/6250], Loss: 2.1784\n",
      "Epoch [2/5], Step [920/6250], Loss: 1.4900\n",
      "Epoch [2/5], Step [930/6250], Loss: 1.6460\n",
      "Epoch [2/5], Step [940/6250], Loss: 3.0629\n",
      "Epoch [2/5], Step [950/6250], Loss: 2.1784\n",
      "Epoch [2/5], Step [960/6250], Loss: 2.1124\n",
      "Epoch [2/5], Step [970/6250], Loss: 1.8485\n",
      "Epoch [2/5], Step [980/6250], Loss: 3.0347\n",
      "Epoch [2/5], Step [990/6250], Loss: 2.3218\n",
      "Epoch [2/5], Step [1000/6250], Loss: 1.9605\n",
      "Epoch [2/5], Step [1010/6250], Loss: 2.2097\n",
      "Epoch [2/5], Step [1020/6250], Loss: 1.9517\n",
      "Epoch [2/5], Step [1030/6250], Loss: 3.0536\n",
      "Epoch [2/5], Step [1040/6250], Loss: 2.2026\n",
      "Epoch [2/5], Step [1050/6250], Loss: 1.9385\n",
      "Epoch [2/5], Step [1060/6250], Loss: 2.2820\n",
      "Epoch [2/5], Step [1070/6250], Loss: 1.7726\n",
      "Epoch [2/5], Step [1080/6250], Loss: 1.9527\n",
      "Epoch [2/5], Step [1090/6250], Loss: 3.8255\n",
      "Epoch [2/5], Step [1100/6250], Loss: 1.6673\n",
      "Epoch [2/5], Step [1110/6250], Loss: 2.6443\n",
      "Epoch [2/5], Step [1120/6250], Loss: 3.0733\n",
      "Epoch [2/5], Step [1130/6250], Loss: 1.5019\n",
      "Epoch [2/5], Step [1140/6250], Loss: 1.9842\n",
      "Epoch [2/5], Step [1150/6250], Loss: 1.6269\n",
      "Epoch [2/5], Step [1160/6250], Loss: 1.7553\n",
      "Epoch [2/5], Step [1170/6250], Loss: 2.9268\n",
      "Epoch [2/5], Step [1180/6250], Loss: 2.3947\n",
      "Epoch [2/5], Step [1190/6250], Loss: 2.5364\n",
      "Epoch [2/5], Step [1200/6250], Loss: 1.9752\n",
      "Epoch [2/5], Step [1210/6250], Loss: 2.7071\n",
      "Epoch [2/5], Step [1220/6250], Loss: 4.3433\n",
      "Epoch [2/5], Step [1230/6250], Loss: 1.3468\n",
      "Epoch [2/5], Step [1240/6250], Loss: 1.3784\n",
      "Epoch [2/5], Step [1250/6250], Loss: 1.7931\n",
      "Epoch [2/5], Step [1260/6250], Loss: 2.9401\n",
      "Epoch [2/5], Step [1270/6250], Loss: 1.9000\n",
      "Epoch [2/5], Step [1280/6250], Loss: 3.7384\n",
      "Epoch [2/5], Step [1290/6250], Loss: 2.1559\n",
      "Epoch [2/5], Step [1300/6250], Loss: 1.6809\n",
      "Epoch [2/5], Step [1310/6250], Loss: 2.4736\n",
      "Epoch [2/5], Step [1320/6250], Loss: 3.2702\n",
      "Epoch [2/5], Step [1330/6250], Loss: 1.2366\n",
      "Epoch [2/5], Step [1340/6250], Loss: 1.9614\n",
      "Epoch [2/5], Step [1350/6250], Loss: 2.7186\n",
      "Epoch [2/5], Step [1360/6250], Loss: 2.3149\n",
      "Epoch [2/5], Step [1370/6250], Loss: 1.8303\n",
      "Epoch [2/5], Step [1380/6250], Loss: 1.9676\n",
      "Epoch [2/5], Step [1390/6250], Loss: 2.7457\n",
      "Epoch [2/5], Step [1400/6250], Loss: 2.9819\n",
      "Epoch [2/5], Step [1410/6250], Loss: 1.5799\n",
      "Epoch [2/5], Step [1420/6250], Loss: 2.6713\n",
      "Epoch [2/5], Step [1430/6250], Loss: 3.0909\n",
      "Epoch [2/5], Step [1440/6250], Loss: 2.4015\n",
      "Epoch [2/5], Step [1450/6250], Loss: 2.5307\n",
      "Epoch [2/5], Step [1460/6250], Loss: 2.7839\n",
      "Epoch [2/5], Step [1470/6250], Loss: 1.7867\n",
      "Epoch [2/5], Step [1480/6250], Loss: 2.9974\n",
      "Epoch [2/5], Step [1490/6250], Loss: 1.9138\n",
      "Epoch [2/5], Step [1500/6250], Loss: 1.5309\n",
      "Epoch [2/5], Step [1510/6250], Loss: 1.3167\n",
      "Epoch [2/5], Step [1520/6250], Loss: 4.0278\n",
      "Epoch [2/5], Step [1530/6250], Loss: 2.0796\n",
      "Epoch [2/5], Step [1540/6250], Loss: 2.1095\n",
      "Epoch [2/5], Step [1550/6250], Loss: 1.2812\n",
      "Epoch [2/5], Step [1560/6250], Loss: 1.9020\n",
      "Epoch [2/5], Step [1570/6250], Loss: 1.7467\n",
      "Epoch [2/5], Step [1580/6250], Loss: 2.1214\n",
      "Epoch [2/5], Step [1590/6250], Loss: 1.7425\n",
      "Epoch [2/5], Step [1600/6250], Loss: 1.9898\n",
      "Epoch [2/5], Step [1610/6250], Loss: 1.4799\n",
      "Epoch [2/5], Step [1620/6250], Loss: 3.2415\n",
      "Epoch [2/5], Step [1630/6250], Loss: 1.4927\n",
      "Epoch [2/5], Step [1640/6250], Loss: 2.8513\n",
      "Epoch [2/5], Step [1650/6250], Loss: 2.2312\n",
      "Epoch [2/5], Step [1660/6250], Loss: 2.8056\n",
      "Epoch [2/5], Step [1670/6250], Loss: 2.9116\n",
      "Epoch [2/5], Step [1680/6250], Loss: 1.6578\n",
      "Epoch [2/5], Step [1690/6250], Loss: 2.0822\n",
      "Epoch [2/5], Step [1700/6250], Loss: 2.5003\n",
      "Epoch [2/5], Step [1710/6250], Loss: 1.8804\n",
      "Epoch [2/5], Step [1720/6250], Loss: 1.5114\n",
      "Epoch [2/5], Step [1730/6250], Loss: 2.5027\n",
      "Epoch [2/5], Step [1740/6250], Loss: 2.0953\n",
      "Epoch [2/5], Step [1750/6250], Loss: 2.1194\n",
      "Epoch [2/5], Step [1760/6250], Loss: 2.4718\n",
      "Epoch [2/5], Step [1770/6250], Loss: 2.3396\n",
      "Epoch [2/5], Step [1780/6250], Loss: 2.5508\n",
      "Epoch [2/5], Step [1790/6250], Loss: 2.7032\n",
      "Epoch [2/5], Step [1800/6250], Loss: 1.6173\n",
      "Epoch [2/5], Step [1810/6250], Loss: 2.7688\n",
      "Epoch [2/5], Step [1820/6250], Loss: 1.8676\n",
      "Epoch [2/5], Step [1830/6250], Loss: 1.7949\n",
      "Epoch [2/5], Step [1840/6250], Loss: 3.0304\n",
      "Epoch [2/5], Step [1850/6250], Loss: 1.8673\n",
      "Epoch [2/5], Step [1860/6250], Loss: 3.3778\n",
      "Epoch [2/5], Step [1870/6250], Loss: 1.9302\n",
      "Epoch [2/5], Step [1880/6250], Loss: 1.5330\n",
      "Epoch [2/5], Step [1890/6250], Loss: 2.7150\n",
      "Epoch [2/5], Step [1900/6250], Loss: 2.4210\n",
      "Epoch [2/5], Step [1910/6250], Loss: 2.5491\n",
      "Epoch [2/5], Step [1920/6250], Loss: 3.3568\n",
      "Epoch [2/5], Step [1930/6250], Loss: 2.7223\n",
      "Epoch [2/5], Step [1940/6250], Loss: 1.5159\n",
      "Epoch [2/5], Step [1950/6250], Loss: 1.7087\n",
      "Epoch [2/5], Step [1960/6250], Loss: 2.0446\n",
      "Epoch [2/5], Step [1970/6250], Loss: 1.7681\n",
      "Epoch [2/5], Step [1980/6250], Loss: 1.2543\n",
      "Epoch [2/5], Step [1990/6250], Loss: 2.6444\n",
      "Epoch [2/5], Step [2000/6250], Loss: 1.6457\n",
      "Epoch [2/5], Step [2010/6250], Loss: 2.5270\n",
      "Epoch [2/5], Step [2020/6250], Loss: 3.4537\n",
      "Epoch [2/5], Step [2030/6250], Loss: 3.1359\n",
      "Epoch [2/5], Step [2040/6250], Loss: 1.1914\n",
      "Epoch [2/5], Step [2050/6250], Loss: 1.9561\n",
      "Epoch [2/5], Step [2060/6250], Loss: 2.9970\n",
      "Epoch [2/5], Step [2070/6250], Loss: 2.0094\n",
      "Epoch [2/5], Step [2080/6250], Loss: 2.6634\n",
      "Epoch [2/5], Step [2090/6250], Loss: 2.4558\n",
      "Epoch [2/5], Step [2100/6250], Loss: 2.1491\n",
      "Epoch [2/5], Step [2110/6250], Loss: 2.8601\n",
      "Epoch [2/5], Step [2120/6250], Loss: 2.2378\n",
      "Epoch [2/5], Step [2130/6250], Loss: 1.6452\n",
      "Epoch [2/5], Step [2140/6250], Loss: 1.6019\n",
      "Epoch [2/5], Step [2150/6250], Loss: 1.7215\n",
      "Epoch [2/5], Step [2160/6250], Loss: 1.8577\n",
      "Epoch [2/5], Step [2170/6250], Loss: 1.9024\n",
      "Epoch [2/5], Step [2180/6250], Loss: 1.7955\n",
      "Epoch [2/5], Step [2190/6250], Loss: 2.9170\n",
      "Epoch [2/5], Step [2200/6250], Loss: 1.9065\n",
      "Epoch [2/5], Step [2210/6250], Loss: 1.9742\n",
      "Epoch [2/5], Step [2220/6250], Loss: 2.7652\n",
      "Epoch [2/5], Step [2230/6250], Loss: 2.1732\n",
      "Epoch [2/5], Step [2240/6250], Loss: 2.1875\n",
      "Epoch [2/5], Step [2250/6250], Loss: 1.7508\n",
      "Epoch [2/5], Step [2260/6250], Loss: 1.9550\n",
      "Epoch [2/5], Step [2270/6250], Loss: 4.1092\n",
      "Epoch [2/5], Step [2280/6250], Loss: 2.7007\n",
      "Epoch [2/5], Step [2290/6250], Loss: 1.4967\n",
      "Epoch [2/5], Step [2300/6250], Loss: 2.3309\n",
      "Epoch [2/5], Step [2310/6250], Loss: 1.2084\n",
      "Epoch [2/5], Step [2320/6250], Loss: 1.7649\n",
      "Epoch [2/5], Step [2330/6250], Loss: 4.1966\n",
      "Epoch [2/5], Step [2340/6250], Loss: 1.7153\n",
      "Epoch [2/5], Step [2350/6250], Loss: 1.2191\n",
      "Epoch [2/5], Step [2360/6250], Loss: 1.4789\n",
      "Epoch [2/5], Step [2370/6250], Loss: 1.8970\n",
      "Epoch [2/5], Step [2380/6250], Loss: 2.0219\n",
      "Epoch [2/5], Step [2390/6250], Loss: 2.8231\n",
      "Epoch [2/5], Step [2400/6250], Loss: 1.4378\n",
      "Epoch [2/5], Step [2410/6250], Loss: 1.9113\n",
      "Epoch [2/5], Step [2420/6250], Loss: 2.7716\n",
      "Epoch [2/5], Step [2430/6250], Loss: 2.3288\n",
      "Epoch [2/5], Step [2440/6250], Loss: 1.4435\n",
      "Epoch [2/5], Step [2450/6250], Loss: 1.9251\n",
      "Epoch [2/5], Step [2460/6250], Loss: 1.5713\n",
      "Epoch [2/5], Step [2470/6250], Loss: 2.0905\n",
      "Epoch [2/5], Step [2480/6250], Loss: 2.1784\n",
      "Epoch [2/5], Step [2490/6250], Loss: 2.1102\n",
      "Epoch [2/5], Step [2500/6250], Loss: 1.5395\n",
      "Epoch [2/5], Step [2510/6250], Loss: 2.3542\n",
      "Epoch [2/5], Step [2520/6250], Loss: 1.5683\n",
      "Epoch [2/5], Step [2530/6250], Loss: 2.8964\n",
      "Epoch [2/5], Step [2540/6250], Loss: 1.4665\n",
      "Epoch [2/5], Step [2550/6250], Loss: 1.4487\n",
      "Epoch [2/5], Step [2560/6250], Loss: 2.0547\n",
      "Epoch [2/5], Step [2570/6250], Loss: 1.5520\n",
      "Epoch [2/5], Step [2580/6250], Loss: 2.2610\n",
      "Epoch [2/5], Step [2590/6250], Loss: 3.1294\n",
      "Epoch [2/5], Step [2600/6250], Loss: 2.0226\n",
      "Epoch [2/5], Step [2610/6250], Loss: 2.2208\n",
      "Epoch [2/5], Step [2620/6250], Loss: 2.0260\n",
      "Epoch [2/5], Step [2630/6250], Loss: 1.9396\n",
      "Epoch [2/5], Step [2640/6250], Loss: 1.5775\n",
      "Epoch [2/5], Step [2650/6250], Loss: 2.2524\n",
      "Epoch [2/5], Step [2660/6250], Loss: 3.2407\n",
      "Epoch [2/5], Step [2670/6250], Loss: 1.5435\n",
      "Epoch [2/5], Step [2680/6250], Loss: 1.9989\n",
      "Epoch [2/5], Step [2690/6250], Loss: 2.1983\n",
      "Epoch [2/5], Step [2700/6250], Loss: 2.4016\n",
      "Epoch [2/5], Step [2710/6250], Loss: 2.4782\n",
      "Epoch [2/5], Step [2720/6250], Loss: 2.2726\n",
      "Epoch [2/5], Step [2730/6250], Loss: 1.4271\n",
      "Epoch [2/5], Step [2740/6250], Loss: 2.9625\n",
      "Epoch [2/5], Step [2750/6250], Loss: 2.5417\n",
      "Epoch [2/5], Step [2760/6250], Loss: 2.5226\n",
      "Epoch [2/5], Step [2770/6250], Loss: 2.3387\n",
      "Epoch [2/5], Step [2780/6250], Loss: 3.2248\n",
      "Epoch [2/5], Step [2790/6250], Loss: 1.7748\n",
      "Epoch [2/5], Step [2800/6250], Loss: 3.2943\n",
      "Epoch [2/5], Step [2810/6250], Loss: 3.7126\n",
      "Epoch [2/5], Step [2820/6250], Loss: 3.3331\n",
      "Epoch [2/5], Step [2830/6250], Loss: 3.2504\n",
      "Epoch [2/5], Step [2840/6250], Loss: 2.5299\n",
      "Epoch [2/5], Step [2850/6250], Loss: 2.9161\n",
      "Epoch [2/5], Step [2860/6250], Loss: 1.6896\n",
      "Epoch [2/5], Step [2870/6250], Loss: 1.7361\n",
      "Epoch [2/5], Step [2880/6250], Loss: 2.9340\n",
      "Epoch [2/5], Step [2890/6250], Loss: 2.5245\n",
      "Epoch [2/5], Step [2900/6250], Loss: 1.9655\n",
      "Epoch [2/5], Step [2910/6250], Loss: 2.4268\n",
      "Epoch [2/5], Step [2920/6250], Loss: 2.4203\n",
      "Epoch [2/5], Step [2930/6250], Loss: 1.2372\n",
      "Epoch [2/5], Step [2940/6250], Loss: 2.9288\n",
      "Epoch [2/5], Step [2950/6250], Loss: 2.7494\n",
      "Epoch [2/5], Step [2960/6250], Loss: 1.4798\n",
      "Epoch [2/5], Step [2970/6250], Loss: 2.6442\n",
      "Epoch [2/5], Step [2980/6250], Loss: 1.8281\n",
      "Epoch [2/5], Step [2990/6250], Loss: 2.0975\n",
      "Epoch [2/5], Step [3000/6250], Loss: 3.4274\n",
      "Epoch [2/5], Step [3010/6250], Loss: 3.3140\n",
      "Epoch [2/5], Step [3020/6250], Loss: 2.5075\n",
      "Epoch [2/5], Step [3030/6250], Loss: 1.7646\n",
      "Epoch [2/5], Step [3040/6250], Loss: 2.4548\n",
      "Epoch [2/5], Step [3050/6250], Loss: 2.2449\n",
      "Epoch [2/5], Step [3060/6250], Loss: 1.9003\n",
      "Epoch [2/5], Step [3070/6250], Loss: 1.9239\n",
      "Epoch [2/5], Step [3080/6250], Loss: 2.1724\n",
      "Epoch [2/5], Step [3090/6250], Loss: 2.2121\n",
      "Epoch [2/5], Step [3100/6250], Loss: 1.5486\n",
      "Epoch [2/5], Step [3110/6250], Loss: 1.4700\n",
      "Epoch [2/5], Step [3120/6250], Loss: 3.2466\n",
      "Epoch [2/5], Step [3130/6250], Loss: 2.3017\n",
      "Epoch [2/5], Step [3140/6250], Loss: 2.5475\n",
      "Epoch [2/5], Step [3150/6250], Loss: 1.8912\n",
      "Epoch [2/5], Step [3160/6250], Loss: 1.9484\n",
      "Epoch [2/5], Step [3170/6250], Loss: 1.9195\n",
      "Epoch [2/5], Step [3180/6250], Loss: 2.2488\n",
      "Epoch [2/5], Step [3190/6250], Loss: 2.5107\n",
      "Epoch [2/5], Step [3200/6250], Loss: 2.5976\n",
      "Epoch [2/5], Step [3210/6250], Loss: 2.2384\n",
      "Epoch [2/5], Step [3220/6250], Loss: 2.3302\n",
      "Epoch [2/5], Step [3230/6250], Loss: 1.9699\n",
      "Epoch [2/5], Step [3240/6250], Loss: 3.8322\n",
      "Epoch [2/5], Step [3250/6250], Loss: 1.9039\n",
      "Epoch [2/5], Step [3260/6250], Loss: 1.7582\n",
      "Epoch [2/5], Step [3270/6250], Loss: 1.7778\n",
      "Epoch [2/5], Step [3280/6250], Loss: 2.7576\n",
      "Epoch [2/5], Step [3290/6250], Loss: 1.9696\n",
      "Epoch [2/5], Step [3300/6250], Loss: 2.6184\n",
      "Epoch [2/5], Step [3310/6250], Loss: 1.9612\n",
      "Epoch [2/5], Step [3320/6250], Loss: 2.8508\n",
      "Epoch [2/5], Step [3330/6250], Loss: 3.3001\n",
      "Epoch [2/5], Step [3340/6250], Loss: 2.8365\n",
      "Epoch [2/5], Step [3350/6250], Loss: 2.1602\n",
      "Epoch [2/5], Step [3360/6250], Loss: 1.8334\n",
      "Epoch [2/5], Step [3370/6250], Loss: 2.4261\n",
      "Epoch [2/5], Step [3380/6250], Loss: 2.6763\n",
      "Epoch [2/5], Step [3390/6250], Loss: 2.0140\n",
      "Epoch [2/5], Step [3400/6250], Loss: 1.0713\n",
      "Epoch [2/5], Step [3410/6250], Loss: 3.8386\n",
      "Epoch [2/5], Step [3420/6250], Loss: 2.2131\n",
      "Epoch [2/5], Step [3430/6250], Loss: 2.3012\n",
      "Epoch [2/5], Step [3440/6250], Loss: 1.9896\n",
      "Epoch [2/5], Step [3450/6250], Loss: 2.5668\n",
      "Epoch [2/5], Step [3460/6250], Loss: 1.5865\n",
      "Epoch [2/5], Step [3470/6250], Loss: 3.4942\n",
      "Epoch [2/5], Step [3480/6250], Loss: 1.9237\n",
      "Epoch [2/5], Step [3490/6250], Loss: 2.5251\n",
      "Epoch [2/5], Step [3500/6250], Loss: 3.8550\n",
      "Epoch [2/5], Step [3510/6250], Loss: 1.7225\n",
      "Epoch [2/5], Step [3520/6250], Loss: 2.0710\n",
      "Epoch [2/5], Step [3530/6250], Loss: 1.7221\n",
      "Epoch [2/5], Step [3540/6250], Loss: 2.3023\n",
      "Epoch [2/5], Step [3550/6250], Loss: 2.2667\n",
      "Epoch [2/5], Step [3560/6250], Loss: 1.7976\n",
      "Epoch [2/5], Step [3570/6250], Loss: 2.1349\n",
      "Epoch [2/5], Step [3580/6250], Loss: 1.5875\n",
      "Epoch [2/5], Step [3590/6250], Loss: 1.9264\n",
      "Epoch [2/5], Step [3600/6250], Loss: 2.8978\n",
      "Epoch [2/5], Step [3610/6250], Loss: 2.8608\n",
      "Epoch [2/5], Step [3620/6250], Loss: 1.5252\n",
      "Epoch [2/5], Step [3630/6250], Loss: 1.9161\n",
      "Epoch [2/5], Step [3640/6250], Loss: 2.1664\n",
      "Epoch [2/5], Step [3650/6250], Loss: 2.1370\n",
      "Epoch [2/5], Step [3660/6250], Loss: 2.1281\n",
      "Epoch [2/5], Step [3670/6250], Loss: 2.0381\n",
      "Epoch [2/5], Step [3680/6250], Loss: 1.7814\n",
      "Epoch [2/5], Step [3690/6250], Loss: 2.2969\n",
      "Epoch [2/5], Step [3700/6250], Loss: 2.1445\n",
      "Epoch [2/5], Step [3710/6250], Loss: 3.1043\n",
      "Epoch [2/5], Step [3720/6250], Loss: 1.7788\n",
      "Epoch [2/5], Step [3730/6250], Loss: 2.2171\n",
      "Epoch [2/5], Step [3740/6250], Loss: 2.0564\n",
      "Epoch [2/5], Step [3750/6250], Loss: 2.8648\n",
      "Epoch [2/5], Step [3760/6250], Loss: 1.4040\n",
      "Epoch [2/5], Step [3770/6250], Loss: 1.3634\n",
      "Epoch [2/5], Step [3780/6250], Loss: 2.2567\n",
      "Epoch [2/5], Step [3790/6250], Loss: 2.4254\n",
      "Epoch [2/5], Step [3800/6250], Loss: 1.5560\n",
      "Epoch [2/5], Step [3810/6250], Loss: 3.7984\n",
      "Epoch [2/5], Step [3820/6250], Loss: 3.3138\n",
      "Epoch [2/5], Step [3830/6250], Loss: 2.5400\n",
      "Epoch [2/5], Step [3840/6250], Loss: 3.2145\n",
      "Epoch [2/5], Step [3850/6250], Loss: 1.7296\n",
      "Epoch [2/5], Step [3860/6250], Loss: 2.8047\n",
      "Epoch [2/5], Step [3870/6250], Loss: 1.6834\n",
      "Epoch [2/5], Step [3880/6250], Loss: 2.2202\n",
      "Epoch [2/5], Step [3890/6250], Loss: 2.0172\n",
      "Epoch [2/5], Step [3900/6250], Loss: 3.2908\n",
      "Epoch [2/5], Step [3910/6250], Loss: 2.4222\n",
      "Epoch [2/5], Step [3920/6250], Loss: 1.8658\n",
      "Epoch [2/5], Step [3930/6250], Loss: 2.5822\n",
      "Epoch [2/5], Step [3940/6250], Loss: 2.8923\n",
      "Epoch [2/5], Step [3950/6250], Loss: 1.8008\n",
      "Epoch [2/5], Step [3960/6250], Loss: 1.9888\n",
      "Epoch [2/5], Step [3970/6250], Loss: 2.2241\n",
      "Epoch [2/5], Step [3980/6250], Loss: 1.9237\n",
      "Epoch [2/5], Step [3990/6250], Loss: 1.8508\n",
      "Epoch [2/5], Step [4000/6250], Loss: 3.4243\n",
      "Epoch [2/5], Step [4010/6250], Loss: 3.1553\n",
      "Epoch [2/5], Step [4020/6250], Loss: 1.6806\n",
      "Epoch [2/5], Step [4030/6250], Loss: 2.6736\n",
      "Epoch [2/5], Step [4040/6250], Loss: 2.9281\n",
      "Epoch [2/5], Step [4050/6250], Loss: 2.5802\n",
      "Epoch [2/5], Step [4060/6250], Loss: 2.0445\n",
      "Epoch [2/5], Step [4070/6250], Loss: 3.4774\n",
      "Epoch [2/5], Step [4080/6250], Loss: 2.1177\n",
      "Epoch [2/5], Step [4090/6250], Loss: 2.0692\n",
      "Epoch [2/5], Step [4100/6250], Loss: 1.9202\n",
      "Epoch [2/5], Step [4110/6250], Loss: 1.8599\n",
      "Epoch [2/5], Step [4120/6250], Loss: 1.9987\n",
      "Epoch [2/5], Step [4130/6250], Loss: 1.9112\n",
      "Epoch [2/5], Step [4140/6250], Loss: 1.6031\n",
      "Epoch [2/5], Step [4150/6250], Loss: 2.0120\n",
      "Epoch [2/5], Step [4160/6250], Loss: 1.3410\n",
      "Epoch [2/5], Step [4170/6250], Loss: 2.9626\n",
      "Epoch [2/5], Step [4180/6250], Loss: 2.4912\n",
      "Epoch [2/5], Step [4190/6250], Loss: 1.9629\n",
      "Epoch [2/5], Step [4200/6250], Loss: 3.2953\n",
      "Epoch [2/5], Step [4210/6250], Loss: 2.3291\n",
      "Epoch [2/5], Step [4220/6250], Loss: 1.8368\n",
      "Epoch [2/5], Step [4230/6250], Loss: 2.1292\n",
      "Epoch [2/5], Step [4240/6250], Loss: 3.5815\n",
      "Epoch [2/5], Step [4250/6250], Loss: 3.8968\n",
      "Epoch [2/5], Step [4260/6250], Loss: 1.6040\n",
      "Epoch [2/5], Step [4270/6250], Loss: 1.1947\n",
      "Epoch [2/5], Step [4280/6250], Loss: 1.9149\n",
      "Epoch [2/5], Step [4290/6250], Loss: 1.8934\n",
      "Epoch [2/5], Step [4300/6250], Loss: 2.3361\n",
      "Epoch [2/5], Step [4310/6250], Loss: 2.6335\n",
      "Epoch [2/5], Step [4320/6250], Loss: 1.9186\n",
      "Epoch [2/5], Step [4330/6250], Loss: 1.8287\n",
      "Epoch [2/5], Step [4340/6250], Loss: 2.5385\n",
      "Epoch [2/5], Step [4350/6250], Loss: 1.7863\n",
      "Epoch [2/5], Step [4360/6250], Loss: 3.0128\n",
      "Epoch [2/5], Step [4370/6250], Loss: 1.1948\n",
      "Epoch [2/5], Step [4380/6250], Loss: 2.9119\n",
      "Epoch [2/5], Step [4390/6250], Loss: 2.2170\n",
      "Epoch [2/5], Step [4400/6250], Loss: 2.5047\n",
      "Epoch [2/5], Step [4410/6250], Loss: 2.7108\n",
      "Epoch [2/5], Step [4420/6250], Loss: 1.7157\n",
      "Epoch [2/5], Step [4430/6250], Loss: 1.9239\n",
      "Epoch [2/5], Step [4440/6250], Loss: 3.0694\n",
      "Epoch [2/5], Step [4450/6250], Loss: 2.2026\n",
      "Epoch [2/5], Step [4460/6250], Loss: 3.0271\n",
      "Epoch [2/5], Step [4470/6250], Loss: 2.2770\n",
      "Epoch [2/5], Step [4480/6250], Loss: 1.8406\n",
      "Epoch [2/5], Step [4490/6250], Loss: 2.8693\n",
      "Epoch [2/5], Step [4500/6250], Loss: 1.2362\n",
      "Epoch [2/5], Step [4510/6250], Loss: 1.6684\n",
      "Epoch [2/5], Step [4520/6250], Loss: 2.2028\n",
      "Epoch [2/5], Step [4530/6250], Loss: 2.2635\n",
      "Epoch [2/5], Step [4540/6250], Loss: 2.7704\n",
      "Epoch [2/5], Step [4550/6250], Loss: 3.0575\n",
      "Epoch [2/5], Step [4560/6250], Loss: 2.2762\n",
      "Epoch [2/5], Step [4570/6250], Loss: 1.8018\n",
      "Epoch [2/5], Step [4580/6250], Loss: 3.2942\n",
      "Epoch [2/5], Step [4590/6250], Loss: 1.7672\n",
      "Epoch [2/5], Step [4600/6250], Loss: 1.7390\n",
      "Epoch [2/5], Step [4610/6250], Loss: 2.1739\n",
      "Epoch [2/5], Step [4620/6250], Loss: 2.7605\n",
      "Epoch [2/5], Step [4630/6250], Loss: 1.0944\n",
      "Epoch [2/5], Step [4640/6250], Loss: 1.8232\n",
      "Epoch [2/5], Step [4650/6250], Loss: 2.1774\n",
      "Epoch [2/5], Step [4660/6250], Loss: 1.8952\n",
      "Epoch [2/5], Step [4670/6250], Loss: 2.7343\n",
      "Epoch [2/5], Step [4680/6250], Loss: 1.6656\n",
      "Epoch [2/5], Step [4690/6250], Loss: 1.5109\n",
      "Epoch [2/5], Step [4700/6250], Loss: 2.1758\n",
      "Epoch [2/5], Step [4710/6250], Loss: 2.4216\n",
      "Epoch [2/5], Step [4720/6250], Loss: 2.4255\n",
      "Epoch [2/5], Step [4730/6250], Loss: 2.3583\n",
      "Epoch [2/5], Step [4740/6250], Loss: 1.4204\n",
      "Epoch [2/5], Step [4750/6250], Loss: 2.6405\n",
      "Epoch [2/5], Step [4760/6250], Loss: 3.6561\n",
      "Epoch [2/5], Step [4770/6250], Loss: 1.7711\n",
      "Epoch [2/5], Step [4780/6250], Loss: 2.0609\n",
      "Epoch [2/5], Step [4790/6250], Loss: 1.9504\n",
      "Epoch [2/5], Step [4800/6250], Loss: 2.0979\n",
      "Epoch [2/5], Step [4810/6250], Loss: 2.4956\n",
      "Epoch [2/5], Step [4820/6250], Loss: 1.9341\n",
      "Epoch [2/5], Step [4830/6250], Loss: 2.9261\n",
      "Epoch [2/5], Step [4840/6250], Loss: 1.6382\n",
      "Epoch [2/5], Step [4850/6250], Loss: 2.9871\n",
      "Epoch [2/5], Step [4860/6250], Loss: 1.4828\n",
      "Epoch [2/5], Step [4870/6250], Loss: 2.8113\n",
      "Epoch [2/5], Step [4880/6250], Loss: 2.3905\n",
      "Epoch [2/5], Step [4890/6250], Loss: 1.4546\n",
      "Epoch [2/5], Step [4900/6250], Loss: 2.3398\n",
      "Epoch [2/5], Step [4910/6250], Loss: 3.3557\n",
      "Epoch [2/5], Step [4920/6250], Loss: 2.2987\n",
      "Epoch [2/5], Step [4930/6250], Loss: 2.0927\n",
      "Epoch [2/5], Step [4940/6250], Loss: 2.6597\n",
      "Epoch [2/5], Step [4950/6250], Loss: 1.6086\n",
      "Epoch [2/5], Step [4960/6250], Loss: 2.6487\n",
      "Epoch [2/5], Step [4970/6250], Loss: 2.0019\n",
      "Epoch [2/5], Step [4980/6250], Loss: 3.4668\n",
      "Epoch [2/5], Step [4990/6250], Loss: 2.4606\n",
      "Epoch [2/5], Step [5000/6250], Loss: 2.5265\n",
      "Epoch [2/5], Step [5010/6250], Loss: 1.6570\n",
      "Epoch [2/5], Step [5020/6250], Loss: 1.5562\n",
      "Epoch [2/5], Step [5030/6250], Loss: 1.6564\n",
      "Epoch [2/5], Step [5040/6250], Loss: 1.7663\n",
      "Epoch [2/5], Step [5050/6250], Loss: 2.2610\n",
      "Epoch [2/5], Step [5060/6250], Loss: 2.6255\n",
      "Epoch [2/5], Step [5070/6250], Loss: 3.0368\n",
      "Epoch [2/5], Step [5080/6250], Loss: 2.6304\n",
      "Epoch [2/5], Step [5090/6250], Loss: 2.5764\n",
      "Epoch [2/5], Step [5100/6250], Loss: 2.3518\n",
      "Epoch [2/5], Step [5110/6250], Loss: 3.4506\n",
      "Epoch [2/5], Step [5120/6250], Loss: 3.0235\n",
      "Epoch [2/5], Step [5130/6250], Loss: 1.7023\n",
      "Epoch [2/5], Step [5140/6250], Loss: 2.4635\n",
      "Epoch [2/5], Step [5150/6250], Loss: 3.5303\n",
      "Epoch [2/5], Step [5160/6250], Loss: 2.8731\n",
      "Epoch [2/5], Step [5170/6250], Loss: 2.3995\n",
      "Epoch [2/5], Step [5180/6250], Loss: 2.5052\n",
      "Epoch [2/5], Step [5190/6250], Loss: 1.9546\n",
      "Epoch [2/5], Step [5200/6250], Loss: 2.3467\n",
      "Epoch [2/5], Step [5210/6250], Loss: 2.5924\n",
      "Epoch [2/5], Step [5220/6250], Loss: 1.9323\n",
      "Epoch [2/5], Step [5230/6250], Loss: 2.3141\n",
      "Epoch [2/5], Step [5240/6250], Loss: 2.8594\n",
      "Epoch [2/5], Step [5250/6250], Loss: 2.3090\n",
      "Epoch [2/5], Step [5260/6250], Loss: 3.8035\n",
      "Epoch [2/5], Step [5270/6250], Loss: 4.2237\n",
      "Epoch [2/5], Step [5280/6250], Loss: 1.8243\n",
      "Epoch [2/5], Step [5290/6250], Loss: 1.7854\n",
      "Epoch [2/5], Step [5300/6250], Loss: 1.9665\n",
      "Epoch [2/5], Step [5310/6250], Loss: 2.0101\n",
      "Epoch [2/5], Step [5320/6250], Loss: 1.7081\n",
      "Epoch [2/5], Step [5330/6250], Loss: 1.4248\n",
      "Epoch [2/5], Step [5340/6250], Loss: 2.8385\n",
      "Epoch [2/5], Step [5350/6250], Loss: 2.8835\n",
      "Epoch [2/5], Step [5360/6250], Loss: 2.2400\n",
      "Epoch [2/5], Step [5370/6250], Loss: 1.7765\n",
      "Epoch [2/5], Step [5380/6250], Loss: 1.7433\n",
      "Epoch [2/5], Step [5390/6250], Loss: 1.5741\n",
      "Epoch [2/5], Step [5400/6250], Loss: 3.9037\n",
      "Epoch [2/5], Step [5410/6250], Loss: 3.2490\n",
      "Epoch [2/5], Step [5420/6250], Loss: 1.2973\n",
      "Epoch [2/5], Step [5430/6250], Loss: 2.4148\n",
      "Epoch [2/5], Step [5440/6250], Loss: 2.0579\n",
      "Epoch [2/5], Step [5450/6250], Loss: 1.3336\n",
      "Epoch [2/5], Step [5460/6250], Loss: 2.5437\n",
      "Epoch [2/5], Step [5470/6250], Loss: 1.9150\n",
      "Epoch [2/5], Step [5480/6250], Loss: 1.7060\n",
      "Epoch [2/5], Step [5490/6250], Loss: 1.5440\n",
      "Epoch [2/5], Step [5500/6250], Loss: 1.7777\n",
      "Epoch [2/5], Step [5510/6250], Loss: 3.4705\n",
      "Epoch [2/5], Step [5520/6250], Loss: 1.5617\n",
      "Epoch [2/5], Step [5530/6250], Loss: 2.6472\n",
      "Epoch [2/5], Step [5540/6250], Loss: 2.0233\n",
      "Epoch [2/5], Step [5550/6250], Loss: 1.8543\n",
      "Epoch [2/5], Step [5560/6250], Loss: 2.0378\n",
      "Epoch [2/5], Step [5570/6250], Loss: 1.5249\n",
      "Epoch [2/5], Step [5580/6250], Loss: 1.9966\n",
      "Epoch [2/5], Step [5590/6250], Loss: 2.5374\n",
      "Epoch [2/5], Step [5600/6250], Loss: 2.4744\n",
      "Epoch [2/5], Step [5610/6250], Loss: 2.0536\n",
      "Epoch [2/5], Step [5620/6250], Loss: 1.5065\n",
      "Epoch [2/5], Step [5630/6250], Loss: 1.6625\n",
      "Epoch [2/5], Step [5640/6250], Loss: 2.2277\n",
      "Epoch [2/5], Step [5650/6250], Loss: 2.5200\n",
      "Epoch [2/5], Step [5660/6250], Loss: 1.9875\n",
      "Epoch [2/5], Step [5670/6250], Loss: 2.6746\n",
      "Epoch [2/5], Step [5680/6250], Loss: 2.2408\n",
      "Epoch [2/5], Step [5690/6250], Loss: 1.6073\n",
      "Epoch [2/5], Step [5700/6250], Loss: 2.1688\n",
      "Epoch [2/5], Step [5710/6250], Loss: 3.7013\n",
      "Epoch [2/5], Step [5720/6250], Loss: 1.9900\n",
      "Epoch [2/5], Step [5730/6250], Loss: 1.9704\n",
      "Epoch [2/5], Step [5740/6250], Loss: 2.9064\n",
      "Epoch [2/5], Step [5750/6250], Loss: 2.9134\n",
      "Epoch [2/5], Step [5760/6250], Loss: 2.4957\n",
      "Epoch [2/5], Step [5770/6250], Loss: 3.0399\n",
      "Epoch [2/5], Step [5780/6250], Loss: 3.2058\n",
      "Epoch [2/5], Step [5790/6250], Loss: 2.9813\n",
      "Epoch [2/5], Step [5800/6250], Loss: 2.2172\n",
      "Epoch [2/5], Step [5810/6250], Loss: 2.1894\n",
      "Epoch [2/5], Step [5820/6250], Loss: 2.0588\n",
      "Epoch [2/5], Step [5830/6250], Loss: 2.1424\n",
      "Epoch [2/5], Step [5840/6250], Loss: 2.1457\n",
      "Epoch [2/5], Step [5850/6250], Loss: 3.4296\n",
      "Epoch [2/5], Step [5860/6250], Loss: 2.3461\n",
      "Epoch [2/5], Step [5870/6250], Loss: 1.4769\n",
      "Epoch [2/5], Step [5880/6250], Loss: 2.5901\n",
      "Epoch [2/5], Step [5890/6250], Loss: 2.6965\n",
      "Epoch [2/5], Step [5900/6250], Loss: 1.7353\n",
      "Epoch [2/5], Step [5910/6250], Loss: 1.8412\n",
      "Epoch [2/5], Step [5920/6250], Loss: 2.5801\n",
      "Epoch [2/5], Step [5930/6250], Loss: 1.6937\n",
      "Epoch [2/5], Step [5940/6250], Loss: 2.2958\n",
      "Epoch [2/5], Step [5950/6250], Loss: 2.4951\n",
      "Epoch [2/5], Step [5960/6250], Loss: 3.5372\n",
      "Epoch [2/5], Step [5970/6250], Loss: 2.0191\n",
      "Epoch [2/5], Step [5980/6250], Loss: 2.3641\n",
      "Epoch [2/5], Step [5990/6250], Loss: 2.0814\n",
      "Epoch [2/5], Step [6000/6250], Loss: 2.4135\n",
      "Epoch [2/5], Step [6010/6250], Loss: 1.7486\n",
      "Epoch [2/5], Step [6020/6250], Loss: 2.7933\n",
      "Epoch [2/5], Step [6030/6250], Loss: 1.8584\n",
      "Epoch [2/5], Step [6040/6250], Loss: 1.7831\n",
      "Epoch [2/5], Step [6050/6250], Loss: 2.6865\n",
      "Epoch [2/5], Step [6060/6250], Loss: 2.9129\n",
      "Epoch [2/5], Step [6070/6250], Loss: 2.1974\n",
      "Epoch [2/5], Step [6080/6250], Loss: 2.9884\n",
      "Epoch [2/5], Step [6090/6250], Loss: 2.0575\n",
      "Epoch [2/5], Step [6100/6250], Loss: 2.5401\n",
      "Epoch [2/5], Step [6110/6250], Loss: 1.8376\n",
      "Epoch [2/5], Step [6120/6250], Loss: 2.3814\n",
      "Epoch [2/5], Step [6130/6250], Loss: 1.6027\n",
      "Epoch [2/5], Step [6140/6250], Loss: 2.3603\n",
      "Epoch [2/5], Step [6150/6250], Loss: 2.3344\n",
      "Epoch [2/5], Step [6160/6250], Loss: 2.8393\n",
      "Epoch [2/5], Step [6170/6250], Loss: 2.4894\n",
      "Epoch [2/5], Step [6180/6250], Loss: 3.0931\n",
      "Epoch [2/5], Step [6190/6250], Loss: 3.0438\n",
      "Epoch [2/5], Step [6200/6250], Loss: 2.0952\n",
      "Epoch [2/5], Step [6210/6250], Loss: 2.4198\n",
      "Epoch [2/5], Step [6220/6250], Loss: 2.9101\n",
      "Epoch [2/5], Step [6230/6250], Loss: 2.5562\n",
      "Epoch [2/5], Step [6240/6250], Loss: 1.6288\n",
      "Epoch [2/5], Step [6250/6250], Loss: 2.9961\n",
      "Epoch 2, Loss: 2.9961392879486084\n",
      "Epoch [3/5], Step [10/6250], Loss: 2.4367\n",
      "Epoch [3/5], Step [20/6250], Loss: 1.7668\n",
      "Epoch [3/5], Step [30/6250], Loss: 1.7166\n",
      "Epoch [3/5], Step [40/6250], Loss: 2.5849\n",
      "Epoch [3/5], Step [50/6250], Loss: 2.5992\n",
      "Epoch [3/5], Step [60/6250], Loss: 2.2972\n",
      "Epoch [3/5], Step [70/6250], Loss: 1.2830\n",
      "Epoch [3/5], Step [80/6250], Loss: 2.0328\n",
      "Epoch [3/5], Step [90/6250], Loss: 1.9288\n",
      "Epoch [3/5], Step [100/6250], Loss: 2.3002\n",
      "Epoch [3/5], Step [110/6250], Loss: 1.9676\n",
      "Epoch [3/5], Step [120/6250], Loss: 2.0571\n",
      "Epoch [3/5], Step [130/6250], Loss: 2.1749\n",
      "Epoch [3/5], Step [140/6250], Loss: 1.6231\n",
      "Epoch [3/5], Step [150/6250], Loss: 1.8128\n",
      "Epoch [3/5], Step [160/6250], Loss: 1.3811\n",
      "Epoch [3/5], Step [170/6250], Loss: 1.5165\n",
      "Epoch [3/5], Step [180/6250], Loss: 1.9509\n",
      "Epoch [3/5], Step [190/6250], Loss: 2.0360\n",
      "Epoch [3/5], Step [200/6250], Loss: 2.5409\n",
      "Epoch [3/5], Step [210/6250], Loss: 1.3832\n",
      "Epoch [3/5], Step [220/6250], Loss: 1.9098\n",
      "Epoch [3/5], Step [230/6250], Loss: 1.9550\n",
      "Epoch [3/5], Step [240/6250], Loss: 2.6317\n",
      "Epoch [3/5], Step [250/6250], Loss: 1.9431\n",
      "Epoch [3/5], Step [260/6250], Loss: 1.6753\n",
      "Epoch [3/5], Step [270/6250], Loss: 2.4383\n",
      "Epoch [3/5], Step [280/6250], Loss: 2.0284\n",
      "Epoch [3/5], Step [290/6250], Loss: 2.1634\n",
      "Epoch [3/5], Step [300/6250], Loss: 2.4416\n",
      "Epoch [3/5], Step [310/6250], Loss: 2.2745\n",
      "Epoch [3/5], Step [320/6250], Loss: 1.2773\n",
      "Epoch [3/5], Step [330/6250], Loss: 2.2106\n",
      "Epoch [3/5], Step [340/6250], Loss: 2.1737\n",
      "Epoch [3/5], Step [350/6250], Loss: 2.1389\n",
      "Epoch [3/5], Step [360/6250], Loss: 2.1376\n",
      "Epoch [3/5], Step [370/6250], Loss: 3.0946\n",
      "Epoch [3/5], Step [380/6250], Loss: 1.6605\n",
      "Epoch [3/5], Step [390/6250], Loss: 1.2781\n",
      "Epoch [3/5], Step [400/6250], Loss: 1.5254\n",
      "Epoch [3/5], Step [410/6250], Loss: 1.1606\n",
      "Epoch [3/5], Step [420/6250], Loss: 2.8354\n",
      "Epoch [3/5], Step [430/6250], Loss: 2.7116\n",
      "Epoch [3/5], Step [440/6250], Loss: 1.7599\n",
      "Epoch [3/5], Step [450/6250], Loss: 1.6109\n",
      "Epoch [3/5], Step [460/6250], Loss: 1.5721\n",
      "Epoch [3/5], Step [470/6250], Loss: 1.7819\n",
      "Epoch [3/5], Step [480/6250], Loss: 1.8658\n",
      "Epoch [3/5], Step [490/6250], Loss: 2.6576\n",
      "Epoch [3/5], Step [500/6250], Loss: 1.7404\n",
      "Epoch [3/5], Step [510/6250], Loss: 2.4678\n",
      "Epoch [3/5], Step [520/6250], Loss: 1.7876\n",
      "Epoch [3/5], Step [530/6250], Loss: 1.4117\n",
      "Epoch [3/5], Step [540/6250], Loss: 2.1739\n",
      "Epoch [3/5], Step [550/6250], Loss: 1.7633\n",
      "Epoch [3/5], Step [560/6250], Loss: 2.1345\n",
      "Epoch [3/5], Step [570/6250], Loss: 1.8442\n",
      "Epoch [3/5], Step [580/6250], Loss: 1.7038\n",
      "Epoch [3/5], Step [590/6250], Loss: 2.3334\n",
      "Epoch [3/5], Step [600/6250], Loss: 2.2351\n",
      "Epoch [3/5], Step [610/6250], Loss: 2.4036\n",
      "Epoch [3/5], Step [620/6250], Loss: 2.6944\n",
      "Epoch [3/5], Step [630/6250], Loss: 3.1005\n",
      "Epoch [3/5], Step [640/6250], Loss: 1.3742\n",
      "Epoch [3/5], Step [650/6250], Loss: 1.3412\n",
      "Epoch [3/5], Step [660/6250], Loss: 2.3103\n",
      "Epoch [3/5], Step [670/6250], Loss: 2.6496\n",
      "Epoch [3/5], Step [680/6250], Loss: 2.4997\n",
      "Epoch [3/5], Step [690/6250], Loss: 1.7803\n",
      "Epoch [3/5], Step [700/6250], Loss: 2.0290\n",
      "Epoch [3/5], Step [710/6250], Loss: 2.3012\n",
      "Epoch [3/5], Step [720/6250], Loss: 1.5148\n",
      "Epoch [3/5], Step [730/6250], Loss: 2.0396\n",
      "Epoch [3/5], Step [740/6250], Loss: 1.8332\n",
      "Epoch [3/5], Step [750/6250], Loss: 2.6568\n",
      "Epoch [3/5], Step [760/6250], Loss: 2.3520\n",
      "Epoch [3/5], Step [770/6250], Loss: 1.5845\n",
      "Epoch [3/5], Step [780/6250], Loss: 2.4311\n",
      "Epoch [3/5], Step [790/6250], Loss: 2.0130\n",
      "Epoch [3/5], Step [800/6250], Loss: 1.5313\n",
      "Epoch [3/5], Step [810/6250], Loss: 1.5531\n",
      "Epoch [3/5], Step [820/6250], Loss: 2.0689\n",
      "Epoch [3/5], Step [830/6250], Loss: 1.9522\n",
      "Epoch [3/5], Step [840/6250], Loss: 2.4009\n",
      "Epoch [3/5], Step [850/6250], Loss: 2.6782\n",
      "Epoch [3/5], Step [860/6250], Loss: 1.3940\n",
      "Epoch [3/5], Step [870/6250], Loss: 2.5117\n",
      "Epoch [3/5], Step [880/6250], Loss: 2.3363\n",
      "Epoch [3/5], Step [890/6250], Loss: 1.7038\n",
      "Epoch [3/5], Step [900/6250], Loss: 2.1493\n",
      "Epoch [3/5], Step [910/6250], Loss: 2.1026\n",
      "Epoch [3/5], Step [920/6250], Loss: 1.9413\n",
      "Epoch [3/5], Step [930/6250], Loss: 2.6073\n",
      "Epoch [3/5], Step [940/6250], Loss: 2.2996\n",
      "Epoch [3/5], Step [950/6250], Loss: 2.4323\n",
      "Epoch [3/5], Step [960/6250], Loss: 1.1659\n",
      "Epoch [3/5], Step [970/6250], Loss: 2.4412\n",
      "Epoch [3/5], Step [980/6250], Loss: 2.4619\n",
      "Epoch [3/5], Step [990/6250], Loss: 2.0193\n",
      "Epoch [3/5], Step [1000/6250], Loss: 2.2347\n",
      "Epoch [3/5], Step [1010/6250], Loss: 3.1037\n",
      "Epoch [3/5], Step [1020/6250], Loss: 2.0649\n",
      "Epoch [3/5], Step [1030/6250], Loss: 1.7036\n",
      "Epoch [3/5], Step [1040/6250], Loss: 2.0922\n",
      "Epoch [3/5], Step [1050/6250], Loss: 1.6176\n",
      "Epoch [3/5], Step [1060/6250], Loss: 2.9035\n",
      "Epoch [3/5], Step [1070/6250], Loss: 1.8265\n",
      "Epoch [3/5], Step [1080/6250], Loss: 1.5343\n",
      "Epoch [3/5], Step [1090/6250], Loss: 1.4330\n",
      "Epoch [3/5], Step [1100/6250], Loss: 2.7098\n",
      "Epoch [3/5], Step [1110/6250], Loss: 2.5961\n",
      "Epoch [3/5], Step [1120/6250], Loss: 2.2184\n",
      "Epoch [3/5], Step [1130/6250], Loss: 2.4635\n",
      "Epoch [3/5], Step [1140/6250], Loss: 1.4116\n",
      "Epoch [3/5], Step [1150/6250], Loss: 1.7894\n",
      "Epoch [3/5], Step [1160/6250], Loss: 2.5703\n",
      "Epoch [3/5], Step [1170/6250], Loss: 1.6481\n",
      "Epoch [3/5], Step [1180/6250], Loss: 2.1712\n",
      "Epoch [3/5], Step [1190/6250], Loss: 2.3771\n",
      "Epoch [3/5], Step [1200/6250], Loss: 1.8234\n",
      "Epoch [3/5], Step [1210/6250], Loss: 1.5028\n",
      "Epoch [3/5], Step [1220/6250], Loss: 2.6474\n",
      "Epoch [3/5], Step [1230/6250], Loss: 1.7295\n",
      "Epoch [3/5], Step [1240/6250], Loss: 2.0564\n",
      "Epoch [3/5], Step [1250/6250], Loss: 2.0715\n",
      "Epoch [3/5], Step [1260/6250], Loss: 2.1810\n",
      "Epoch [3/5], Step [1270/6250], Loss: 1.9559\n",
      "Epoch [3/5], Step [1280/6250], Loss: 2.1741\n",
      "Epoch [3/5], Step [1290/6250], Loss: 2.5937\n",
      "Epoch [3/5], Step [1300/6250], Loss: 1.4636\n",
      "Epoch [3/5], Step [1310/6250], Loss: 1.9602\n",
      "Epoch [3/5], Step [1320/6250], Loss: 2.5563\n",
      "Epoch [3/5], Step [1330/6250], Loss: 2.2785\n",
      "Epoch [3/5], Step [1340/6250], Loss: 1.2345\n",
      "Epoch [3/5], Step [1350/6250], Loss: 2.2425\n",
      "Epoch [3/5], Step [1360/6250], Loss: 3.4700\n",
      "Epoch [3/5], Step [1370/6250], Loss: 1.6930\n",
      "Epoch [3/5], Step [1380/6250], Loss: 2.8075\n",
      "Epoch [3/5], Step [1390/6250], Loss: 1.5837\n",
      "Epoch [3/5], Step [1400/6250], Loss: 2.7478\n",
      "Epoch [3/5], Step [1410/6250], Loss: 1.3940\n",
      "Epoch [3/5], Step [1420/6250], Loss: 3.5193\n",
      "Epoch [3/5], Step [1430/6250], Loss: 2.5880\n",
      "Epoch [3/5], Step [1440/6250], Loss: 1.3515\n",
      "Epoch [3/5], Step [1450/6250], Loss: 1.4362\n",
      "Epoch [3/5], Step [1460/6250], Loss: 2.9377\n",
      "Epoch [3/5], Step [1470/6250], Loss: 2.3200\n",
      "Epoch [3/5], Step [1480/6250], Loss: 2.2998\n",
      "Epoch [3/5], Step [1490/6250], Loss: 2.7366\n",
      "Epoch [3/5], Step [1500/6250], Loss: 2.0856\n",
      "Epoch [3/5], Step [1510/6250], Loss: 2.3279\n",
      "Epoch [3/5], Step [1520/6250], Loss: 2.3960\n",
      "Epoch [3/5], Step [1530/6250], Loss: 1.4357\n",
      "Epoch [3/5], Step [1540/6250], Loss: 2.1045\n",
      "Epoch [3/5], Step [1550/6250], Loss: 1.5930\n",
      "Epoch [3/5], Step [1560/6250], Loss: 2.1697\n",
      "Epoch [3/5], Step [1570/6250], Loss: 2.3809\n",
      "Epoch [3/5], Step [1580/6250], Loss: 2.3520\n",
      "Epoch [3/5], Step [1590/6250], Loss: 2.0809\n",
      "Epoch [3/5], Step [1600/6250], Loss: 1.7738\n",
      "Epoch [3/5], Step [1610/6250], Loss: 1.9687\n",
      "Epoch [3/5], Step [1620/6250], Loss: 2.2732\n",
      "Epoch [3/5], Step [1630/6250], Loss: 2.4764\n",
      "Epoch [3/5], Step [1640/6250], Loss: 2.4062\n",
      "Epoch [3/5], Step [1650/6250], Loss: 1.5670\n",
      "Epoch [3/5], Step [1660/6250], Loss: 1.5793\n",
      "Epoch [3/5], Step [1670/6250], Loss: 2.4759\n",
      "Epoch [3/5], Step [1680/6250], Loss: 1.9973\n",
      "Epoch [3/5], Step [1690/6250], Loss: 2.4530\n",
      "Epoch [3/5], Step [1700/6250], Loss: 2.5730\n",
      "Epoch [3/5], Step [1710/6250], Loss: 2.1300\n",
      "Epoch [3/5], Step [1720/6250], Loss: 2.5328\n",
      "Epoch [3/5], Step [1730/6250], Loss: 2.5380\n",
      "Epoch [3/5], Step [1740/6250], Loss: 2.8583\n",
      "Epoch [3/5], Step [1750/6250], Loss: 1.0704\n",
      "Epoch [3/5], Step [1760/6250], Loss: 2.5416\n",
      "Epoch [3/5], Step [1770/6250], Loss: 3.0104\n",
      "Epoch [3/5], Step [1780/6250], Loss: 2.3724\n",
      "Epoch [3/5], Step [1790/6250], Loss: 1.8959\n",
      "Epoch [3/5], Step [1800/6250], Loss: 3.0381\n",
      "Epoch [3/5], Step [1810/6250], Loss: 2.6643\n",
      "Epoch [3/5], Step [1820/6250], Loss: 2.6708\n",
      "Epoch [3/5], Step [1830/6250], Loss: 1.9960\n",
      "Epoch [3/5], Step [1840/6250], Loss: 2.0594\n",
      "Epoch [3/5], Step [1850/6250], Loss: 2.4293\n",
      "Epoch [3/5], Step [1860/6250], Loss: 1.8365\n",
      "Epoch [3/5], Step [1870/6250], Loss: 1.8092\n",
      "Epoch [3/5], Step [1880/6250], Loss: 2.7503\n",
      "Epoch [3/5], Step [1890/6250], Loss: 1.7685\n",
      "Epoch [3/5], Step [1900/6250], Loss: 3.6644\n",
      "Epoch [3/5], Step [1910/6250], Loss: 2.2616\n",
      "Epoch [3/5], Step [1920/6250], Loss: 1.4477\n",
      "Epoch [3/5], Step [1930/6250], Loss: 1.6295\n",
      "Epoch [3/5], Step [1940/6250], Loss: 1.7429\n",
      "Epoch [3/5], Step [1950/6250], Loss: 1.3330\n",
      "Epoch [3/5], Step [1960/6250], Loss: 1.5855\n",
      "Epoch [3/5], Step [1970/6250], Loss: 2.1623\n",
      "Epoch [3/5], Step [1980/6250], Loss: 1.5355\n",
      "Epoch [3/5], Step [1990/6250], Loss: 2.2352\n",
      "Epoch [3/5], Step [2000/6250], Loss: 1.7776\n",
      "Epoch [3/5], Step [2010/6250], Loss: 1.6503\n",
      "Epoch [3/5], Step [2020/6250], Loss: 1.8256\n",
      "Epoch [3/5], Step [2030/6250], Loss: 1.8438\n",
      "Epoch [3/5], Step [2040/6250], Loss: 1.6096\n",
      "Epoch [3/5], Step [2050/6250], Loss: 2.5947\n",
      "Epoch [3/5], Step [2060/6250], Loss: 1.6056\n",
      "Epoch [3/5], Step [2070/6250], Loss: 1.9279\n",
      "Epoch [3/5], Step [2080/6250], Loss: 2.9822\n",
      "Epoch [3/5], Step [2090/6250], Loss: 1.7540\n",
      "Epoch [3/5], Step [2100/6250], Loss: 1.4033\n",
      "Epoch [3/5], Step [2110/6250], Loss: 2.4779\n",
      "Epoch [3/5], Step [2120/6250], Loss: 1.7589\n",
      "Epoch [3/5], Step [2130/6250], Loss: 2.9552\n",
      "Epoch [3/5], Step [2140/6250], Loss: 2.5178\n",
      "Epoch [3/5], Step [2150/6250], Loss: 2.0108\n",
      "Epoch [3/5], Step [2160/6250], Loss: 1.6836\n",
      "Epoch [3/5], Step [2170/6250], Loss: 1.6277\n",
      "Epoch [3/5], Step [2180/6250], Loss: 1.7164\n",
      "Epoch [3/5], Step [2190/6250], Loss: 1.6122\n",
      "Epoch [3/5], Step [2200/6250], Loss: 1.9148\n",
      "Epoch [3/5], Step [2210/6250], Loss: 1.6080\n",
      "Epoch [3/5], Step [2220/6250], Loss: 2.1199\n",
      "Epoch [3/5], Step [2230/6250], Loss: 2.4224\n",
      "Epoch [3/5], Step [2240/6250], Loss: 1.7392\n",
      "Epoch [3/5], Step [2250/6250], Loss: 2.2920\n",
      "Epoch [3/5], Step [2260/6250], Loss: 1.9751\n",
      "Epoch [3/5], Step [2270/6250], Loss: 1.9780\n",
      "Epoch [3/5], Step [2280/6250], Loss: 2.5452\n",
      "Epoch [3/5], Step [2290/6250], Loss: 1.1936\n",
      "Epoch [3/5], Step [2300/6250], Loss: 2.7114\n",
      "Epoch [3/5], Step [2310/6250], Loss: 1.7010\n",
      "Epoch [3/5], Step [2320/6250], Loss: 2.6665\n",
      "Epoch [3/5], Step [2330/6250], Loss: 2.2586\n",
      "Epoch [3/5], Step [2340/6250], Loss: 1.5612\n",
      "Epoch [3/5], Step [2350/6250], Loss: 2.9966\n",
      "Epoch [3/5], Step [2360/6250], Loss: 1.6334\n",
      "Epoch [3/5], Step [2370/6250], Loss: 1.5995\n",
      "Epoch [3/5], Step [2380/6250], Loss: 2.2352\n",
      "Epoch [3/5], Step [2390/6250], Loss: 2.7208\n",
      "Epoch [3/5], Step [2400/6250], Loss: 2.8679\n",
      "Epoch [3/5], Step [2410/6250], Loss: 1.2004\n",
      "Epoch [3/5], Step [2420/6250], Loss: 1.9145\n",
      "Epoch [3/5], Step [2430/6250], Loss: 2.0846\n",
      "Epoch [3/5], Step [2440/6250], Loss: 1.6175\n",
      "Epoch [3/5], Step [2450/6250], Loss: 1.6909\n",
      "Epoch [3/5], Step [2460/6250], Loss: 2.9090\n",
      "Epoch [3/5], Step [2470/6250], Loss: 2.6174\n",
      "Epoch [3/5], Step [2480/6250], Loss: 1.7375\n",
      "Epoch [3/5], Step [2490/6250], Loss: 2.6152\n",
      "Epoch [3/5], Step [2500/6250], Loss: 2.1405\n",
      "Epoch [3/5], Step [2510/6250], Loss: 3.3449\n",
      "Epoch [3/5], Step [2520/6250], Loss: 2.2210\n",
      "Epoch [3/5], Step [2530/6250], Loss: 2.2750\n",
      "Epoch [3/5], Step [2540/6250], Loss: 2.3131\n",
      "Epoch [3/5], Step [2550/6250], Loss: 1.3161\n",
      "Epoch [3/5], Step [2560/6250], Loss: 1.5570\n",
      "Epoch [3/5], Step [2570/6250], Loss: 3.0647\n",
      "Epoch [3/5], Step [2580/6250], Loss: 1.8651\n",
      "Epoch [3/5], Step [2590/6250], Loss: 2.9972\n",
      "Epoch [3/5], Step [2600/6250], Loss: 1.6614\n",
      "Epoch [3/5], Step [2610/6250], Loss: 2.1428\n",
      "Epoch [3/5], Step [2620/6250], Loss: 1.3035\n",
      "Epoch [3/5], Step [2630/6250], Loss: 1.7603\n",
      "Epoch [3/5], Step [2640/6250], Loss: 1.8253\n",
      "Epoch [3/5], Step [2650/6250], Loss: 1.9381\n",
      "Epoch [3/5], Step [2660/6250], Loss: 1.7344\n",
      "Epoch [3/5], Step [2670/6250], Loss: 1.6498\n",
      "Epoch [3/5], Step [2680/6250], Loss: 2.6432\n",
      "Epoch [3/5], Step [2690/6250], Loss: 1.8178\n",
      "Epoch [3/5], Step [2700/6250], Loss: 1.8558\n",
      "Epoch [3/5], Step [2710/6250], Loss: 2.7132\n",
      "Epoch [3/5], Step [2720/6250], Loss: 1.5568\n",
      "Epoch [3/5], Step [2730/6250], Loss: 2.1001\n",
      "Epoch [3/5], Step [2740/6250], Loss: 2.3924\n",
      "Epoch [3/5], Step [2750/6250], Loss: 1.2486\n",
      "Epoch [3/5], Step [2760/6250], Loss: 2.2764\n",
      "Epoch [3/5], Step [2770/6250], Loss: 2.7552\n",
      "Epoch [3/5], Step [2780/6250], Loss: 1.3890\n",
      "Epoch [3/5], Step [2790/6250], Loss: 2.9679\n",
      "Epoch [3/5], Step [2800/6250], Loss: 1.6874\n",
      "Epoch [3/5], Step [2810/6250], Loss: 2.6876\n",
      "Epoch [3/5], Step [2820/6250], Loss: 2.0761\n",
      "Epoch [3/5], Step [2830/6250], Loss: 2.7686\n",
      "Epoch [3/5], Step [2840/6250], Loss: 1.9561\n",
      "Epoch [3/5], Step [2850/6250], Loss: 1.8960\n",
      "Epoch [3/5], Step [2860/6250], Loss: 2.5191\n",
      "Epoch [3/5], Step [2870/6250], Loss: 1.7849\n",
      "Epoch [3/5], Step [2880/6250], Loss: 1.5120\n",
      "Epoch [3/5], Step [2890/6250], Loss: 1.6959\n",
      "Epoch [3/5], Step [2900/6250], Loss: 2.2064\n",
      "Epoch [3/5], Step [2910/6250], Loss: 1.5429\n",
      "Epoch [3/5], Step [2920/6250], Loss: 2.1009\n",
      "Epoch [3/5], Step [2930/6250], Loss: 2.4293\n",
      "Epoch [3/5], Step [2940/6250], Loss: 3.3443\n",
      "Epoch [3/5], Step [2950/6250], Loss: 2.0379\n",
      "Epoch [3/5], Step [2960/6250], Loss: 2.6435\n",
      "Epoch [3/5], Step [2970/6250], Loss: 2.5317\n",
      "Epoch [3/5], Step [2980/6250], Loss: 1.5126\n",
      "Epoch [3/5], Step [2990/6250], Loss: 1.7448\n",
      "Epoch [3/5], Step [3000/6250], Loss: 3.6125\n",
      "Epoch [3/5], Step [3010/6250], Loss: 3.1252\n",
      "Epoch [3/5], Step [3020/6250], Loss: 2.6765\n",
      "Epoch [3/5], Step [3030/6250], Loss: 2.4237\n",
      "Epoch [3/5], Step [3040/6250], Loss: 2.3674\n",
      "Epoch [3/5], Step [3050/6250], Loss: 1.4058\n",
      "Epoch [3/5], Step [3060/6250], Loss: 2.1897\n",
      "Epoch [3/5], Step [3070/6250], Loss: 2.4079\n",
      "Epoch [3/5], Step [3080/6250], Loss: 1.6419\n",
      "Epoch [3/5], Step [3090/6250], Loss: 2.9233\n",
      "Epoch [3/5], Step [3100/6250], Loss: 2.3483\n",
      "Epoch [3/5], Step [3110/6250], Loss: 1.9065\n",
      "Epoch [3/5], Step [3120/6250], Loss: 2.5094\n",
      "Epoch [3/5], Step [3130/6250], Loss: 2.5912\n",
      "Epoch [3/5], Step [3140/6250], Loss: 2.3378\n",
      "Epoch [3/5], Step [3150/6250], Loss: 2.7406\n",
      "Epoch [3/5], Step [3160/6250], Loss: 2.3187\n",
      "Epoch [3/5], Step [3170/6250], Loss: 2.2899\n",
      "Epoch [3/5], Step [3180/6250], Loss: 2.7561\n",
      "Epoch [3/5], Step [3190/6250], Loss: 0.9976\n",
      "Epoch [3/5], Step [3200/6250], Loss: 1.5250\n",
      "Epoch [3/5], Step [3210/6250], Loss: 1.6911\n",
      "Epoch [3/5], Step [3220/6250], Loss: 2.4130\n",
      "Epoch [3/5], Step [3230/6250], Loss: 1.6922\n",
      "Epoch [3/5], Step [3240/6250], Loss: 1.2773\n",
      "Epoch [3/5], Step [3250/6250], Loss: 1.9171\n",
      "Epoch [3/5], Step [3260/6250], Loss: 2.1950\n",
      "Epoch [3/5], Step [3270/6250], Loss: 2.6888\n",
      "Epoch [3/5], Step [3280/6250], Loss: 2.0875\n",
      "Epoch [3/5], Step [3290/6250], Loss: 2.3945\n",
      "Epoch [3/5], Step [3300/6250], Loss: 3.0359\n",
      "Epoch [3/5], Step [3310/6250], Loss: 1.7960\n",
      "Epoch [3/5], Step [3320/6250], Loss: 1.9972\n",
      "Epoch [3/5], Step [3330/6250], Loss: 2.4414\n",
      "Epoch [3/5], Step [3340/6250], Loss: 1.5996\n",
      "Epoch [3/5], Step [3350/6250], Loss: 2.3426\n",
      "Epoch [3/5], Step [3360/6250], Loss: 2.0523\n",
      "Epoch [3/5], Step [3370/6250], Loss: 2.2387\n",
      "Epoch [3/5], Step [3380/6250], Loss: 2.0828\n",
      "Epoch [3/5], Step [3390/6250], Loss: 2.0537\n",
      "Epoch [3/5], Step [3400/6250], Loss: 3.0143\n",
      "Epoch [3/5], Step [3410/6250], Loss: 2.3829\n",
      "Epoch [3/5], Step [3420/6250], Loss: 1.8872\n",
      "Epoch [3/5], Step [3430/6250], Loss: 1.2144\n",
      "Epoch [3/5], Step [3440/6250], Loss: 2.0418\n",
      "Epoch [3/5], Step [3450/6250], Loss: 1.4292\n",
      "Epoch [3/5], Step [3460/6250], Loss: 0.9583\n",
      "Epoch [3/5], Step [3470/6250], Loss: 2.0011\n",
      "Epoch [3/5], Step [3480/6250], Loss: 2.0558\n",
      "Epoch [3/5], Step [3490/6250], Loss: 1.8981\n",
      "Epoch [3/5], Step [3500/6250], Loss: 2.0484\n",
      "Epoch [3/5], Step [3510/6250], Loss: 2.8455\n",
      "Epoch [3/5], Step [3520/6250], Loss: 1.9375\n",
      "Epoch [3/5], Step [3530/6250], Loss: 2.4359\n",
      "Epoch [3/5], Step [3540/6250], Loss: 2.2568\n",
      "Epoch [3/5], Step [3550/6250], Loss: 1.5860\n",
      "Epoch [3/5], Step [3560/6250], Loss: 1.3216\n",
      "Epoch [3/5], Step [3570/6250], Loss: 1.3264\n",
      "Epoch [3/5], Step [3580/6250], Loss: 1.8697\n",
      "Epoch [3/5], Step [3590/6250], Loss: 1.9389\n",
      "Epoch [3/5], Step [3600/6250], Loss: 2.6056\n",
      "Epoch [3/5], Step [3610/6250], Loss: 1.7151\n",
      "Epoch [3/5], Step [3620/6250], Loss: 1.9760\n",
      "Epoch [3/5], Step [3630/6250], Loss: 1.9316\n",
      "Epoch [3/5], Step [3640/6250], Loss: 2.5296\n",
      "Epoch [3/5], Step [3650/6250], Loss: 2.3270\n",
      "Epoch [3/5], Step [3660/6250], Loss: 2.0819\n",
      "Epoch [3/5], Step [3670/6250], Loss: 3.3128\n",
      "Epoch [3/5], Step [3680/6250], Loss: 1.7402\n",
      "Epoch [3/5], Step [3690/6250], Loss: 1.3217\n",
      "Epoch [3/5], Step [3700/6250], Loss: 2.2806\n",
      "Epoch [3/5], Step [3710/6250], Loss: 1.6613\n",
      "Epoch [3/5], Step [3720/6250], Loss: 1.4798\n",
      "Epoch [3/5], Step [3730/6250], Loss: 2.5859\n",
      "Epoch [3/5], Step [3740/6250], Loss: 2.2034\n",
      "Epoch [3/5], Step [3750/6250], Loss: 2.9690\n",
      "Epoch [3/5], Step [3760/6250], Loss: 2.0756\n",
      "Epoch [3/5], Step [3770/6250], Loss: 2.1708\n",
      "Epoch [3/5], Step [3780/6250], Loss: 1.7435\n",
      "Epoch [3/5], Step [3790/6250], Loss: 1.9150\n",
      "Epoch [3/5], Step [3800/6250], Loss: 2.6578\n",
      "Epoch [3/5], Step [3810/6250], Loss: 2.4718\n",
      "Epoch [3/5], Step [3820/6250], Loss: 1.7130\n",
      "Epoch [3/5], Step [3830/6250], Loss: 1.6557\n",
      "Epoch [3/5], Step [3840/6250], Loss: 1.9923\n",
      "Epoch [3/5], Step [3850/6250], Loss: 1.2192\n",
      "Epoch [3/5], Step [3860/6250], Loss: 1.7162\n",
      "Epoch [3/5], Step [3870/6250], Loss: 2.1209\n",
      "Epoch [3/5], Step [3880/6250], Loss: 2.1836\n",
      "Epoch [3/5], Step [3890/6250], Loss: 1.8350\n",
      "Epoch [3/5], Step [3900/6250], Loss: 2.7473\n",
      "Epoch [3/5], Step [3910/6250], Loss: 1.9607\n",
      "Epoch [3/5], Step [3920/6250], Loss: 1.1008\n",
      "Epoch [3/5], Step [3930/6250], Loss: 2.7142\n",
      "Epoch [3/5], Step [3940/6250], Loss: 2.0810\n",
      "Epoch [3/5], Step [3950/6250], Loss: 2.1165\n",
      "Epoch [3/5], Step [3960/6250], Loss: 2.0900\n",
      "Epoch [3/5], Step [3970/6250], Loss: 1.5504\n",
      "Epoch [3/5], Step [3980/6250], Loss: 2.1155\n",
      "Epoch [3/5], Step [3990/6250], Loss: 2.3777\n",
      "Epoch [3/5], Step [4000/6250], Loss: 3.0766\n",
      "Epoch [3/5], Step [4010/6250], Loss: 1.9008\n",
      "Epoch [3/5], Step [4020/6250], Loss: 2.3872\n",
      "Epoch [3/5], Step [4030/6250], Loss: 2.0866\n",
      "Epoch [3/5], Step [4040/6250], Loss: 4.0559\n",
      "Epoch [3/5], Step [4050/6250], Loss: 1.4239\n",
      "Epoch [3/5], Step [4060/6250], Loss: 2.2002\n",
      "Epoch [3/5], Step [4070/6250], Loss: 2.3522\n",
      "Epoch [3/5], Step [4080/6250], Loss: 2.0411\n",
      "Epoch [3/5], Step [4090/6250], Loss: 1.5119\n",
      "Epoch [3/5], Step [4100/6250], Loss: 2.9402\n",
      "Epoch [3/5], Step [4110/6250], Loss: 1.5827\n",
      "Epoch [3/5], Step [4120/6250], Loss: 1.8983\n",
      "Epoch [3/5], Step [4130/6250], Loss: 2.8528\n",
      "Epoch [3/5], Step [4140/6250], Loss: 2.3671\n",
      "Epoch [3/5], Step [4150/6250], Loss: 1.8400\n",
      "Epoch [3/5], Step [4160/6250], Loss: 1.6092\n",
      "Epoch [3/5], Step [4170/6250], Loss: 2.1764\n",
      "Epoch [3/5], Step [4180/6250], Loss: 2.0066\n",
      "Epoch [3/5], Step [4190/6250], Loss: 1.2963\n",
      "Epoch [3/5], Step [4200/6250], Loss: 2.2173\n",
      "Epoch [3/5], Step [4210/6250], Loss: 2.2721\n",
      "Epoch [3/5], Step [4220/6250], Loss: 1.6677\n",
      "Epoch [3/5], Step [4230/6250], Loss: 2.1401\n",
      "Epoch [3/5], Step [4240/6250], Loss: 2.3514\n",
      "Epoch [3/5], Step [4250/6250], Loss: 2.0682\n",
      "Epoch [3/5], Step [4260/6250], Loss: 1.9978\n",
      "Epoch [3/5], Step [4270/6250], Loss: 1.9006\n",
      "Epoch [3/5], Step [4280/6250], Loss: 2.7875\n",
      "Epoch [3/5], Step [4290/6250], Loss: 3.0709\n",
      "Epoch [3/5], Step [4300/6250], Loss: 2.9331\n",
      "Epoch [3/5], Step [4310/6250], Loss: 1.7688\n",
      "Epoch [3/5], Step [4320/6250], Loss: 1.7251\n",
      "Epoch [3/5], Step [4330/6250], Loss: 1.9471\n",
      "Epoch [3/5], Step [4340/6250], Loss: 2.7162\n",
      "Epoch [3/5], Step [4350/6250], Loss: 1.8013\n",
      "Epoch [3/5], Step [4360/6250], Loss: 1.2459\n",
      "Epoch [3/5], Step [4370/6250], Loss: 2.4261\n",
      "Epoch [3/5], Step [4380/6250], Loss: 2.0074\n",
      "Epoch [3/5], Step [4390/6250], Loss: 1.3762\n",
      "Epoch [3/5], Step [4400/6250], Loss: 2.3659\n",
      "Epoch [3/5], Step [4410/6250], Loss: 2.4108\n",
      "Epoch [3/5], Step [4420/6250], Loss: 1.2641\n",
      "Epoch [3/5], Step [4430/6250], Loss: 1.5571\n",
      "Epoch [3/5], Step [4440/6250], Loss: 2.5961\n",
      "Epoch [3/5], Step [4450/6250], Loss: 1.9615\n",
      "Epoch [3/5], Step [4460/6250], Loss: 1.8539\n",
      "Epoch [3/5], Step [4470/6250], Loss: 2.3285\n",
      "Epoch [3/5], Step [4480/6250], Loss: 1.6182\n",
      "Epoch [3/5], Step [4490/6250], Loss: 1.9643\n",
      "Epoch [3/5], Step [4500/6250], Loss: 1.9791\n",
      "Epoch [3/5], Step [4510/6250], Loss: 2.8264\n",
      "Epoch [3/5], Step [4520/6250], Loss: 1.6041\n",
      "Epoch [3/5], Step [4530/6250], Loss: 2.2238\n",
      "Epoch [3/5], Step [4540/6250], Loss: 1.4081\n",
      "Epoch [3/5], Step [4550/6250], Loss: 2.0932\n",
      "Epoch [3/5], Step [4560/6250], Loss: 2.1692\n",
      "Epoch [3/5], Step [4570/6250], Loss: 2.7469\n",
      "Epoch [3/5], Step [4580/6250], Loss: 3.0140\n",
      "Epoch [3/5], Step [4590/6250], Loss: 2.7948\n",
      "Epoch [3/5], Step [4600/6250], Loss: 3.4705\n",
      "Epoch [3/5], Step [4610/6250], Loss: 2.2594\n",
      "Epoch [3/5], Step [4620/6250], Loss: 2.1604\n",
      "Epoch [3/5], Step [4630/6250], Loss: 1.6217\n",
      "Epoch [3/5], Step [4640/6250], Loss: 1.1313\n",
      "Epoch [3/5], Step [4650/6250], Loss: 2.1618\n",
      "Epoch [3/5], Step [4660/6250], Loss: 1.4774\n",
      "Epoch [3/5], Step [4670/6250], Loss: 2.3665\n",
      "Epoch [3/5], Step [4680/6250], Loss: 2.0649\n",
      "Epoch [3/5], Step [4690/6250], Loss: 3.2840\n",
      "Epoch [3/5], Step [4700/6250], Loss: 1.7575\n",
      "Epoch [3/5], Step [4710/6250], Loss: 1.7601\n",
      "Epoch [3/5], Step [4720/6250], Loss: 2.5531\n",
      "Epoch [3/5], Step [4730/6250], Loss: 2.6795\n",
      "Epoch [3/5], Step [4740/6250], Loss: 2.2703\n",
      "Epoch [3/5], Step [4750/6250], Loss: 1.5942\n",
      "Epoch [3/5], Step [4760/6250], Loss: 3.4259\n",
      "Epoch [3/5], Step [4770/6250], Loss: 2.0140\n",
      "Epoch [3/5], Step [4780/6250], Loss: 1.5887\n",
      "Epoch [3/5], Step [4790/6250], Loss: 3.0844\n",
      "Epoch [3/5], Step [4800/6250], Loss: 1.4693\n",
      "Epoch [3/5], Step [4810/6250], Loss: 2.2323\n",
      "Epoch [3/5], Step [4820/6250], Loss: 2.8868\n",
      "Epoch [3/5], Step [4830/6250], Loss: 2.3110\n",
      "Epoch [3/5], Step [4840/6250], Loss: 2.2018\n",
      "Epoch [3/5], Step [4850/6250], Loss: 2.6837\n",
      "Epoch [3/5], Step [4860/6250], Loss: 2.2561\n",
      "Epoch [3/5], Step [4870/6250], Loss: 1.2869\n",
      "Epoch [3/5], Step [4880/6250], Loss: 2.0026\n",
      "Epoch [3/5], Step [4890/6250], Loss: 2.4109\n",
      "Epoch [3/5], Step [4900/6250], Loss: 3.5283\n",
      "Epoch [3/5], Step [4910/6250], Loss: 1.7279\n",
      "Epoch [3/5], Step [4920/6250], Loss: 2.0630\n",
      "Epoch [3/5], Step [4930/6250], Loss: 1.6457\n",
      "Epoch [3/5], Step [4940/6250], Loss: 3.1839\n",
      "Epoch [3/5], Step [4950/6250], Loss: 1.8337\n",
      "Epoch [3/5], Step [4960/6250], Loss: 2.3020\n",
      "Epoch [3/5], Step [4970/6250], Loss: 1.9444\n",
      "Epoch [3/5], Step [4980/6250], Loss: 2.9074\n",
      "Epoch [3/5], Step [4990/6250], Loss: 1.4717\n",
      "Epoch [3/5], Step [5000/6250], Loss: 1.7535\n",
      "Epoch [3/5], Step [5010/6250], Loss: 2.9396\n",
      "Epoch [3/5], Step [5020/6250], Loss: 2.6367\n",
      "Epoch [3/5], Step [5030/6250], Loss: 2.4451\n",
      "Epoch [3/5], Step [5040/6250], Loss: 2.9972\n",
      "Epoch [3/5], Step [5050/6250], Loss: 3.4773\n",
      "Epoch [3/5], Step [5060/6250], Loss: 2.0673\n",
      "Epoch [3/5], Step [5070/6250], Loss: 2.1592\n",
      "Epoch [3/5], Step [5080/6250], Loss: 1.9680\n",
      "Epoch [3/5], Step [5090/6250], Loss: 2.6653\n",
      "Epoch [3/5], Step [5100/6250], Loss: 3.3375\n",
      "Epoch [3/5], Step [5110/6250], Loss: 1.5376\n",
      "Epoch [3/5], Step [5120/6250], Loss: 1.8149\n",
      "Epoch [3/5], Step [5130/6250], Loss: 1.9702\n",
      "Epoch [3/5], Step [5140/6250], Loss: 3.5330\n",
      "Epoch [3/5], Step [5150/6250], Loss: 1.8558\n",
      "Epoch [3/5], Step [5160/6250], Loss: 2.4259\n",
      "Epoch [3/5], Step [5170/6250], Loss: 2.3046\n",
      "Epoch [3/5], Step [5180/6250], Loss: 2.1398\n",
      "Epoch [3/5], Step [5190/6250], Loss: 3.1892\n",
      "Epoch [3/5], Step [5200/6250], Loss: 2.3276\n",
      "Epoch [3/5], Step [5210/6250], Loss: 2.3093\n",
      "Epoch [3/5], Step [5220/6250], Loss: 2.1208\n",
      "Epoch [3/5], Step [5230/6250], Loss: 1.8513\n",
      "Epoch [3/5], Step [5240/6250], Loss: 2.4493\n",
      "Epoch [3/5], Step [5250/6250], Loss: 2.7413\n",
      "Epoch [3/5], Step [5260/6250], Loss: 2.5569\n",
      "Epoch [3/5], Step [5270/6250], Loss: 1.7170\n",
      "Epoch [3/5], Step [5280/6250], Loss: 2.0716\n",
      "Epoch [3/5], Step [5290/6250], Loss: 2.3339\n",
      "Epoch [3/5], Step [5300/6250], Loss: 2.4179\n",
      "Epoch [3/5], Step [5310/6250], Loss: 2.3051\n",
      "Epoch [3/5], Step [5320/6250], Loss: 3.2231\n",
      "Epoch [3/5], Step [5330/6250], Loss: 2.1447\n",
      "Epoch [3/5], Step [5340/6250], Loss: 2.2976\n",
      "Epoch [3/5], Step [5350/6250], Loss: 1.6575\n",
      "Epoch [3/5], Step [5360/6250], Loss: 1.3693\n",
      "Epoch [3/5], Step [5370/6250], Loss: 2.7065\n",
      "Epoch [3/5], Step [5380/6250], Loss: 1.9858\n",
      "Epoch [3/5], Step [5390/6250], Loss: 2.2165\n",
      "Epoch [3/5], Step [5400/6250], Loss: 2.0115\n",
      "Epoch [3/5], Step [5410/6250], Loss: 2.0003\n",
      "Epoch [3/5], Step [5420/6250], Loss: 3.2300\n",
      "Epoch [3/5], Step [5430/6250], Loss: 1.9598\n",
      "Epoch [3/5], Step [5440/6250], Loss: 2.5804\n",
      "Epoch [3/5], Step [5450/6250], Loss: 2.8896\n",
      "Epoch [3/5], Step [5460/6250], Loss: 1.6601\n",
      "Epoch [3/5], Step [5470/6250], Loss: 2.7401\n",
      "Epoch [3/5], Step [5480/6250], Loss: 2.4557\n",
      "Epoch [3/5], Step [5490/6250], Loss: 1.7655\n",
      "Epoch [3/5], Step [5500/6250], Loss: 1.6410\n",
      "Epoch [3/5], Step [5510/6250], Loss: 2.2096\n",
      "Epoch [3/5], Step [5520/6250], Loss: 1.6732\n",
      "Epoch [3/5], Step [5530/6250], Loss: 1.7055\n",
      "Epoch [3/5], Step [5540/6250], Loss: 2.9045\n",
      "Epoch [3/5], Step [5550/6250], Loss: 2.7547\n",
      "Epoch [3/5], Step [5560/6250], Loss: 2.7076\n",
      "Epoch [3/5], Step [5570/6250], Loss: 2.6179\n",
      "Epoch [3/5], Step [5580/6250], Loss: 1.6779\n",
      "Epoch [3/5], Step [5590/6250], Loss: 1.2267\n",
      "Epoch [3/5], Step [5600/6250], Loss: 3.1704\n",
      "Epoch [3/5], Step [5610/6250], Loss: 3.4017\n",
      "Epoch [3/5], Step [5620/6250], Loss: 2.2139\n",
      "Epoch [3/5], Step [5630/6250], Loss: 1.5964\n",
      "Epoch [3/5], Step [5640/6250], Loss: 2.6333\n",
      "Epoch [3/5], Step [5650/6250], Loss: 1.5677\n",
      "Epoch [3/5], Step [5660/6250], Loss: 2.1798\n",
      "Epoch [3/5], Step [5670/6250], Loss: 2.4054\n",
      "Epoch [3/5], Step [5680/6250], Loss: 2.2408\n",
      "Epoch [3/5], Step [5690/6250], Loss: 1.8363\n",
      "Epoch [3/5], Step [5700/6250], Loss: 2.3441\n",
      "Epoch [3/5], Step [5710/6250], Loss: 2.5515\n",
      "Epoch [3/5], Step [5720/6250], Loss: 1.8380\n",
      "Epoch [3/5], Step [5730/6250], Loss: 2.1483\n",
      "Epoch [3/5], Step [5740/6250], Loss: 1.1435\n",
      "Epoch [3/5], Step [5750/6250], Loss: 2.8749\n",
      "Epoch [3/5], Step [5760/6250], Loss: 1.1690\n",
      "Epoch [3/5], Step [5770/6250], Loss: 1.0289\n",
      "Epoch [3/5], Step [5780/6250], Loss: 2.1405\n",
      "Epoch [3/5], Step [5790/6250], Loss: 1.7905\n",
      "Epoch [3/5], Step [5800/6250], Loss: 1.5477\n",
      "Epoch [3/5], Step [5810/6250], Loss: 2.0330\n",
      "Epoch [3/5], Step [5820/6250], Loss: 1.6252\n",
      "Epoch [3/5], Step [5830/6250], Loss: 2.1998\n",
      "Epoch [3/5], Step [5840/6250], Loss: 1.7355\n",
      "Epoch [3/5], Step [5850/6250], Loss: 2.0940\n",
      "Epoch [3/5], Step [5860/6250], Loss: 3.0543\n",
      "Epoch [3/5], Step [5870/6250], Loss: 2.3307\n",
      "Epoch [3/5], Step [5880/6250], Loss: 3.2006\n",
      "Epoch [3/5], Step [5890/6250], Loss: 4.1066\n",
      "Epoch [3/5], Step [5900/6250], Loss: 1.9099\n",
      "Epoch [3/5], Step [5910/6250], Loss: 2.3855\n",
      "Epoch [3/5], Step [5920/6250], Loss: 2.7755\n",
      "Epoch [3/5], Step [5930/6250], Loss: 2.2669\n",
      "Epoch [3/5], Step [5940/6250], Loss: 1.2904\n",
      "Epoch [3/5], Step [5950/6250], Loss: 2.4362\n",
      "Epoch [3/5], Step [5960/6250], Loss: 1.7163\n",
      "Epoch [3/5], Step [5970/6250], Loss: 2.9804\n",
      "Epoch [3/5], Step [5980/6250], Loss: 3.3907\n",
      "Epoch [3/5], Step [5990/6250], Loss: 1.9425\n",
      "Epoch [3/5], Step [6000/6250], Loss: 1.2577\n",
      "Epoch [3/5], Step [6010/6250], Loss: 2.0301\n",
      "Epoch [3/5], Step [6020/6250], Loss: 2.1089\n",
      "Epoch [3/5], Step [6030/6250], Loss: 1.9749\n",
      "Epoch [3/5], Step [6040/6250], Loss: 2.3980\n",
      "Epoch [3/5], Step [6050/6250], Loss: 2.0223\n",
      "Epoch [3/5], Step [6060/6250], Loss: 1.9899\n",
      "Epoch [3/5], Step [6070/6250], Loss: 2.6028\n",
      "Epoch [3/5], Step [6080/6250], Loss: 3.6976\n",
      "Epoch [3/5], Step [6090/6250], Loss: 2.0246\n",
      "Epoch [3/5], Step [6100/6250], Loss: 2.0120\n",
      "Epoch [3/5], Step [6110/6250], Loss: 0.9941\n",
      "Epoch [3/5], Step [6120/6250], Loss: 1.8905\n",
      "Epoch [3/5], Step [6130/6250], Loss: 1.6472\n",
      "Epoch [3/5], Step [6140/6250], Loss: 1.9158\n",
      "Epoch [3/5], Step [6150/6250], Loss: 3.7725\n",
      "Epoch [3/5], Step [6160/6250], Loss: 3.2861\n",
      "Epoch [3/5], Step [6170/6250], Loss: 3.0804\n",
      "Epoch [3/5], Step [6180/6250], Loss: 2.9233\n",
      "Epoch [3/5], Step [6190/6250], Loss: 2.6642\n",
      "Epoch [3/5], Step [6200/6250], Loss: 3.1875\n",
      "Epoch [3/5], Step [6210/6250], Loss: 2.0534\n",
      "Epoch [3/5], Step [6220/6250], Loss: 2.5922\n",
      "Epoch [3/5], Step [6230/6250], Loss: 2.1927\n",
      "Epoch [3/5], Step [6240/6250], Loss: 1.9141\n",
      "Epoch [3/5], Step [6250/6250], Loss: 2.5899\n",
      "Epoch 3, Loss: 2.589940071105957\n",
      "Epoch [4/5], Step [10/6250], Loss: 1.3386\n",
      "Epoch [4/5], Step [20/6250], Loss: 2.0697\n",
      "Epoch [4/5], Step [30/6250], Loss: 3.0799\n",
      "Epoch [4/5], Step [40/6250], Loss: 1.4949\n",
      "Epoch [4/5], Step [50/6250], Loss: 1.7238\n",
      "Epoch [4/5], Step [60/6250], Loss: 1.9225\n",
      "Epoch [4/5], Step [70/6250], Loss: 2.8285\n",
      "Epoch [4/5], Step [80/6250], Loss: 1.9911\n",
      "Epoch [4/5], Step [90/6250], Loss: 2.3744\n",
      "Epoch [4/5], Step [100/6250], Loss: 3.4012\n",
      "Epoch [4/5], Step [110/6250], Loss: 2.1891\n",
      "Epoch [4/5], Step [120/6250], Loss: 2.4246\n",
      "Epoch [4/5], Step [130/6250], Loss: 1.3896\n",
      "Epoch [4/5], Step [140/6250], Loss: 2.0558\n",
      "Epoch [4/5], Step [150/6250], Loss: 1.7005\n",
      "Epoch [4/5], Step [160/6250], Loss: 2.0222\n",
      "Epoch [4/5], Step [170/6250], Loss: 2.9131\n",
      "Epoch [4/5], Step [180/6250], Loss: 2.4817\n",
      "Epoch [4/5], Step [190/6250], Loss: 2.0302\n",
      "Epoch [4/5], Step [200/6250], Loss: 2.8615\n",
      "Epoch [4/5], Step [210/6250], Loss: 2.0330\n",
      "Epoch [4/5], Step [220/6250], Loss: 1.9867\n",
      "Epoch [4/5], Step [230/6250], Loss: 2.7836\n",
      "Epoch [4/5], Step [240/6250], Loss: 2.4637\n",
      "Epoch [4/5], Step [250/6250], Loss: 2.2686\n",
      "Epoch [4/5], Step [260/6250], Loss: 2.1973\n",
      "Epoch [4/5], Step [270/6250], Loss: 1.6057\n",
      "Epoch [4/5], Step [280/6250], Loss: 1.6722\n",
      "Epoch [4/5], Step [290/6250], Loss: 2.0835\n",
      "Epoch [4/5], Step [300/6250], Loss: 1.7138\n",
      "Epoch [4/5], Step [310/6250], Loss: 2.2116\n",
      "Epoch [4/5], Step [320/6250], Loss: 2.1803\n",
      "Epoch [4/5], Step [330/6250], Loss: 2.0623\n",
      "Epoch [4/5], Step [340/6250], Loss: 2.3808\n",
      "Epoch [4/5], Step [350/6250], Loss: 2.4209\n",
      "Epoch [4/5], Step [360/6250], Loss: 3.2633\n",
      "Epoch [4/5], Step [370/6250], Loss: 2.4003\n",
      "Epoch [4/5], Step [380/6250], Loss: 2.7529\n",
      "Epoch [4/5], Step [390/6250], Loss: 2.4058\n",
      "Epoch [4/5], Step [400/6250], Loss: 2.2641\n",
      "Epoch [4/5], Step [410/6250], Loss: 1.9753\n",
      "Epoch [4/5], Step [420/6250], Loss: 2.2358\n",
      "Epoch [4/5], Step [430/6250], Loss: 2.1010\n",
      "Epoch [4/5], Step [440/6250], Loss: 1.5376\n",
      "Epoch [4/5], Step [450/6250], Loss: 2.2768\n",
      "Epoch [4/5], Step [460/6250], Loss: 1.9860\n",
      "Epoch [4/5], Step [470/6250], Loss: 2.8150\n",
      "Epoch [4/5], Step [480/6250], Loss: 3.0743\n",
      "Epoch [4/5], Step [490/6250], Loss: 2.6151\n",
      "Epoch [4/5], Step [500/6250], Loss: 1.7544\n",
      "Epoch [4/5], Step [510/6250], Loss: 2.2346\n",
      "Epoch [4/5], Step [520/6250], Loss: 3.5759\n",
      "Epoch [4/5], Step [530/6250], Loss: 2.1505\n",
      "Epoch [4/5], Step [540/6250], Loss: 1.0559\n",
      "Epoch [4/5], Step [550/6250], Loss: 3.8958\n",
      "Epoch [4/5], Step [560/6250], Loss: 1.9463\n",
      "Epoch [4/5], Step [570/6250], Loss: 1.6697\n",
      "Epoch [4/5], Step [580/6250], Loss: 1.6232\n",
      "Epoch [4/5], Step [590/6250], Loss: 1.2270\n",
      "Epoch [4/5], Step [600/6250], Loss: 2.5405\n",
      "Epoch [4/5], Step [610/6250], Loss: 1.1536\n",
      "Epoch [4/5], Step [620/6250], Loss: 1.7541\n",
      "Epoch [4/5], Step [630/6250], Loss: 1.9983\n",
      "Epoch [4/5], Step [640/6250], Loss: 1.8207\n",
      "Epoch [4/5], Step [650/6250], Loss: 2.4621\n",
      "Epoch [4/5], Step [660/6250], Loss: 2.1587\n",
      "Epoch [4/5], Step [670/6250], Loss: 2.1430\n",
      "Epoch [4/5], Step [680/6250], Loss: 2.3877\n",
      "Epoch [4/5], Step [690/6250], Loss: 1.4870\n",
      "Epoch [4/5], Step [700/6250], Loss: 1.8112\n",
      "Epoch [4/5], Step [710/6250], Loss: 1.2494\n",
      "Epoch [4/5], Step [720/6250], Loss: 2.6452\n",
      "Epoch [4/5], Step [730/6250], Loss: 2.5131\n",
      "Epoch [4/5], Step [740/6250], Loss: 1.7460\n",
      "Epoch [4/5], Step [750/6250], Loss: 2.3092\n",
      "Epoch [4/5], Step [760/6250], Loss: 2.0695\n",
      "Epoch [4/5], Step [770/6250], Loss: 2.2482\n",
      "Epoch [4/5], Step [780/6250], Loss: 2.2088\n",
      "Epoch [4/5], Step [790/6250], Loss: 3.3307\n",
      "Epoch [4/5], Step [800/6250], Loss: 2.4973\n",
      "Epoch [4/5], Step [810/6250], Loss: 1.6410\n",
      "Epoch [4/5], Step [820/6250], Loss: 1.7675\n",
      "Epoch [4/5], Step [830/6250], Loss: 1.8378\n",
      "Epoch [4/5], Step [840/6250], Loss: 3.2975\n",
      "Epoch [4/5], Step [850/6250], Loss: 2.4122\n",
      "Epoch [4/5], Step [860/6250], Loss: 2.5981\n",
      "Epoch [4/5], Step [870/6250], Loss: 2.7346\n",
      "Epoch [4/5], Step [880/6250], Loss: 1.9413\n",
      "Epoch [4/5], Step [890/6250], Loss: 2.7722\n",
      "Epoch [4/5], Step [900/6250], Loss: 2.0934\n",
      "Epoch [4/5], Step [910/6250], Loss: 2.2404\n",
      "Epoch [4/5], Step [920/6250], Loss: 1.4235\n",
      "Epoch [4/5], Step [930/6250], Loss: 2.7191\n",
      "Epoch [4/5], Step [940/6250], Loss: 1.6172\n",
      "Epoch [4/5], Step [950/6250], Loss: 3.5025\n",
      "Epoch [4/5], Step [960/6250], Loss: 2.3520\n",
      "Epoch [4/5], Step [970/6250], Loss: 2.0659\n",
      "Epoch [4/5], Step [980/6250], Loss: 2.3075\n",
      "Epoch [4/5], Step [990/6250], Loss: 4.2414\n",
      "Epoch [4/5], Step [1000/6250], Loss: 2.3586\n",
      "Epoch [4/5], Step [1010/6250], Loss: 2.5045\n",
      "Epoch [4/5], Step [1020/6250], Loss: 3.2164\n",
      "Epoch [4/5], Step [1030/6250], Loss: 2.5689\n",
      "Epoch [4/5], Step [1040/6250], Loss: 3.1018\n",
      "Epoch [4/5], Step [1050/6250], Loss: 2.8051\n",
      "Epoch [4/5], Step [1060/6250], Loss: 1.9916\n",
      "Epoch [4/5], Step [1070/6250], Loss: 2.0105\n",
      "Epoch [4/5], Step [1080/6250], Loss: 2.1552\n",
      "Epoch [4/5], Step [1090/6250], Loss: 3.4499\n",
      "Epoch [4/5], Step [1100/6250], Loss: 1.8001\n",
      "Epoch [4/5], Step [1110/6250], Loss: 2.5452\n",
      "Epoch [4/5], Step [1120/6250], Loss: 2.0101\n",
      "Epoch [4/5], Step [1130/6250], Loss: 1.5753\n",
      "Epoch [4/5], Step [1140/6250], Loss: 1.4545\n",
      "Epoch [4/5], Step [1150/6250], Loss: 2.4360\n",
      "Epoch [4/5], Step [1160/6250], Loss: 2.0825\n",
      "Epoch [4/5], Step [1170/6250], Loss: 2.9291\n",
      "Epoch [4/5], Step [1180/6250], Loss: 2.3715\n",
      "Epoch [4/5], Step [1190/6250], Loss: 2.1421\n",
      "Epoch [4/5], Step [1200/6250], Loss: 2.1324\n",
      "Epoch [4/5], Step [1210/6250], Loss: 2.7180\n",
      "Epoch [4/5], Step [1220/6250], Loss: 1.9292\n",
      "Epoch [4/5], Step [1230/6250], Loss: 3.5342\n",
      "Epoch [4/5], Step [1240/6250], Loss: 1.3875\n",
      "Epoch [4/5], Step [1250/6250], Loss: 2.6642\n",
      "Epoch [4/5], Step [1260/6250], Loss: 1.4973\n",
      "Epoch [4/5], Step [1270/6250], Loss: 2.5660\n",
      "Epoch [4/5], Step [1280/6250], Loss: 2.9210\n",
      "Epoch [4/5], Step [1290/6250], Loss: 2.3724\n",
      "Epoch [4/5], Step [1300/6250], Loss: 1.9872\n",
      "Epoch [4/5], Step [1310/6250], Loss: 1.9414\n",
      "Epoch [4/5], Step [1320/6250], Loss: 4.4785\n",
      "Epoch [4/5], Step [1330/6250], Loss: 2.0477\n",
      "Epoch [4/5], Step [1340/6250], Loss: 2.1182\n",
      "Epoch [4/5], Step [1350/6250], Loss: 2.7446\n",
      "Epoch [4/5], Step [1360/6250], Loss: 2.9830\n",
      "Epoch [4/5], Step [1370/6250], Loss: 2.8992\n",
      "Epoch [4/5], Step [1380/6250], Loss: 2.1275\n",
      "Epoch [4/5], Step [1390/6250], Loss: 3.0293\n",
      "Epoch [4/5], Step [1400/6250], Loss: 2.9534\n",
      "Epoch [4/5], Step [1410/6250], Loss: 2.6069\n",
      "Epoch [4/5], Step [1420/6250], Loss: 2.7594\n",
      "Epoch [4/5], Step [1430/6250], Loss: 1.6826\n",
      "Epoch [4/5], Step [1440/6250], Loss: 2.0592\n",
      "Epoch [4/5], Step [1450/6250], Loss: 2.1739\n",
      "Epoch [4/5], Step [1460/6250], Loss: 2.7354\n",
      "Epoch [4/5], Step [1470/6250], Loss: 3.1295\n",
      "Epoch [4/5], Step [1480/6250], Loss: 2.2864\n",
      "Epoch [4/5], Step [1490/6250], Loss: 2.3061\n",
      "Epoch [4/5], Step [1500/6250], Loss: 4.5164\n",
      "Epoch [4/5], Step [1510/6250], Loss: 2.1835\n",
      "Epoch [4/5], Step [1520/6250], Loss: 3.4036\n",
      "Epoch [4/5], Step [1530/6250], Loss: 2.5379\n",
      "Epoch [4/5], Step [1540/6250], Loss: 2.2102\n",
      "Epoch [4/5], Step [1550/6250], Loss: 2.4923\n",
      "Epoch [4/5], Step [1560/6250], Loss: 3.2828\n",
      "Epoch [4/5], Step [1570/6250], Loss: 1.9174\n",
      "Epoch [4/5], Step [1580/6250], Loss: 2.9796\n",
      "Epoch [4/5], Step [1590/6250], Loss: 1.9462\n",
      "Epoch [4/5], Step [1600/6250], Loss: 2.6252\n",
      "Epoch [4/5], Step [1610/6250], Loss: 2.0336\n",
      "Epoch [4/5], Step [1620/6250], Loss: 2.9240\n",
      "Epoch [4/5], Step [1630/6250], Loss: 3.6007\n",
      "Epoch [4/5], Step [1640/6250], Loss: 3.2503\n",
      "Epoch [4/5], Step [1650/6250], Loss: 2.5520\n",
      "Epoch [4/5], Step [1660/6250], Loss: 1.2612\n",
      "Epoch [4/5], Step [1670/6250], Loss: 1.4751\n",
      "Epoch [4/5], Step [1680/6250], Loss: 3.4009\n",
      "Epoch [4/5], Step [1690/6250], Loss: 2.1149\n",
      "Epoch [4/5], Step [1700/6250], Loss: 3.6584\n",
      "Epoch [4/5], Step [1710/6250], Loss: 1.3300\n",
      "Epoch [4/5], Step [1720/6250], Loss: 1.9818\n",
      "Epoch [4/5], Step [1730/6250], Loss: 2.4814\n",
      "Epoch [4/5], Step [1740/6250], Loss: 2.0906\n",
      "Epoch [4/5], Step [1750/6250], Loss: 3.7521\n",
      "Epoch [4/5], Step [1760/6250], Loss: 2.1830\n",
      "Epoch [4/5], Step [1770/6250], Loss: 2.2308\n",
      "Epoch [4/5], Step [1780/6250], Loss: 2.0535\n",
      "Epoch [4/5], Step [1790/6250], Loss: 2.7389\n",
      "Epoch [4/5], Step [1800/6250], Loss: 3.0914\n",
      "Epoch [4/5], Step [1810/6250], Loss: 1.4964\n",
      "Epoch [4/5], Step [1820/6250], Loss: 2.1143\n",
      "Epoch [4/5], Step [1830/6250], Loss: 1.6502\n",
      "Epoch [4/5], Step [1840/6250], Loss: 1.9119\n",
      "Epoch [4/5], Step [1850/6250], Loss: 2.3245\n",
      "Epoch [4/5], Step [1860/6250], Loss: 2.2491\n",
      "Epoch [4/5], Step [1870/6250], Loss: 1.4911\n",
      "Epoch [4/5], Step [1880/6250], Loss: 2.1988\n",
      "Epoch [4/5], Step [1890/6250], Loss: 1.3247\n",
      "Epoch [4/5], Step [1900/6250], Loss: 2.1516\n",
      "Epoch [4/5], Step [1910/6250], Loss: 2.4213\n",
      "Epoch [4/5], Step [1920/6250], Loss: 2.5134\n",
      "Epoch [4/5], Step [1930/6250], Loss: 2.2809\n",
      "Epoch [4/5], Step [1940/6250], Loss: 3.2007\n",
      "Epoch [4/5], Step [1950/6250], Loss: 1.7078\n",
      "Epoch [4/5], Step [1960/6250], Loss: 1.5674\n",
      "Epoch [4/5], Step [1970/6250], Loss: 1.6897\n",
      "Epoch [4/5], Step [1980/6250], Loss: 1.6724\n",
      "Epoch [4/5], Step [1990/6250], Loss: 1.3133\n",
      "Epoch [4/5], Step [2000/6250], Loss: 1.6674\n",
      "Epoch [4/5], Step [2010/6250], Loss: 2.5907\n",
      "Epoch [4/5], Step [2020/6250], Loss: 2.8727\n",
      "Epoch [4/5], Step [2030/6250], Loss: 1.6669\n",
      "Epoch [4/5], Step [2040/6250], Loss: 1.1728\n",
      "Epoch [4/5], Step [2050/6250], Loss: 2.5012\n",
      "Epoch [4/5], Step [2060/6250], Loss: 2.2484\n",
      "Epoch [4/5], Step [2070/6250], Loss: 2.9746\n",
      "Epoch [4/5], Step [2080/6250], Loss: 2.1729\n",
      "Epoch [4/5], Step [2090/6250], Loss: 2.9440\n",
      "Epoch [4/5], Step [2100/6250], Loss: 2.7790\n",
      "Epoch [4/5], Step [2110/6250], Loss: 2.7917\n",
      "Epoch [4/5], Step [2120/6250], Loss: 1.8327\n",
      "Epoch [4/5], Step [2130/6250], Loss: 2.1736\n",
      "Epoch [4/5], Step [2140/6250], Loss: 1.2703\n",
      "Epoch [4/5], Step [2150/6250], Loss: 2.5203\n",
      "Epoch [4/5], Step [2160/6250], Loss: 2.4637\n",
      "Epoch [4/5], Step [2170/6250], Loss: 1.7544\n",
      "Epoch [4/5], Step [2180/6250], Loss: 3.0189\n",
      "Epoch [4/5], Step [2190/6250], Loss: 2.2498\n",
      "Epoch [4/5], Step [2200/6250], Loss: 1.3929\n",
      "Epoch [4/5], Step [2210/6250], Loss: 1.9696\n",
      "Epoch [4/5], Step [2220/6250], Loss: 1.9421\n",
      "Epoch [4/5], Step [2230/6250], Loss: 3.5570\n",
      "Epoch [4/5], Step [2240/6250], Loss: 3.7634\n",
      "Epoch [4/5], Step [2250/6250], Loss: 3.8060\n",
      "Epoch [4/5], Step [2260/6250], Loss: 2.9016\n",
      "Epoch [4/5], Step [2270/6250], Loss: 1.8237\n",
      "Epoch [4/5], Step [2280/6250], Loss: 2.5458\n",
      "Epoch [4/5], Step [2290/6250], Loss: 1.5623\n",
      "Epoch [4/5], Step [2300/6250], Loss: 2.5018\n",
      "Epoch [4/5], Step [2310/6250], Loss: 1.7640\n",
      "Epoch [4/5], Step [2320/6250], Loss: 2.1396\n",
      "Epoch [4/5], Step [2330/6250], Loss: 2.0645\n",
      "Epoch [4/5], Step [2340/6250], Loss: 1.8260\n",
      "Epoch [4/5], Step [2350/6250], Loss: 2.9067\n",
      "Epoch [4/5], Step [2360/6250], Loss: 2.1069\n",
      "Epoch [4/5], Step [2370/6250], Loss: 1.9669\n",
      "Epoch [4/5], Step [2380/6250], Loss: 2.8058\n",
      "Epoch [4/5], Step [2390/6250], Loss: 1.5231\n",
      "Epoch [4/5], Step [2400/6250], Loss: 3.3995\n",
      "Epoch [4/5], Step [2410/6250], Loss: 2.6330\n",
      "Epoch [4/5], Step [2420/6250], Loss: 2.1806\n",
      "Epoch [4/5], Step [2430/6250], Loss: 2.7151\n",
      "Epoch [4/5], Step [2440/6250], Loss: 2.4227\n",
      "Epoch [4/5], Step [2450/6250], Loss: 3.3984\n",
      "Epoch [4/5], Step [2460/6250], Loss: 4.1338\n",
      "Epoch [4/5], Step [2470/6250], Loss: 2.7549\n",
      "Epoch [4/5], Step [2480/6250], Loss: 2.1655\n",
      "Epoch [4/5], Step [2490/6250], Loss: 3.7641\n",
      "Epoch [4/5], Step [2500/6250], Loss: 1.7759\n",
      "Epoch [4/5], Step [2510/6250], Loss: 3.0026\n",
      "Epoch [4/5], Step [2520/6250], Loss: 1.8808\n",
      "Epoch [4/5], Step [2530/6250], Loss: 2.4755\n",
      "Epoch [4/5], Step [2540/6250], Loss: 3.5756\n",
      "Epoch [4/5], Step [2550/6250], Loss: 1.4299\n",
      "Epoch [4/5], Step [2560/6250], Loss: 1.6251\n",
      "Epoch [4/5], Step [2570/6250], Loss: 2.1690\n",
      "Epoch [4/5], Step [2580/6250], Loss: 3.1028\n",
      "Epoch [4/5], Step [2590/6250], Loss: 1.7079\n",
      "Epoch [4/5], Step [2600/6250], Loss: 1.9986\n",
      "Epoch [4/5], Step [2610/6250], Loss: 2.8181\n",
      "Epoch [4/5], Step [2620/6250], Loss: 1.6826\n",
      "Epoch [4/5], Step [2630/6250], Loss: 1.5300\n",
      "Epoch [4/5], Step [2640/6250], Loss: 1.8021\n",
      "Epoch [4/5], Step [2650/6250], Loss: 3.1352\n",
      "Epoch [4/5], Step [2660/6250], Loss: 1.7176\n",
      "Epoch [4/5], Step [2670/6250], Loss: 1.6509\n",
      "Epoch [4/5], Step [2680/6250], Loss: 1.6664\n",
      "Epoch [4/5], Step [2690/6250], Loss: 2.1453\n",
      "Epoch [4/5], Step [2700/6250], Loss: 2.2836\n",
      "Epoch [4/5], Step [2710/6250], Loss: 2.9965\n",
      "Epoch [4/5], Step [2720/6250], Loss: 2.9519\n",
      "Epoch [4/5], Step [2730/6250], Loss: 1.7989\n",
      "Epoch [4/5], Step [2740/6250], Loss: 2.5770\n",
      "Epoch [4/5], Step [2750/6250], Loss: 1.5724\n",
      "Epoch [4/5], Step [2760/6250], Loss: 2.0163\n",
      "Epoch [4/5], Step [2770/6250], Loss: 2.7232\n",
      "Epoch [4/5], Step [2780/6250], Loss: 1.8498\n",
      "Epoch [4/5], Step [2790/6250], Loss: 2.3580\n",
      "Epoch [4/5], Step [2800/6250], Loss: 2.1880\n",
      "Epoch [4/5], Step [2810/6250], Loss: 3.0969\n",
      "Epoch [4/5], Step [2820/6250], Loss: 2.4539\n",
      "Epoch [4/5], Step [2830/6250], Loss: 1.6493\n",
      "Epoch [4/5], Step [2840/6250], Loss: 1.1132\n",
      "Epoch [4/5], Step [2850/6250], Loss: 2.3157\n",
      "Epoch [4/5], Step [2860/6250], Loss: 2.4578\n",
      "Epoch [4/5], Step [2870/6250], Loss: 2.1016\n",
      "Epoch [4/5], Step [2880/6250], Loss: 2.5530\n",
      "Epoch [4/5], Step [2890/6250], Loss: 1.9001\n",
      "Epoch [4/5], Step [2900/6250], Loss: 2.0303\n",
      "Epoch [4/5], Step [2910/6250], Loss: 2.1425\n",
      "Epoch [4/5], Step [2920/6250], Loss: 2.4763\n",
      "Epoch [4/5], Step [2930/6250], Loss: 1.7041\n",
      "Epoch [4/5], Step [2940/6250], Loss: 2.5655\n",
      "Epoch [4/5], Step [2950/6250], Loss: 2.0629\n",
      "Epoch [4/5], Step [2960/6250], Loss: 1.6944\n",
      "Epoch [4/5], Step [2970/6250], Loss: 3.0830\n",
      "Epoch [4/5], Step [2980/6250], Loss: 2.3425\n",
      "Epoch [4/5], Step [2990/6250], Loss: 2.5195\n",
      "Epoch [4/5], Step [3000/6250], Loss: 2.5340\n",
      "Epoch [4/5], Step [3010/6250], Loss: 2.5436\n",
      "Epoch [4/5], Step [3020/6250], Loss: 3.5256\n",
      "Epoch [4/5], Step [3030/6250], Loss: 2.2623\n",
      "Epoch [4/5], Step [3040/6250], Loss: 2.9946\n",
      "Epoch [4/5], Step [3050/6250], Loss: 1.3792\n",
      "Epoch [4/5], Step [3060/6250], Loss: 1.2908\n",
      "Epoch [4/5], Step [3070/6250], Loss: 2.2440\n",
      "Epoch [4/5], Step [3080/6250], Loss: 2.4956\n",
      "Epoch [4/5], Step [3090/6250], Loss: 1.6833\n",
      "Epoch [4/5], Step [3100/6250], Loss: 2.0986\n",
      "Epoch [4/5], Step [3110/6250], Loss: 2.4737\n",
      "Epoch [4/5], Step [3120/6250], Loss: 2.1066\n",
      "Epoch [4/5], Step [3130/6250], Loss: 3.1435\n",
      "Epoch [4/5], Step [3140/6250], Loss: 2.1839\n",
      "Epoch [4/5], Step [3150/6250], Loss: 3.1463\n",
      "Epoch [4/5], Step [3160/6250], Loss: 2.3682\n",
      "Epoch [4/5], Step [3170/6250], Loss: 3.7368\n",
      "Epoch [4/5], Step [3180/6250], Loss: 1.9643\n",
      "Epoch [4/5], Step [3190/6250], Loss: 2.3914\n",
      "Epoch [4/5], Step [3200/6250], Loss: 2.0287\n",
      "Epoch [4/5], Step [3210/6250], Loss: 2.8491\n",
      "Epoch [4/5], Step [3220/6250], Loss: 1.9013\n",
      "Epoch [4/5], Step [3230/6250], Loss: 1.8730\n",
      "Epoch [4/5], Step [3240/6250], Loss: 2.8805\n",
      "Epoch [4/5], Step [3250/6250], Loss: 3.1128\n",
      "Epoch [4/5], Step [3260/6250], Loss: 2.3733\n",
      "Epoch [4/5], Step [3270/6250], Loss: 2.5594\n",
      "Epoch [4/5], Step [3280/6250], Loss: 3.1418\n",
      "Epoch [4/5], Step [3290/6250], Loss: 2.1520\n",
      "Epoch [4/5], Step [3300/6250], Loss: 1.9728\n",
      "Epoch [4/5], Step [3310/6250], Loss: 3.3104\n",
      "Epoch [4/5], Step [3320/6250], Loss: 3.1165\n",
      "Epoch [4/5], Step [3330/6250], Loss: 2.6911\n",
      "Epoch [4/5], Step [3340/6250], Loss: 1.7185\n",
      "Epoch [4/5], Step [3350/6250], Loss: 2.0255\n",
      "Epoch [4/5], Step [3360/6250], Loss: 1.4831\n",
      "Epoch [4/5], Step [3370/6250], Loss: 2.4464\n",
      "Epoch [4/5], Step [3380/6250], Loss: 1.6519\n",
      "Epoch [4/5], Step [3390/6250], Loss: 2.1416\n",
      "Epoch [4/5], Step [3400/6250], Loss: 3.1187\n",
      "Epoch [4/5], Step [3410/6250], Loss: 2.0267\n",
      "Epoch [4/5], Step [3420/6250], Loss: 1.7757\n",
      "Epoch [4/5], Step [3430/6250], Loss: 3.2125\n",
      "Epoch [4/5], Step [3440/6250], Loss: 1.5602\n",
      "Epoch [4/5], Step [3450/6250], Loss: 2.9851\n",
      "Epoch [4/5], Step [3460/6250], Loss: 1.4603\n",
      "Epoch [4/5], Step [3470/6250], Loss: 3.1756\n",
      "Epoch [4/5], Step [3480/6250], Loss: 1.9570\n",
      "Epoch [4/5], Step [3490/6250], Loss: 1.9385\n",
      "Epoch [4/5], Step [3500/6250], Loss: 2.0309\n",
      "Epoch [4/5], Step [3510/6250], Loss: 2.5164\n",
      "Epoch [4/5], Step [3520/6250], Loss: 2.5200\n",
      "Epoch [4/5], Step [3530/6250], Loss: 2.3161\n",
      "Epoch [4/5], Step [3540/6250], Loss: 3.4839\n",
      "Epoch [4/5], Step [3550/6250], Loss: 2.0277\n",
      "Epoch [4/5], Step [3560/6250], Loss: 2.2077\n",
      "Epoch [4/5], Step [3570/6250], Loss: 1.4585\n",
      "Epoch [4/5], Step [3580/6250], Loss: 2.8465\n",
      "Epoch [4/5], Step [3590/6250], Loss: 3.6410\n",
      "Epoch [4/5], Step [3600/6250], Loss: 1.4076\n",
      "Epoch [4/5], Step [3610/6250], Loss: 1.6419\n",
      "Epoch [4/5], Step [3620/6250], Loss: 3.0260\n",
      "Epoch [4/5], Step [3630/6250], Loss: 1.5606\n",
      "Epoch [4/5], Step [3640/6250], Loss: 2.1409\n",
      "Epoch [4/5], Step [3650/6250], Loss: 2.6219\n",
      "Epoch [4/5], Step [3660/6250], Loss: 1.4460\n",
      "Epoch [4/5], Step [3670/6250], Loss: 1.6175\n",
      "Epoch [4/5], Step [3680/6250], Loss: 2.8843\n",
      "Epoch [4/5], Step [3690/6250], Loss: 4.1008\n",
      "Epoch [4/5], Step [3700/6250], Loss: 2.8470\n",
      "Epoch [4/5], Step [3710/6250], Loss: 2.0840\n",
      "Epoch [4/5], Step [3720/6250], Loss: 3.1406\n",
      "Epoch [4/5], Step [3730/6250], Loss: 2.0674\n",
      "Epoch [4/5], Step [3740/6250], Loss: 2.1747\n",
      "Epoch [4/5], Step [3750/6250], Loss: 2.8825\n",
      "Epoch [4/5], Step [3760/6250], Loss: 3.0916\n",
      "Epoch [4/5], Step [3770/6250], Loss: 2.2784\n",
      "Epoch [4/5], Step [3780/6250], Loss: 2.8126\n",
      "Epoch [4/5], Step [3790/6250], Loss: 2.3803\n",
      "Epoch [4/5], Step [3800/6250], Loss: 1.7113\n",
      "Epoch [4/5], Step [3810/6250], Loss: 2.1126\n",
      "Epoch [4/5], Step [3820/6250], Loss: 2.1758\n",
      "Epoch [4/5], Step [3830/6250], Loss: 2.0738\n",
      "Epoch [4/5], Step [3840/6250], Loss: 2.6156\n",
      "Epoch [4/5], Step [3850/6250], Loss: 2.1578\n",
      "Epoch [4/5], Step [3860/6250], Loss: 2.7362\n",
      "Epoch [4/5], Step [3870/6250], Loss: 2.5881\n",
      "Epoch [4/5], Step [3880/6250], Loss: 1.8369\n",
      "Epoch [4/5], Step [3890/6250], Loss: 2.3096\n",
      "Epoch [4/5], Step [3900/6250], Loss: 3.7858\n",
      "Epoch [4/5], Step [3910/6250], Loss: 2.4929\n",
      "Epoch [4/5], Step [3920/6250], Loss: 2.3291\n",
      "Epoch [4/5], Step [3930/6250], Loss: 1.9762\n",
      "Epoch [4/5], Step [3940/6250], Loss: 2.4211\n",
      "Epoch [4/5], Step [3950/6250], Loss: 2.5710\n",
      "Epoch [4/5], Step [3960/6250], Loss: 1.7405\n",
      "Epoch [4/5], Step [3970/6250], Loss: 2.8845\n",
      "Epoch [4/5], Step [3980/6250], Loss: 2.1034\n",
      "Epoch [4/5], Step [3990/6250], Loss: 2.0101\n",
      "Epoch [4/5], Step [4000/6250], Loss: 1.6241\n",
      "Epoch [4/5], Step [4010/6250], Loss: 3.3183\n",
      "Epoch [4/5], Step [4020/6250], Loss: 2.8256\n",
      "Epoch [4/5], Step [4030/6250], Loss: 3.2502\n",
      "Epoch [4/5], Step [4040/6250], Loss: 2.0569\n",
      "Epoch [4/5], Step [4050/6250], Loss: 2.3851\n",
      "Epoch [4/5], Step [4060/6250], Loss: 2.4822\n",
      "Epoch [4/5], Step [4070/6250], Loss: 3.0282\n",
      "Epoch [4/5], Step [4080/6250], Loss: 3.3108\n",
      "Epoch [4/5], Step [4090/6250], Loss: 2.8473\n",
      "Epoch [4/5], Step [4100/6250], Loss: 2.1823\n",
      "Epoch [4/5], Step [4110/6250], Loss: 1.4667\n",
      "Epoch [4/5], Step [4120/6250], Loss: 1.2682\n",
      "Epoch [4/5], Step [4130/6250], Loss: 1.9297\n",
      "Epoch [4/5], Step [4140/6250], Loss: 1.5827\n",
      "Epoch [4/5], Step [4150/6250], Loss: 2.3150\n",
      "Epoch [4/5], Step [4160/6250], Loss: 2.0295\n",
      "Epoch [4/5], Step [4170/6250], Loss: 1.0182\n",
      "Epoch [4/5], Step [4180/6250], Loss: 1.3981\n",
      "Epoch [4/5], Step [4190/6250], Loss: 1.4486\n",
      "Epoch [4/5], Step [4200/6250], Loss: 3.4013\n",
      "Epoch [4/5], Step [4210/6250], Loss: 2.7323\n",
      "Epoch [4/5], Step [4220/6250], Loss: 2.0107\n",
      "Epoch [4/5], Step [4230/6250], Loss: 2.4054\n",
      "Epoch [4/5], Step [4240/6250], Loss: 2.0336\n",
      "Epoch [4/5], Step [4250/6250], Loss: 2.3442\n",
      "Epoch [4/5], Step [4260/6250], Loss: 1.3620\n",
      "Epoch [4/5], Step [4270/6250], Loss: 2.8136\n",
      "Epoch [4/5], Step [4280/6250], Loss: 2.7273\n",
      "Epoch [4/5], Step [4290/6250], Loss: 2.5445\n",
      "Epoch [4/5], Step [4300/6250], Loss: 1.5994\n",
      "Epoch [4/5], Step [4310/6250], Loss: 2.3218\n",
      "Epoch [4/5], Step [4320/6250], Loss: 2.0043\n",
      "Epoch [4/5], Step [4330/6250], Loss: 2.3754\n",
      "Epoch [4/5], Step [4340/6250], Loss: 3.3652\n",
      "Epoch [4/5], Step [4350/6250], Loss: 2.3352\n",
      "Epoch [4/5], Step [4360/6250], Loss: 2.1705\n",
      "Epoch [4/5], Step [4370/6250], Loss: 2.2467\n",
      "Epoch [4/5], Step [4380/6250], Loss: 2.5798\n",
      "Epoch [4/5], Step [4390/6250], Loss: 2.8830\n",
      "Epoch [4/5], Step [4400/6250], Loss: 2.7183\n",
      "Epoch [4/5], Step [4410/6250], Loss: 1.3255\n",
      "Epoch [4/5], Step [4420/6250], Loss: 2.2181\n",
      "Epoch [4/5], Step [4430/6250], Loss: 2.5110\n",
      "Epoch [4/5], Step [4440/6250], Loss: 3.5200\n",
      "Epoch [4/5], Step [4450/6250], Loss: 1.5715\n",
      "Epoch [4/5], Step [4460/6250], Loss: 1.9277\n",
      "Epoch [4/5], Step [4470/6250], Loss: 3.1977\n",
      "Epoch [4/5], Step [4480/6250], Loss: 1.8851\n",
      "Epoch [4/5], Step [4490/6250], Loss: 2.4690\n",
      "Epoch [4/5], Step [4500/6250], Loss: 3.4634\n",
      "Epoch [4/5], Step [4510/6250], Loss: 2.2770\n",
      "Epoch [4/5], Step [4520/6250], Loss: 2.2237\n",
      "Epoch [4/5], Step [4530/6250], Loss: 1.6390\n",
      "Epoch [4/5], Step [4540/6250], Loss: 2.4762\n",
      "Epoch [4/5], Step [4550/6250], Loss: 2.3702\n",
      "Epoch [4/5], Step [4560/6250], Loss: 3.0616\n",
      "Epoch [4/5], Step [4570/6250], Loss: 1.5855\n",
      "Epoch [4/5], Step [4580/6250], Loss: 1.8380\n",
      "Epoch [4/5], Step [4590/6250], Loss: 1.8796\n",
      "Epoch [4/5], Step [4600/6250], Loss: 2.7116\n",
      "Epoch [4/5], Step [4610/6250], Loss: 2.0144\n",
      "Epoch [4/5], Step [4620/6250], Loss: 1.8837\n",
      "Epoch [4/5], Step [4630/6250], Loss: 3.3686\n",
      "Epoch [4/5], Step [4640/6250], Loss: 3.1511\n",
      "Epoch [4/5], Step [4650/6250], Loss: 2.3305\n",
      "Epoch [4/5], Step [4660/6250], Loss: 1.8042\n",
      "Epoch [4/5], Step [4670/6250], Loss: 1.7990\n",
      "Epoch [4/5], Step [4680/6250], Loss: 2.8690\n",
      "Epoch [4/5], Step [4690/6250], Loss: 2.1270\n",
      "Epoch [4/5], Step [4700/6250], Loss: 2.7222\n",
      "Epoch [4/5], Step [4710/6250], Loss: 3.0866\n",
      "Epoch [4/5], Step [4720/6250], Loss: 3.6577\n",
      "Epoch [4/5], Step [4730/6250], Loss: 2.1141\n",
      "Epoch [4/5], Step [4740/6250], Loss: 3.7457\n",
      "Epoch [4/5], Step [4750/6250], Loss: 1.7688\n",
      "Epoch [4/5], Step [4760/6250], Loss: 2.2118\n",
      "Epoch [4/5], Step [4770/6250], Loss: 2.4294\n",
      "Epoch [4/5], Step [4780/6250], Loss: 1.6412\n",
      "Epoch [4/5], Step [4790/6250], Loss: 2.8850\n",
      "Epoch [4/5], Step [4800/6250], Loss: 2.9520\n",
      "Epoch [4/5], Step [4810/6250], Loss: 1.5835\n",
      "Epoch [4/5], Step [4820/6250], Loss: 2.0415\n",
      "Epoch [4/5], Step [4830/6250], Loss: 1.9160\n",
      "Epoch [4/5], Step [4840/6250], Loss: 1.8397\n",
      "Epoch [4/5], Step [4850/6250], Loss: 2.3879\n",
      "Epoch [4/5], Step [4860/6250], Loss: 2.5768\n",
      "Epoch [4/5], Step [4870/6250], Loss: 1.8819\n",
      "Epoch [4/5], Step [4880/6250], Loss: 1.9510\n",
      "Epoch [4/5], Step [4890/6250], Loss: 3.1197\n",
      "Epoch [4/5], Step [4900/6250], Loss: 2.8564\n",
      "Epoch [4/5], Step [4910/6250], Loss: 2.2151\n",
      "Epoch [4/5], Step [4920/6250], Loss: 1.6935\n",
      "Epoch [4/5], Step [4930/6250], Loss: 2.5272\n",
      "Epoch [4/5], Step [4940/6250], Loss: 1.8702\n",
      "Epoch [4/5], Step [4950/6250], Loss: 1.9692\n",
      "Epoch [4/5], Step [4960/6250], Loss: 2.4527\n",
      "Epoch [4/5], Step [4970/6250], Loss: 1.8643\n",
      "Epoch [4/5], Step [4980/6250], Loss: 3.7340\n",
      "Epoch [4/5], Step [4990/6250], Loss: 2.6966\n",
      "Epoch [4/5], Step [5000/6250], Loss: 1.8335\n",
      "Epoch [4/5], Step [5010/6250], Loss: 2.8859\n",
      "Epoch [4/5], Step [5020/6250], Loss: 1.7881\n",
      "Epoch [4/5], Step [5030/6250], Loss: 1.5346\n",
      "Epoch [4/5], Step [5040/6250], Loss: 1.9433\n",
      "Epoch [4/5], Step [5050/6250], Loss: 3.2206\n",
      "Epoch [4/5], Step [5060/6250], Loss: 1.9530\n",
      "Epoch [4/5], Step [5070/6250], Loss: 2.7097\n",
      "Epoch [4/5], Step [5080/6250], Loss: 2.2909\n",
      "Epoch [4/5], Step [5090/6250], Loss: 2.9970\n",
      "Epoch [4/5], Step [5100/6250], Loss: 1.2774\n",
      "Epoch [4/5], Step [5110/6250], Loss: 2.9805\n",
      "Epoch [4/5], Step [5120/6250], Loss: 2.4856\n",
      "Epoch [4/5], Step [5130/6250], Loss: 2.3755\n",
      "Epoch [4/5], Step [5140/6250], Loss: 2.2550\n",
      "Epoch [4/5], Step [5150/6250], Loss: 2.1372\n",
      "Epoch [4/5], Step [5160/6250], Loss: 2.6529\n",
      "Epoch [4/5], Step [5170/6250], Loss: 2.2735\n",
      "Epoch [4/5], Step [5180/6250], Loss: 3.1165\n",
      "Epoch [4/5], Step [5190/6250], Loss: 2.2707\n",
      "Epoch [4/5], Step [5200/6250], Loss: 3.2782\n",
      "Epoch [4/5], Step [5210/6250], Loss: 1.7374\n",
      "Epoch [4/5], Step [5220/6250], Loss: 2.2832\n",
      "Epoch [4/5], Step [5230/6250], Loss: 1.4942\n",
      "Epoch [4/5], Step [5240/6250], Loss: 2.3345\n",
      "Epoch [4/5], Step [5250/6250], Loss: 2.7757\n",
      "Epoch [4/5], Step [5260/6250], Loss: 2.3794\n",
      "Epoch [4/5], Step [5270/6250], Loss: 1.3539\n",
      "Epoch [4/5], Step [5280/6250], Loss: 1.9519\n",
      "Epoch [4/5], Step [5290/6250], Loss: 1.6543\n",
      "Epoch [4/5], Step [5300/6250], Loss: 1.6572\n",
      "Epoch [4/5], Step [5310/6250], Loss: 2.2510\n",
      "Epoch [4/5], Step [5320/6250], Loss: 2.8873\n",
      "Epoch [4/5], Step [5330/6250], Loss: 1.6889\n",
      "Epoch [4/5], Step [5340/6250], Loss: 3.3758\n",
      "Epoch [4/5], Step [5350/6250], Loss: 2.5870\n",
      "Epoch [4/5], Step [5360/6250], Loss: 1.7372\n",
      "Epoch [4/5], Step [5370/6250], Loss: 1.5069\n",
      "Epoch [4/5], Step [5380/6250], Loss: 2.8477\n",
      "Epoch [4/5], Step [5390/6250], Loss: 3.0980\n",
      "Epoch [4/5], Step [5400/6250], Loss: 2.2609\n",
      "Epoch [4/5], Step [5410/6250], Loss: 2.2759\n",
      "Epoch [4/5], Step [5420/6250], Loss: 1.4463\n",
      "Epoch [4/5], Step [5430/6250], Loss: 2.4964\n",
      "Epoch [4/5], Step [5440/6250], Loss: 1.8193\n",
      "Epoch [4/5], Step [5450/6250], Loss: 2.3020\n",
      "Epoch [4/5], Step [5460/6250], Loss: 2.3781\n",
      "Epoch [4/5], Step [5470/6250], Loss: 3.3384\n",
      "Epoch [4/5], Step [5480/6250], Loss: 2.4658\n",
      "Epoch [4/5], Step [5490/6250], Loss: 2.5387\n",
      "Epoch [4/5], Step [5500/6250], Loss: 2.0749\n",
      "Epoch [4/5], Step [5510/6250], Loss: 2.3489\n",
      "Epoch [4/5], Step [5520/6250], Loss: 1.6150\n",
      "Epoch [4/5], Step [5530/6250], Loss: 3.0354\n",
      "Epoch [4/5], Step [5540/6250], Loss: 2.5279\n",
      "Epoch [4/5], Step [5550/6250], Loss: 3.1252\n",
      "Epoch [4/5], Step [5560/6250], Loss: 2.0696\n",
      "Epoch [4/5], Step [5570/6250], Loss: 1.2331\n",
      "Epoch [4/5], Step [5580/6250], Loss: 2.3384\n",
      "Epoch [4/5], Step [5590/6250], Loss: 3.2322\n",
      "Epoch [4/5], Step [5600/6250], Loss: 1.8879\n",
      "Epoch [4/5], Step [5610/6250], Loss: 2.5724\n",
      "Epoch [4/5], Step [5620/6250], Loss: 2.9026\n",
      "Epoch [4/5], Step [5630/6250], Loss: 2.3531\n",
      "Epoch [4/5], Step [5640/6250], Loss: 1.9710\n",
      "Epoch [4/5], Step [5650/6250], Loss: 1.6254\n",
      "Epoch [4/5], Step [5660/6250], Loss: 3.1106\n",
      "Epoch [4/5], Step [5670/6250], Loss: 3.1446\n",
      "Epoch [4/5], Step [5680/6250], Loss: 2.0700\n",
      "Epoch [4/5], Step [5690/6250], Loss: 2.9767\n",
      "Epoch [4/5], Step [5700/6250], Loss: 2.8290\n",
      "Epoch [4/5], Step [5710/6250], Loss: 2.2313\n",
      "Epoch [4/5], Step [5720/6250], Loss: 1.8246\n",
      "Epoch [4/5], Step [5730/6250], Loss: 2.5369\n",
      "Epoch [4/5], Step [5740/6250], Loss: 2.5348\n",
      "Epoch [4/5], Step [5750/6250], Loss: 2.4760\n",
      "Epoch [4/5], Step [5760/6250], Loss: 1.8906\n",
      "Epoch [4/5], Step [5770/6250], Loss: 2.0658\n",
      "Epoch [4/5], Step [5780/6250], Loss: 2.8120\n",
      "Epoch [4/5], Step [5790/6250], Loss: 2.1175\n",
      "Epoch [4/5], Step [5800/6250], Loss: 2.5113\n",
      "Epoch [4/5], Step [5810/6250], Loss: 2.7957\n",
      "Epoch [4/5], Step [5820/6250], Loss: 2.8514\n",
      "Epoch [4/5], Step [5830/6250], Loss: 2.8160\n",
      "Epoch [4/5], Step [5840/6250], Loss: 2.0998\n",
      "Epoch [4/5], Step [5850/6250], Loss: 1.4347\n",
      "Epoch [4/5], Step [5860/6250], Loss: 1.8286\n",
      "Epoch [4/5], Step [5870/6250], Loss: 2.6514\n",
      "Epoch [4/5], Step [5880/6250], Loss: 2.2292\n",
      "Epoch [4/5], Step [5890/6250], Loss: 3.0799\n",
      "Epoch [4/5], Step [5900/6250], Loss: 2.7942\n",
      "Epoch [4/5], Step [5910/6250], Loss: 1.0525\n",
      "Epoch [4/5], Step [5920/6250], Loss: 2.4932\n",
      "Epoch [4/5], Step [5930/6250], Loss: 2.7434\n",
      "Epoch [4/5], Step [5940/6250], Loss: 1.8245\n",
      "Epoch [4/5], Step [5950/6250], Loss: 2.6725\n",
      "Epoch [4/5], Step [5960/6250], Loss: 2.1533\n",
      "Epoch [4/5], Step [5970/6250], Loss: 2.6367\n",
      "Epoch [4/5], Step [5980/6250], Loss: 2.9056\n",
      "Epoch [4/5], Step [5990/6250], Loss: 1.6895\n",
      "Epoch [4/5], Step [6000/6250], Loss: 2.3109\n",
      "Epoch [4/5], Step [6010/6250], Loss: 2.4829\n",
      "Epoch [4/5], Step [6020/6250], Loss: 2.3148\n",
      "Epoch [4/5], Step [6030/6250], Loss: 2.6030\n",
      "Epoch [4/5], Step [6040/6250], Loss: 2.7519\n",
      "Epoch [4/5], Step [6050/6250], Loss: 3.0608\n",
      "Epoch [4/5], Step [6060/6250], Loss: 2.3083\n",
      "Epoch [4/5], Step [6070/6250], Loss: 2.6284\n",
      "Epoch [4/5], Step [6080/6250], Loss: 2.2027\n",
      "Epoch [4/5], Step [6090/6250], Loss: 1.8569\n",
      "Epoch [4/5], Step [6100/6250], Loss: 2.3101\n",
      "Epoch [4/5], Step [6110/6250], Loss: 1.3593\n",
      "Epoch [4/5], Step [6120/6250], Loss: 3.2851\n",
      "Epoch [4/5], Step [6130/6250], Loss: 1.5862\n",
      "Epoch [4/5], Step [6140/6250], Loss: 2.9242\n",
      "Epoch [4/5], Step [6150/6250], Loss: 2.6247\n",
      "Epoch [4/5], Step [6160/6250], Loss: 2.3636\n",
      "Epoch [4/5], Step [6170/6250], Loss: 2.1038\n",
      "Epoch [4/5], Step [6180/6250], Loss: 2.9892\n",
      "Epoch [4/5], Step [6190/6250], Loss: 1.9974\n",
      "Epoch [4/5], Step [6200/6250], Loss: 1.8478\n",
      "Epoch [4/5], Step [6210/6250], Loss: 2.2681\n",
      "Epoch [4/5], Step [6220/6250], Loss: 2.4543\n",
      "Epoch [4/5], Step [6230/6250], Loss: 2.1307\n",
      "Epoch [4/5], Step [6240/6250], Loss: 1.1955\n",
      "Epoch [4/5], Step [6250/6250], Loss: 2.8480\n",
      "Epoch 4, Loss: 2.848015069961548\n",
      "Epoch [5/5], Step [10/6250], Loss: 2.2689\n",
      "Epoch [5/5], Step [20/6250], Loss: 2.4802\n",
      "Epoch [5/5], Step [30/6250], Loss: 2.4670\n",
      "Epoch [5/5], Step [40/6250], Loss: 2.3036\n",
      "Epoch [5/5], Step [50/6250], Loss: 2.4634\n",
      "Epoch [5/5], Step [60/6250], Loss: 1.9339\n",
      "Epoch [5/5], Step [70/6250], Loss: 2.1815\n",
      "Epoch [5/5], Step [80/6250], Loss: 2.5980\n",
      "Epoch [5/5], Step [90/6250], Loss: 1.5917\n",
      "Epoch [5/5], Step [100/6250], Loss: 2.0817\n",
      "Epoch [5/5], Step [110/6250], Loss: 1.9575\n",
      "Epoch [5/5], Step [120/6250], Loss: 2.3931\n",
      "Epoch [5/5], Step [130/6250], Loss: 1.3519\n",
      "Epoch [5/5], Step [140/6250], Loss: 1.7602\n",
      "Epoch [5/5], Step [150/6250], Loss: 1.8715\n",
      "Epoch [5/5], Step [160/6250], Loss: 1.5257\n",
      "Epoch [5/5], Step [170/6250], Loss: 1.6755\n",
      "Epoch [5/5], Step [180/6250], Loss: 2.3202\n",
      "Epoch [5/5], Step [190/6250], Loss: 3.5097\n",
      "Epoch [5/5], Step [200/6250], Loss: 2.1894\n",
      "Epoch [5/5], Step [210/6250], Loss: 2.3813\n",
      "Epoch [5/5], Step [220/6250], Loss: 2.4479\n",
      "Epoch [5/5], Step [230/6250], Loss: 2.5478\n",
      "Epoch [5/5], Step [240/6250], Loss: 1.4589\n",
      "Epoch [5/5], Step [250/6250], Loss: 2.1083\n",
      "Epoch [5/5], Step [260/6250], Loss: 3.0755\n",
      "Epoch [5/5], Step [270/6250], Loss: 2.0756\n",
      "Epoch [5/5], Step [280/6250], Loss: 1.9614\n",
      "Epoch [5/5], Step [290/6250], Loss: 2.0505\n",
      "Epoch [5/5], Step [300/6250], Loss: 2.8592\n",
      "Epoch [5/5], Step [310/6250], Loss: 2.7564\n",
      "Epoch [5/5], Step [320/6250], Loss: 1.6564\n",
      "Epoch [5/5], Step [330/6250], Loss: 1.6526\n",
      "Epoch [5/5], Step [340/6250], Loss: 1.4929\n",
      "Epoch [5/5], Step [350/6250], Loss: 2.1476\n",
      "Epoch [5/5], Step [360/6250], Loss: 3.6808\n",
      "Epoch [5/5], Step [370/6250], Loss: 2.1631\n",
      "Epoch [5/5], Step [380/6250], Loss: 1.5415\n",
      "Epoch [5/5], Step [390/6250], Loss: 1.1084\n",
      "Epoch [5/5], Step [400/6250], Loss: 2.7295\n",
      "Epoch [5/5], Step [410/6250], Loss: 2.2299\n",
      "Epoch [5/5], Step [420/6250], Loss: 2.0679\n",
      "Epoch [5/5], Step [430/6250], Loss: 2.0380\n",
      "Epoch [5/5], Step [440/6250], Loss: 2.1570\n",
      "Epoch [5/5], Step [450/6250], Loss: 1.3728\n",
      "Epoch [5/5], Step [460/6250], Loss: 2.4043\n",
      "Epoch [5/5], Step [470/6250], Loss: 3.1200\n",
      "Epoch [5/5], Step [480/6250], Loss: 1.6411\n",
      "Epoch [5/5], Step [490/6250], Loss: 2.3079\n",
      "Epoch [5/5], Step [500/6250], Loss: 1.6201\n",
      "Epoch [5/5], Step [510/6250], Loss: 2.0618\n",
      "Epoch [5/5], Step [520/6250], Loss: 1.9205\n",
      "Epoch [5/5], Step [530/6250], Loss: 2.8195\n",
      "Epoch [5/5], Step [540/6250], Loss: 1.8659\n",
      "Epoch [5/5], Step [550/6250], Loss: 1.3816\n",
      "Epoch [5/5], Step [560/6250], Loss: 2.3758\n",
      "Epoch [5/5], Step [570/6250], Loss: 2.3346\n",
      "Epoch [5/5], Step [580/6250], Loss: 1.2741\n",
      "Epoch [5/5], Step [590/6250], Loss: 1.4666\n",
      "Epoch [5/5], Step [600/6250], Loss: 1.1247\n",
      "Epoch [5/5], Step [610/6250], Loss: 3.1797\n",
      "Epoch [5/5], Step [620/6250], Loss: 1.3748\n",
      "Epoch [5/5], Step [630/6250], Loss: 3.1306\n",
      "Epoch [5/5], Step [640/6250], Loss: 2.3918\n",
      "Epoch [5/5], Step [650/6250], Loss: 1.8317\n",
      "Epoch [5/5], Step [660/6250], Loss: 1.9161\n",
      "Epoch [5/5], Step [670/6250], Loss: 2.0924\n",
      "Epoch [5/5], Step [680/6250], Loss: 2.2308\n",
      "Epoch [5/5], Step [690/6250], Loss: 2.1504\n",
      "Epoch [5/5], Step [700/6250], Loss: 2.1093\n",
      "Epoch [5/5], Step [710/6250], Loss: 2.5660\n",
      "Epoch [5/5], Step [720/6250], Loss: 2.6831\n",
      "Epoch [5/5], Step [730/6250], Loss: 2.0435\n",
      "Epoch [5/5], Step [740/6250], Loss: 2.3230\n",
      "Epoch [5/5], Step [750/6250], Loss: 2.2009\n",
      "Epoch [5/5], Step [760/6250], Loss: 1.7191\n",
      "Epoch [5/5], Step [770/6250], Loss: 2.1816\n",
      "Epoch [5/5], Step [780/6250], Loss: 3.2380\n",
      "Epoch [5/5], Step [790/6250], Loss: 1.0829\n",
      "Epoch [5/5], Step [800/6250], Loss: 1.4821\n",
      "Epoch [5/5], Step [810/6250], Loss: 1.4885\n",
      "Epoch [5/5], Step [820/6250], Loss: 2.5815\n",
      "Epoch [5/5], Step [830/6250], Loss: 1.4777\n",
      "Epoch [5/5], Step [840/6250], Loss: 1.6877\n",
      "Epoch [5/5], Step [850/6250], Loss: 1.5336\n",
      "Epoch [5/5], Step [860/6250], Loss: 2.8131\n",
      "Epoch [5/5], Step [870/6250], Loss: 1.1528\n",
      "Epoch [5/5], Step [880/6250], Loss: 2.1257\n",
      "Epoch [5/5], Step [890/6250], Loss: 2.0592\n",
      "Epoch [5/5], Step [900/6250], Loss: 1.7812\n",
      "Epoch [5/5], Step [910/6250], Loss: 1.6313\n",
      "Epoch [5/5], Step [920/6250], Loss: 1.7786\n",
      "Epoch [5/5], Step [930/6250], Loss: 1.4948\n",
      "Epoch [5/5], Step [940/6250], Loss: 1.4924\n",
      "Epoch [5/5], Step [950/6250], Loss: 1.8257\n",
      "Epoch [5/5], Step [960/6250], Loss: 2.6767\n",
      "Epoch [5/5], Step [970/6250], Loss: 3.3172\n",
      "Epoch [5/5], Step [980/6250], Loss: 1.2609\n",
      "Epoch [5/5], Step [990/6250], Loss: 2.5201\n",
      "Epoch [5/5], Step [1000/6250], Loss: 2.5978\n",
      "Epoch [5/5], Step [1010/6250], Loss: 2.5968\n",
      "Epoch [5/5], Step [1020/6250], Loss: 1.7075\n",
      "Epoch [5/5], Step [1030/6250], Loss: 2.1020\n",
      "Epoch [5/5], Step [1040/6250], Loss: 1.6520\n",
      "Epoch [5/5], Step [1050/6250], Loss: 2.2186\n",
      "Epoch [5/5], Step [1060/6250], Loss: 2.0838\n",
      "Epoch [5/5], Step [1070/6250], Loss: 2.4306\n",
      "Epoch [5/5], Step [1080/6250], Loss: 1.6158\n",
      "Epoch [5/5], Step [1090/6250], Loss: 1.4989\n",
      "Epoch [5/5], Step [1100/6250], Loss: 2.7555\n",
      "Epoch [5/5], Step [1110/6250], Loss: 2.4980\n",
      "Epoch [5/5], Step [1120/6250], Loss: 1.7741\n",
      "Epoch [5/5], Step [1130/6250], Loss: 2.3678\n",
      "Epoch [5/5], Step [1140/6250], Loss: 1.8657\n",
      "Epoch [5/5], Step [1150/6250], Loss: 1.9601\n",
      "Epoch [5/5], Step [1160/6250], Loss: 2.1464\n",
      "Epoch [5/5], Step [1170/6250], Loss: 2.6237\n",
      "Epoch [5/5], Step [1180/6250], Loss: 2.6312\n",
      "Epoch [5/5], Step [1190/6250], Loss: 2.9960\n",
      "Epoch [5/5], Step [1200/6250], Loss: 1.2121\n",
      "Epoch [5/5], Step [1210/6250], Loss: 2.5279\n",
      "Epoch [5/5], Step [1220/6250], Loss: 2.0500\n",
      "Epoch [5/5], Step [1230/6250], Loss: 1.6034\n",
      "Epoch [5/5], Step [1240/6250], Loss: 3.1055\n",
      "Epoch [5/5], Step [1250/6250], Loss: 2.4257\n",
      "Epoch [5/5], Step [1260/6250], Loss: 1.6407\n",
      "Epoch [5/5], Step [1270/6250], Loss: 2.5145\n",
      "Epoch [5/5], Step [1280/6250], Loss: 4.0647\n",
      "Epoch [5/5], Step [1290/6250], Loss: 2.3653\n",
      "Epoch [5/5], Step [1300/6250], Loss: 2.7950\n",
      "Epoch [5/5], Step [1310/6250], Loss: 4.1939\n",
      "Epoch [5/5], Step [1320/6250], Loss: 1.3384\n",
      "Epoch [5/5], Step [1330/6250], Loss: 2.8572\n",
      "Epoch [5/5], Step [1340/6250], Loss: 2.3784\n",
      "Epoch [5/5], Step [1350/6250], Loss: 1.6884\n",
      "Epoch [5/5], Step [1360/6250], Loss: 2.4519\n",
      "Epoch [5/5], Step [1370/6250], Loss: 2.1697\n",
      "Epoch [5/5], Step [1380/6250], Loss: 2.5113\n",
      "Epoch [5/5], Step [1390/6250], Loss: 2.3576\n",
      "Epoch [5/5], Step [1400/6250], Loss: 2.6326\n",
      "Epoch [5/5], Step [1410/6250], Loss: 1.2574\n",
      "Epoch [5/5], Step [1420/6250], Loss: 2.2998\n",
      "Epoch [5/5], Step [1430/6250], Loss: 1.9828\n",
      "Epoch [5/5], Step [1440/6250], Loss: 2.3186\n",
      "Epoch [5/5], Step [1450/6250], Loss: 1.7508\n",
      "Epoch [5/5], Step [1460/6250], Loss: 1.7764\n",
      "Epoch [5/5], Step [1470/6250], Loss: 2.1960\n",
      "Epoch [5/5], Step [1480/6250], Loss: 1.9043\n",
      "Epoch [5/5], Step [1490/6250], Loss: 2.2875\n",
      "Epoch [5/5], Step [1500/6250], Loss: 1.3363\n",
      "Epoch [5/5], Step [1510/6250], Loss: 2.6598\n",
      "Epoch [5/5], Step [1520/6250], Loss: 1.2291\n",
      "Epoch [5/5], Step [1530/6250], Loss: 2.2352\n",
      "Epoch [5/5], Step [1540/6250], Loss: 3.1204\n",
      "Epoch [5/5], Step [1550/6250], Loss: 1.8289\n",
      "Epoch [5/5], Step [1560/6250], Loss: 2.6490\n",
      "Epoch [5/5], Step [1570/6250], Loss: 1.7236\n",
      "Epoch [5/5], Step [1580/6250], Loss: 1.3716\n",
      "Epoch [5/5], Step [1590/6250], Loss: 2.7477\n",
      "Epoch [5/5], Step [1600/6250], Loss: 2.0710\n",
      "Epoch [5/5], Step [1610/6250], Loss: 2.2208\n",
      "Epoch [5/5], Step [1620/6250], Loss: 3.0165\n",
      "Epoch [5/5], Step [1630/6250], Loss: 2.4890\n",
      "Epoch [5/5], Step [1640/6250], Loss: 2.3575\n",
      "Epoch [5/5], Step [1650/6250], Loss: 1.8939\n",
      "Epoch [5/5], Step [1660/6250], Loss: 2.5088\n",
      "Epoch [5/5], Step [1670/6250], Loss: 1.5927\n",
      "Epoch [5/5], Step [1680/6250], Loss: 2.3655\n",
      "Epoch [5/5], Step [1690/6250], Loss: 1.8471\n",
      "Epoch [5/5], Step [1700/6250], Loss: 2.3471\n",
      "Epoch [5/5], Step [1710/6250], Loss: 2.3678\n",
      "Epoch [5/5], Step [1720/6250], Loss: 2.9178\n",
      "Epoch [5/5], Step [1730/6250], Loss: 1.8511\n",
      "Epoch [5/5], Step [1740/6250], Loss: 3.1690\n",
      "Epoch [5/5], Step [1750/6250], Loss: 2.1322\n",
      "Epoch [5/5], Step [1760/6250], Loss: 1.7135\n",
      "Epoch [5/5], Step [1770/6250], Loss: 1.2556\n",
      "Epoch [5/5], Step [1780/6250], Loss: 1.6997\n",
      "Epoch [5/5], Step [1790/6250], Loss: 2.0720\n",
      "Epoch [5/5], Step [1800/6250], Loss: 2.1182\n",
      "Epoch [5/5], Step [1810/6250], Loss: 1.4402\n",
      "Epoch [5/5], Step [1820/6250], Loss: 1.9381\n",
      "Epoch [5/5], Step [1830/6250], Loss: 1.1288\n",
      "Epoch [5/5], Step [1840/6250], Loss: 2.3361\n",
      "Epoch [5/5], Step [1850/6250], Loss: 2.0732\n",
      "Epoch [5/5], Step [1860/6250], Loss: 1.7946\n",
      "Epoch [5/5], Step [1870/6250], Loss: 1.5174\n",
      "Epoch [5/5], Step [1880/6250], Loss: 2.1586\n",
      "Epoch [5/5], Step [1890/6250], Loss: 2.7267\n",
      "Epoch [5/5], Step [1900/6250], Loss: 1.7118\n",
      "Epoch [5/5], Step [1910/6250], Loss: 1.8010\n",
      "Epoch [5/5], Step [1920/6250], Loss: 3.3230\n",
      "Epoch [5/5], Step [1930/6250], Loss: 1.2862\n",
      "Epoch [5/5], Step [1940/6250], Loss: 1.5148\n",
      "Epoch [5/5], Step [1950/6250], Loss: 1.6890\n",
      "Epoch [5/5], Step [1960/6250], Loss: 3.0178\n",
      "Epoch [5/5], Step [1970/6250], Loss: 1.5286\n",
      "Epoch [5/5], Step [1980/6250], Loss: 2.5869\n",
      "Epoch [5/5], Step [1990/6250], Loss: 3.9303\n",
      "Epoch [5/5], Step [2000/6250], Loss: 2.6054\n",
      "Epoch [5/5], Step [2010/6250], Loss: 2.5737\n",
      "Epoch [5/5], Step [2020/6250], Loss: 2.4782\n",
      "Epoch [5/5], Step [2030/6250], Loss: 1.2187\n",
      "Epoch [5/5], Step [2040/6250], Loss: 2.1679\n",
      "Epoch [5/5], Step [2050/6250], Loss: 2.0478\n",
      "Epoch [5/5], Step [2060/6250], Loss: 2.1543\n",
      "Epoch [5/5], Step [2070/6250], Loss: 2.7971\n",
      "Epoch [5/5], Step [2080/6250], Loss: 2.1770\n",
      "Epoch [5/5], Step [2090/6250], Loss: 1.6980\n",
      "Epoch [5/5], Step [2100/6250], Loss: 1.0877\n",
      "Epoch [5/5], Step [2110/6250], Loss: 1.4512\n",
      "Epoch [5/5], Step [2120/6250], Loss: 2.3746\n",
      "Epoch [5/5], Step [2130/6250], Loss: 2.5091\n",
      "Epoch [5/5], Step [2140/6250], Loss: 2.9806\n",
      "Epoch [5/5], Step [2150/6250], Loss: 2.1695\n",
      "Epoch [5/5], Step [2160/6250], Loss: 2.3976\n",
      "Epoch [5/5], Step [2170/6250], Loss: 1.7847\n",
      "Epoch [5/5], Step [2180/6250], Loss: 2.9328\n",
      "Epoch [5/5], Step [2190/6250], Loss: 1.5066\n",
      "Epoch [5/5], Step [2200/6250], Loss: 2.1932\n",
      "Epoch [5/5], Step [2210/6250], Loss: 1.8359\n",
      "Epoch [5/5], Step [2220/6250], Loss: 2.6486\n",
      "Epoch [5/5], Step [2230/6250], Loss: 2.5135\n",
      "Epoch [5/5], Step [2240/6250], Loss: 1.8793\n",
      "Epoch [5/5], Step [2250/6250], Loss: 2.0008\n",
      "Epoch [5/5], Step [2260/6250], Loss: 1.9259\n",
      "Epoch [5/5], Step [2270/6250], Loss: 1.8064\n",
      "Epoch [5/5], Step [2280/6250], Loss: 1.8857\n",
      "Epoch [5/5], Step [2290/6250], Loss: 3.2768\n",
      "Epoch [5/5], Step [2300/6250], Loss: 1.3465\n",
      "Epoch [5/5], Step [2310/6250], Loss: 1.6975\n",
      "Epoch [5/5], Step [2320/6250], Loss: 2.2266\n",
      "Epoch [5/5], Step [2330/6250], Loss: 2.2951\n",
      "Epoch [5/5], Step [2340/6250], Loss: 2.3439\n",
      "Epoch [5/5], Step [2350/6250], Loss: 1.1086\n",
      "Epoch [5/5], Step [2360/6250], Loss: 1.3901\n",
      "Epoch [5/5], Step [2370/6250], Loss: 2.0178\n",
      "Epoch [5/5], Step [2380/6250], Loss: 1.9183\n",
      "Epoch [5/5], Step [2390/6250], Loss: 2.2966\n",
      "Epoch [5/5], Step [2400/6250], Loss: 1.6174\n",
      "Epoch [5/5], Step [2410/6250], Loss: 2.0878\n",
      "Epoch [5/5], Step [2420/6250], Loss: 2.3250\n",
      "Epoch [5/5], Step [2430/6250], Loss: 1.8426\n",
      "Epoch [5/5], Step [2440/6250], Loss: 3.8686\n",
      "Epoch [5/5], Step [2450/6250], Loss: 2.3154\n",
      "Epoch [5/5], Step [2460/6250], Loss: 1.5862\n",
      "Epoch [5/5], Step [2470/6250], Loss: 1.8860\n",
      "Epoch [5/5], Step [2480/6250], Loss: 1.3862\n",
      "Epoch [5/5], Step [2490/6250], Loss: 2.8366\n",
      "Epoch [5/5], Step [2500/6250], Loss: 1.7522\n",
      "Epoch [5/5], Step [2510/6250], Loss: 2.9364\n",
      "Epoch [5/5], Step [2520/6250], Loss: 2.0386\n",
      "Epoch [5/5], Step [2530/6250], Loss: 1.1984\n",
      "Epoch [5/5], Step [2540/6250], Loss: 1.9700\n",
      "Epoch [5/5], Step [2550/6250], Loss: 2.2574\n",
      "Epoch [5/5], Step [2560/6250], Loss: 1.6143\n",
      "Epoch [5/5], Step [2570/6250], Loss: 2.6884\n",
      "Epoch [5/5], Step [2580/6250], Loss: 1.8288\n",
      "Epoch [5/5], Step [2590/6250], Loss: 1.8512\n",
      "Epoch [5/5], Step [2600/6250], Loss: 2.2500\n",
      "Epoch [5/5], Step [2610/6250], Loss: 2.1068\n",
      "Epoch [5/5], Step [2620/6250], Loss: 2.3416\n",
      "Epoch [5/5], Step [2630/6250], Loss: 2.6604\n",
      "Epoch [5/5], Step [2640/6250], Loss: 3.3529\n",
      "Epoch [5/5], Step [2650/6250], Loss: 2.2043\n",
      "Epoch [5/5], Step [2660/6250], Loss: 2.6843\n",
      "Epoch [5/5], Step [2670/6250], Loss: 2.3424\n",
      "Epoch [5/5], Step [2680/6250], Loss: 2.3957\n",
      "Epoch [5/5], Step [2690/6250], Loss: 1.4050\n",
      "Epoch [5/5], Step [2700/6250], Loss: 2.8237\n",
      "Epoch [5/5], Step [2710/6250], Loss: 2.3369\n",
      "Epoch [5/5], Step [2720/6250], Loss: 3.4380\n",
      "Epoch [5/5], Step [2730/6250], Loss: 2.9107\n",
      "Epoch [5/5], Step [2740/6250], Loss: 2.8397\n",
      "Epoch [5/5], Step [2750/6250], Loss: 2.2699\n",
      "Epoch [5/5], Step [2760/6250], Loss: 1.4144\n",
      "Epoch [5/5], Step [2770/6250], Loss: 1.8883\n",
      "Epoch [5/5], Step [2780/6250], Loss: 2.2812\n",
      "Epoch [5/5], Step [2790/6250], Loss: 2.0521\n",
      "Epoch [5/5], Step [2800/6250], Loss: 2.6431\n",
      "Epoch [5/5], Step [2810/6250], Loss: 2.4068\n",
      "Epoch [5/5], Step [2820/6250], Loss: 2.0764\n",
      "Epoch [5/5], Step [2830/6250], Loss: 2.9582\n",
      "Epoch [5/5], Step [2840/6250], Loss: 2.4663\n",
      "Epoch [5/5], Step [2850/6250], Loss: 1.7158\n",
      "Epoch [5/5], Step [2860/6250], Loss: 2.2969\n",
      "Epoch [5/5], Step [2870/6250], Loss: 2.3163\n",
      "Epoch [5/5], Step [2880/6250], Loss: 1.7017\n",
      "Epoch [5/5], Step [2890/6250], Loss: 2.5586\n",
      "Epoch [5/5], Step [2900/6250], Loss: 2.7690\n",
      "Epoch [5/5], Step [2910/6250], Loss: 1.1373\n",
      "Epoch [5/5], Step [2920/6250], Loss: 2.2014\n",
      "Epoch [5/5], Step [2930/6250], Loss: 1.8051\n",
      "Epoch [5/5], Step [2940/6250], Loss: 1.4560\n",
      "Epoch [5/5], Step [2950/6250], Loss: 3.5988\n",
      "Epoch [5/5], Step [2960/6250], Loss: 3.0531\n",
      "Epoch [5/5], Step [2970/6250], Loss: 1.4748\n",
      "Epoch [5/5], Step [2980/6250], Loss: 2.2632\n",
      "Epoch [5/5], Step [2990/6250], Loss: 1.7825\n",
      "Epoch [5/5], Step [3000/6250], Loss: 1.2367\n",
      "Epoch [5/5], Step [3010/6250], Loss: 1.9627\n",
      "Epoch [5/5], Step [3020/6250], Loss: 1.7073\n",
      "Epoch [5/5], Step [3030/6250], Loss: 2.3742\n",
      "Epoch [5/5], Step [3040/6250], Loss: 2.5014\n",
      "Epoch [5/5], Step [3050/6250], Loss: 2.8874\n",
      "Epoch [5/5], Step [3060/6250], Loss: 1.7380\n",
      "Epoch [5/5], Step [3070/6250], Loss: 2.4514\n",
      "Epoch [5/5], Step [3080/6250], Loss: 1.8056\n",
      "Epoch [5/5], Step [3090/6250], Loss: 1.9488\n",
      "Epoch [5/5], Step [3100/6250], Loss: 2.4890\n",
      "Epoch [5/5], Step [3110/6250], Loss: 2.2110\n",
      "Epoch [5/5], Step [3120/6250], Loss: 2.4147\n",
      "Epoch [5/5], Step [3130/6250], Loss: 2.6708\n",
      "Epoch [5/5], Step [3140/6250], Loss: 2.3405\n",
      "Epoch [5/5], Step [3150/6250], Loss: 1.5801\n",
      "Epoch [5/5], Step [3160/6250], Loss: 1.9741\n",
      "Epoch [5/5], Step [3170/6250], Loss: 2.2425\n",
      "Epoch [5/5], Step [3180/6250], Loss: 2.2091\n",
      "Epoch [5/5], Step [3190/6250], Loss: 1.2067\n",
      "Epoch [5/5], Step [3200/6250], Loss: 2.7319\n",
      "Epoch [5/5], Step [3210/6250], Loss: 1.7284\n",
      "Epoch [5/5], Step [3220/6250], Loss: 2.2216\n",
      "Epoch [5/5], Step [3230/6250], Loss: 2.7145\n",
      "Epoch [5/5], Step [3240/6250], Loss: 3.1222\n",
      "Epoch [5/5], Step [3250/6250], Loss: 2.4901\n",
      "Epoch [5/5], Step [3260/6250], Loss: 1.2223\n",
      "Epoch [5/5], Step [3270/6250], Loss: 2.2239\n",
      "Epoch [5/5], Step [3280/6250], Loss: 1.8770\n",
      "Epoch [5/5], Step [3290/6250], Loss: 1.5528\n",
      "Epoch [5/5], Step [3300/6250], Loss: 2.2903\n",
      "Epoch [5/5], Step [3310/6250], Loss: 1.6625\n",
      "Epoch [5/5], Step [3320/6250], Loss: 2.5745\n",
      "Epoch [5/5], Step [3330/6250], Loss: 2.9396\n",
      "Epoch [5/5], Step [3340/6250], Loss: 2.6843\n",
      "Epoch [5/5], Step [3350/6250], Loss: 2.1498\n",
      "Epoch [5/5], Step [3360/6250], Loss: 2.4911\n",
      "Epoch [5/5], Step [3370/6250], Loss: 1.3504\n",
      "Epoch [5/5], Step [3380/6250], Loss: 3.9959\n",
      "Epoch [5/5], Step [3390/6250], Loss: 1.7143\n",
      "Epoch [5/5], Step [3400/6250], Loss: 2.6701\n",
      "Epoch [5/5], Step [3410/6250], Loss: 2.5386\n",
      "Epoch [5/5], Step [3420/6250], Loss: 1.2498\n",
      "Epoch [5/5], Step [3430/6250], Loss: 1.6737\n",
      "Epoch [5/5], Step [3440/6250], Loss: 1.6081\n",
      "Epoch [5/5], Step [3450/6250], Loss: 2.1655\n",
      "Epoch [5/5], Step [3460/6250], Loss: 2.8127\n",
      "Epoch [5/5], Step [3470/6250], Loss: 1.4014\n",
      "Epoch [5/5], Step [3480/6250], Loss: 3.1209\n",
      "Epoch [5/5], Step [3490/6250], Loss: 2.6514\n",
      "Epoch [5/5], Step [3500/6250], Loss: 2.7929\n",
      "Epoch [5/5], Step [3510/6250], Loss: 2.2268\n",
      "Epoch [5/5], Step [3520/6250], Loss: 2.1783\n",
      "Epoch [5/5], Step [3530/6250], Loss: 1.2977\n",
      "Epoch [5/5], Step [3540/6250], Loss: 2.3272\n",
      "Epoch [5/5], Step [3550/6250], Loss: 2.8555\n",
      "Epoch [5/5], Step [3560/6250], Loss: 1.4113\n",
      "Epoch [5/5], Step [3570/6250], Loss: 2.5861\n",
      "Epoch [5/5], Step [3580/6250], Loss: 2.1380\n",
      "Epoch [5/5], Step [3590/6250], Loss: 2.4834\n",
      "Epoch [5/5], Step [3600/6250], Loss: 1.9678\n",
      "Epoch [5/5], Step [3610/6250], Loss: 2.2497\n",
      "Epoch [5/5], Step [3620/6250], Loss: 1.6730\n",
      "Epoch [5/5], Step [3630/6250], Loss: 3.1215\n",
      "Epoch [5/5], Step [3640/6250], Loss: 1.5663\n",
      "Epoch [5/5], Step [3650/6250], Loss: 2.7388\n",
      "Epoch [5/5], Step [3660/6250], Loss: 2.0897\n",
      "Epoch [5/5], Step [3670/6250], Loss: 1.9173\n",
      "Epoch [5/5], Step [3680/6250], Loss: 1.8521\n",
      "Epoch [5/5], Step [3690/6250], Loss: 2.5501\n",
      "Epoch [5/5], Step [3700/6250], Loss: 1.9911\n",
      "Epoch [5/5], Step [3710/6250], Loss: 2.5388\n",
      "Epoch [5/5], Step [3720/6250], Loss: 2.6736\n",
      "Epoch [5/5], Step [3730/6250], Loss: 1.4255\n",
      "Epoch [5/5], Step [3740/6250], Loss: 1.8816\n",
      "Epoch [5/5], Step [3750/6250], Loss: 2.2928\n",
      "Epoch [5/5], Step [3760/6250], Loss: 1.7896\n",
      "Epoch [5/5], Step [3770/6250], Loss: 1.9958\n",
      "Epoch [5/5], Step [3780/6250], Loss: 2.2917\n",
      "Epoch [5/5], Step [3790/6250], Loss: 2.2438\n",
      "Epoch [5/5], Step [3800/6250], Loss: 3.2820\n",
      "Epoch [5/5], Step [3810/6250], Loss: 1.9328\n",
      "Epoch [5/5], Step [3820/6250], Loss: 3.0258\n",
      "Epoch [5/5], Step [3830/6250], Loss: 1.5436\n",
      "Epoch [5/5], Step [3840/6250], Loss: 2.5328\n",
      "Epoch [5/5], Step [3850/6250], Loss: 2.0535\n",
      "Epoch [5/5], Step [3860/6250], Loss: 2.8304\n",
      "Epoch [5/5], Step [3870/6250], Loss: 2.7318\n",
      "Epoch [5/5], Step [3880/6250], Loss: 2.2502\n",
      "Epoch [5/5], Step [3890/6250], Loss: 3.2960\n",
      "Epoch [5/5], Step [3900/6250], Loss: 2.1469\n",
      "Epoch [5/5], Step [3910/6250], Loss: 3.0676\n",
      "Epoch [5/5], Step [3920/6250], Loss: 1.8147\n",
      "Epoch [5/5], Step [3930/6250], Loss: 1.8017\n",
      "Epoch [5/5], Step [3940/6250], Loss: 1.8589\n",
      "Epoch [5/5], Step [3950/6250], Loss: 1.6632\n",
      "Epoch [5/5], Step [3960/6250], Loss: 1.2046\n",
      "Epoch [5/5], Step [3970/6250], Loss: 2.7357\n",
      "Epoch [5/5], Step [3980/6250], Loss: 2.2222\n",
      "Epoch [5/5], Step [3990/6250], Loss: 2.9720\n",
      "Epoch [5/5], Step [4000/6250], Loss: 1.8601\n",
      "Epoch [5/5], Step [4010/6250], Loss: 2.5119\n",
      "Epoch [5/5], Step [4020/6250], Loss: 3.2248\n",
      "Epoch [5/5], Step [4030/6250], Loss: 2.5032\n",
      "Epoch [5/5], Step [4040/6250], Loss: 2.7142\n",
      "Epoch [5/5], Step [4050/6250], Loss: 2.5977\n",
      "Epoch [5/5], Step [4060/6250], Loss: 3.1613\n",
      "Epoch [5/5], Step [4070/6250], Loss: 2.0224\n",
      "Epoch [5/5], Step [4080/6250], Loss: 1.3462\n",
      "Epoch [5/5], Step [4090/6250], Loss: 1.4100\n",
      "Epoch [5/5], Step [4100/6250], Loss: 2.8548\n",
      "Epoch [5/5], Step [4110/6250], Loss: 2.4010\n",
      "Epoch [5/5], Step [4120/6250], Loss: 3.2928\n",
      "Epoch [5/5], Step [4130/6250], Loss: 1.5170\n",
      "Epoch [5/5], Step [4140/6250], Loss: 2.1889\n",
      "Epoch [5/5], Step [4150/6250], Loss: 3.3456\n",
      "Epoch [5/5], Step [4160/6250], Loss: 1.5924\n",
      "Epoch [5/5], Step [4170/6250], Loss: 1.5750\n",
      "Epoch [5/5], Step [4180/6250], Loss: 1.7696\n",
      "Epoch [5/5], Step [4190/6250], Loss: 1.4454\n",
      "Epoch [5/5], Step [4200/6250], Loss: 2.3202\n",
      "Epoch [5/5], Step [4210/6250], Loss: 2.0812\n",
      "Epoch [5/5], Step [4220/6250], Loss: 1.4566\n",
      "Epoch [5/5], Step [4230/6250], Loss: 2.5509\n",
      "Epoch [5/5], Step [4240/6250], Loss: 2.0327\n",
      "Epoch [5/5], Step [4250/6250], Loss: 1.6886\n",
      "Epoch [5/5], Step [4260/6250], Loss: 2.4884\n",
      "Epoch [5/5], Step [4270/6250], Loss: 2.3631\n",
      "Epoch [5/5], Step [4280/6250], Loss: 1.9900\n",
      "Epoch [5/5], Step [4290/6250], Loss: 1.8365\n",
      "Epoch [5/5], Step [4300/6250], Loss: 2.5931\n",
      "Epoch [5/5], Step [4310/6250], Loss: 3.0096\n",
      "Epoch [5/5], Step [4320/6250], Loss: 2.5007\n",
      "Epoch [5/5], Step [4330/6250], Loss: 2.5328\n",
      "Epoch [5/5], Step [4340/6250], Loss: 2.6716\n",
      "Epoch [5/5], Step [4350/6250], Loss: 2.2247\n",
      "Epoch [5/5], Step [4360/6250], Loss: 2.3011\n",
      "Epoch [5/5], Step [4370/6250], Loss: 2.1880\n",
      "Epoch [5/5], Step [4380/6250], Loss: 2.2323\n",
      "Epoch [5/5], Step [4390/6250], Loss: 2.7038\n",
      "Epoch [5/5], Step [4400/6250], Loss: 2.2240\n",
      "Epoch [5/5], Step [4410/6250], Loss: 2.6160\n",
      "Epoch [5/5], Step [4420/6250], Loss: 2.7817\n",
      "Epoch [5/5], Step [4430/6250], Loss: 3.2310\n",
      "Epoch [5/5], Step [4440/6250], Loss: 1.4688\n",
      "Epoch [5/5], Step [4450/6250], Loss: 1.8952\n",
      "Epoch [5/5], Step [4460/6250], Loss: 2.2295\n",
      "Epoch [5/5], Step [4470/6250], Loss: 2.5187\n",
      "Epoch [5/5], Step [4480/6250], Loss: 1.7696\n",
      "Epoch [5/5], Step [4490/6250], Loss: 2.2428\n",
      "Epoch [5/5], Step [4500/6250], Loss: 1.7210\n",
      "Epoch [5/5], Step [4510/6250], Loss: 2.6508\n",
      "Epoch [5/5], Step [4520/6250], Loss: 2.1486\n",
      "Epoch [5/5], Step [4530/6250], Loss: 2.5297\n",
      "Epoch [5/5], Step [4540/6250], Loss: 2.2245\n",
      "Epoch [5/5], Step [4550/6250], Loss: 1.6676\n",
      "Epoch [5/5], Step [4560/6250], Loss: 2.2973\n",
      "Epoch [5/5], Step [4570/6250], Loss: 3.2279\n",
      "Epoch [5/5], Step [4580/6250], Loss: 1.8792\n",
      "Epoch [5/5], Step [4590/6250], Loss: 3.8192\n",
      "Epoch [5/5], Step [4600/6250], Loss: 1.7019\n",
      "Epoch [5/5], Step [4610/6250], Loss: 1.3442\n",
      "Epoch [5/5], Step [4620/6250], Loss: 2.9215\n",
      "Epoch [5/5], Step [4630/6250], Loss: 1.5592\n",
      "Epoch [5/5], Step [4640/6250], Loss: 1.6871\n",
      "Epoch [5/5], Step [4650/6250], Loss: 2.4693\n",
      "Epoch [5/5], Step [4660/6250], Loss: 1.4180\n",
      "Epoch [5/5], Step [4670/6250], Loss: 1.8669\n",
      "Epoch [5/5], Step [4680/6250], Loss: 2.6569\n",
      "Epoch [5/5], Step [4690/6250], Loss: 2.7883\n",
      "Epoch [5/5], Step [4700/6250], Loss: 2.5647\n",
      "Epoch [5/5], Step [4710/6250], Loss: 1.5215\n",
      "Epoch [5/5], Step [4720/6250], Loss: 1.0655\n",
      "Epoch [5/5], Step [4730/6250], Loss: 4.3223\n",
      "Epoch [5/5], Step [4740/6250], Loss: 2.0186\n",
      "Epoch [5/5], Step [4750/6250], Loss: 2.2675\n",
      "Epoch [5/5], Step [4760/6250], Loss: 1.3317\n",
      "Epoch [5/5], Step [4770/6250], Loss: 3.1881\n",
      "Epoch [5/5], Step [4780/6250], Loss: 1.8708\n",
      "Epoch [5/5], Step [4790/6250], Loss: 1.9920\n",
      "Epoch [5/5], Step [4800/6250], Loss: 2.0348\n",
      "Epoch [5/5], Step [4810/6250], Loss: 1.8907\n",
      "Epoch [5/5], Step [4820/6250], Loss: 2.4025\n",
      "Epoch [5/5], Step [4830/6250], Loss: 2.3599\n",
      "Epoch [5/5], Step [4840/6250], Loss: 3.0401\n",
      "Epoch [5/5], Step [4850/6250], Loss: 1.7585\n",
      "Epoch [5/5], Step [4860/6250], Loss: 2.2169\n",
      "Epoch [5/5], Step [4870/6250], Loss: 1.2219\n",
      "Epoch [5/5], Step [4880/6250], Loss: 1.3041\n",
      "Epoch [5/5], Step [4890/6250], Loss: 2.2634\n",
      "Epoch [5/5], Step [4900/6250], Loss: 1.8700\n",
      "Epoch [5/5], Step [4910/6250], Loss: 2.9828\n",
      "Epoch [5/5], Step [4920/6250], Loss: 1.8339\n",
      "Epoch [5/5], Step [4930/6250], Loss: 2.5791\n",
      "Epoch [5/5], Step [4940/6250], Loss: 1.8332\n",
      "Epoch [5/5], Step [4950/6250], Loss: 1.9603\n",
      "Epoch [5/5], Step [4960/6250], Loss: 2.0432\n",
      "Epoch [5/5], Step [4970/6250], Loss: 1.8909\n",
      "Epoch [5/5], Step [4980/6250], Loss: 1.8335\n",
      "Epoch [5/5], Step [4990/6250], Loss: 1.7162\n",
      "Epoch [5/5], Step [5000/6250], Loss: 1.2570\n",
      "Epoch [5/5], Step [5010/6250], Loss: 2.1728\n",
      "Epoch [5/5], Step [5020/6250], Loss: 2.3265\n",
      "Epoch [5/5], Step [5030/6250], Loss: 1.5883\n",
      "Epoch [5/5], Step [5040/6250], Loss: 3.4126\n",
      "Epoch [5/5], Step [5050/6250], Loss: 1.6851\n",
      "Epoch [5/5], Step [5060/6250], Loss: 1.2945\n",
      "Epoch [5/5], Step [5070/6250], Loss: 2.7449\n",
      "Epoch [5/5], Step [5080/6250], Loss: 2.1836\n",
      "Epoch [5/5], Step [5090/6250], Loss: 1.8431\n",
      "Epoch [5/5], Step [5100/6250], Loss: 2.1194\n",
      "Epoch [5/5], Step [5110/6250], Loss: 2.3452\n",
      "Epoch [5/5], Step [5120/6250], Loss: 1.5359\n",
      "Epoch [5/5], Step [5130/6250], Loss: 2.0970\n",
      "Epoch [5/5], Step [5140/6250], Loss: 1.5866\n",
      "Epoch [5/5], Step [5150/6250], Loss: 2.4911\n",
      "Epoch [5/5], Step [5160/6250], Loss: 1.4775\n",
      "Epoch [5/5], Step [5170/6250], Loss: 2.0804\n",
      "Epoch [5/5], Step [5180/6250], Loss: 2.1367\n",
      "Epoch [5/5], Step [5190/6250], Loss: 2.7865\n",
      "Epoch [5/5], Step [5200/6250], Loss: 2.0843\n",
      "Epoch [5/5], Step [5210/6250], Loss: 1.7109\n",
      "Epoch [5/5], Step [5220/6250], Loss: 1.1978\n",
      "Epoch [5/5], Step [5230/6250], Loss: 1.7479\n",
      "Epoch [5/5], Step [5240/6250], Loss: 3.8379\n",
      "Epoch [5/5], Step [5250/6250], Loss: 1.7698\n",
      "Epoch [5/5], Step [5260/6250], Loss: 2.4010\n",
      "Epoch [5/5], Step [5270/6250], Loss: 2.7717\n",
      "Epoch [5/5], Step [5280/6250], Loss: 4.2474\n",
      "Epoch [5/5], Step [5290/6250], Loss: 2.6885\n",
      "Epoch [5/5], Step [5300/6250], Loss: 3.3257\n",
      "Epoch [5/5], Step [5310/6250], Loss: 3.0196\n",
      "Epoch [5/5], Step [5320/6250], Loss: 2.9020\n",
      "Epoch [5/5], Step [5330/6250], Loss: 2.4437\n",
      "Epoch [5/5], Step [5340/6250], Loss: 2.5202\n",
      "Epoch [5/5], Step [5350/6250], Loss: 2.3549\n",
      "Epoch [5/5], Step [5360/6250], Loss: 1.1013\n",
      "Epoch [5/5], Step [5370/6250], Loss: 2.5482\n",
      "Epoch [5/5], Step [5380/6250], Loss: 2.1181\n",
      "Epoch [5/5], Step [5390/6250], Loss: 2.3237\n",
      "Epoch [5/5], Step [5400/6250], Loss: 3.2316\n",
      "Epoch [5/5], Step [5410/6250], Loss: 2.2749\n",
      "Epoch [5/5], Step [5420/6250], Loss: 3.2660\n",
      "Epoch [5/5], Step [5430/6250], Loss: 1.8540\n",
      "Epoch [5/5], Step [5440/6250], Loss: 1.7525\n",
      "Epoch [5/5], Step [5450/6250], Loss: 1.8140\n",
      "Epoch [5/5], Step [5460/6250], Loss: 2.3620\n",
      "Epoch [5/5], Step [5470/6250], Loss: 3.2584\n",
      "Epoch [5/5], Step [5480/6250], Loss: 3.0668\n",
      "Epoch [5/5], Step [5490/6250], Loss: 1.3691\n",
      "Epoch [5/5], Step [5500/6250], Loss: 2.3262\n",
      "Epoch [5/5], Step [5510/6250], Loss: 2.7375\n",
      "Epoch [5/5], Step [5520/6250], Loss: 1.0583\n",
      "Epoch [5/5], Step [5530/6250], Loss: 2.1259\n",
      "Epoch [5/5], Step [5540/6250], Loss: 2.4069\n",
      "Epoch [5/5], Step [5550/6250], Loss: 2.8948\n",
      "Epoch [5/5], Step [5560/6250], Loss: 2.6433\n",
      "Epoch [5/5], Step [5570/6250], Loss: 2.3519\n",
      "Epoch [5/5], Step [5580/6250], Loss: 1.8388\n",
      "Epoch [5/5], Step [5590/6250], Loss: 1.2568\n",
      "Epoch [5/5], Step [5600/6250], Loss: 3.7691\n",
      "Epoch [5/5], Step [5610/6250], Loss: 2.0182\n",
      "Epoch [5/5], Step [5620/6250], Loss: 3.1319\n",
      "Epoch [5/5], Step [5630/6250], Loss: 1.6135\n",
      "Epoch [5/5], Step [5640/6250], Loss: 2.4078\n",
      "Epoch [5/5], Step [5650/6250], Loss: 2.2080\n",
      "Epoch [5/5], Step [5660/6250], Loss: 2.3369\n",
      "Epoch [5/5], Step [5670/6250], Loss: 2.8418\n",
      "Epoch [5/5], Step [5680/6250], Loss: 2.5043\n",
      "Epoch [5/5], Step [5690/6250], Loss: 3.0801\n",
      "Epoch [5/5], Step [5700/6250], Loss: 3.3102\n",
      "Epoch [5/5], Step [5710/6250], Loss: 2.8303\n",
      "Epoch [5/5], Step [5720/6250], Loss: 2.1858\n",
      "Epoch [5/5], Step [5730/6250], Loss: 1.1391\n",
      "Epoch [5/5], Step [5740/6250], Loss: 2.0801\n",
      "Epoch [5/5], Step [5750/6250], Loss: 2.0798\n",
      "Epoch [5/5], Step [5760/6250], Loss: 1.9909\n",
      "Epoch [5/5], Step [5770/6250], Loss: 3.3983\n",
      "Epoch [5/5], Step [5780/6250], Loss: 2.5369\n",
      "Epoch [5/5], Step [5790/6250], Loss: 2.9316\n",
      "Epoch [5/5], Step [5800/6250], Loss: 1.5640\n",
      "Epoch [5/5], Step [5810/6250], Loss: 1.8507\n",
      "Epoch [5/5], Step [5820/6250], Loss: 3.5627\n",
      "Epoch [5/5], Step [5830/6250], Loss: 1.7015\n",
      "Epoch [5/5], Step [5840/6250], Loss: 3.6597\n",
      "Epoch [5/5], Step [5850/6250], Loss: 2.1298\n",
      "Epoch [5/5], Step [5860/6250], Loss: 1.4848\n",
      "Epoch [5/5], Step [5870/6250], Loss: 2.3190\n",
      "Epoch [5/5], Step [5880/6250], Loss: 2.3386\n",
      "Epoch [5/5], Step [5890/6250], Loss: 2.0550\n",
      "Epoch [5/5], Step [5900/6250], Loss: 2.2037\n",
      "Epoch [5/5], Step [5910/6250], Loss: 2.2086\n",
      "Epoch [5/5], Step [5920/6250], Loss: 2.6063\n",
      "Epoch [5/5], Step [5930/6250], Loss: 1.9303\n",
      "Epoch [5/5], Step [5940/6250], Loss: 2.7715\n",
      "Epoch [5/5], Step [5950/6250], Loss: 2.1610\n",
      "Epoch [5/5], Step [5960/6250], Loss: 1.5229\n",
      "Epoch [5/5], Step [5970/6250], Loss: 2.1546\n",
      "Epoch [5/5], Step [5980/6250], Loss: 2.1877\n",
      "Epoch [5/5], Step [5990/6250], Loss: 1.7370\n",
      "Epoch [5/5], Step [6000/6250], Loss: 1.7065\n",
      "Epoch [5/5], Step [6010/6250], Loss: 1.5499\n",
      "Epoch [5/5], Step [6020/6250], Loss: 1.4727\n",
      "Epoch [5/5], Step [6030/6250], Loss: 2.5666\n",
      "Epoch [5/5], Step [6040/6250], Loss: 2.6911\n",
      "Epoch [5/5], Step [6050/6250], Loss: 2.2161\n",
      "Epoch [5/5], Step [6060/6250], Loss: 2.4281\n",
      "Epoch [5/5], Step [6070/6250], Loss: 1.6351\n",
      "Epoch [5/5], Step [6080/6250], Loss: 2.3558\n",
      "Epoch [5/5], Step [6090/6250], Loss: 1.3535\n",
      "Epoch [5/5], Step [6100/6250], Loss: 2.8009\n",
      "Epoch [5/5], Step [6110/6250], Loss: 2.1395\n",
      "Epoch [5/5], Step [6120/6250], Loss: 2.7066\n",
      "Epoch [5/5], Step [6130/6250], Loss: 1.8155\n",
      "Epoch [5/5], Step [6140/6250], Loss: 1.4405\n",
      "Epoch [5/5], Step [6150/6250], Loss: 1.1956\n",
      "Epoch [5/5], Step [6160/6250], Loss: 3.0419\n",
      "Epoch [5/5], Step [6170/6250], Loss: 2.1551\n",
      "Epoch [5/5], Step [6180/6250], Loss: 1.6480\n",
      "Epoch [5/5], Step [6190/6250], Loss: 1.3033\n",
      "Epoch [5/5], Step [6200/6250], Loss: 2.9705\n",
      "Epoch [5/5], Step [6210/6250], Loss: 2.7578\n",
      "Epoch [5/5], Step [6220/6250], Loss: 2.1630\n",
      "Epoch [5/5], Step [6230/6250], Loss: 1.9908\n",
      "Epoch [5/5], Step [6240/6250], Loss: 2.4137\n",
      "Epoch [5/5], Step [6250/6250], Loss: 2.1211\n",
      "Epoch 5, Loss: 2.1211349964141846\n"
     ]
    }
   ],
   "source": [
    "# define the rnn model\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "# Hyperparameters\n",
    "# input size should be the number of items in the dataset + 1 (for padding) , so calculate it from the .txt file\n",
    "input_size = 0\n",
    "with open('sequential_recommendation_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        sequence = list(map(int, parts[1:]))\n",
    "        input_size = max(input_size, max(sequence))\n",
    "input_size += 1\n",
    "\n",
    "print(f'vocab size: {input_size}')\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = input_size\n",
    "num_epochs = 5\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Initialize the model, loss function, and optimizer and train the model on gpu\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (user_ids, input_seqs, target_seqs) in enumerate(dataloader):\n",
    "        \n",
    "        model.train()\n",
    "        input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "        outputs = model(input_seqs)\n",
    "        # print(outputs.shape, target_seqs.shape)\n",
    "        loss = criterion(outputs.view(-1, num_classes), target_seqs.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    \n",
    "    if loss.item() < 0.8:\n",
    "        break\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'rnnmodel.ckpt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this section, we will perform inference on our trained model. The goal is to predict the next sequence of items based on a given input item. This is a common scenario in recommendation systems where we want to predict what items a user might interact with next, based on their past interactions.\n",
    "\n",
    "The process will work as follows:\n",
    "\n",
    "1. We start by feeding the model an input item.\n",
    "2. The model will generate a prediction for the next item.\n",
    "3. We then take the model's prediction and use it as the new input, repeating the process.\n",
    "4. This is done iteratively, up to 5 times, to generate a sequence of recommended items.\n",
    "\n",
    "This method of using the model's own predictions as input for subsequent predictions is known as autoregression.\n",
    "\n",
    "Let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting item: 60888\n",
      "Generated sequence: [60888, 70708, 142678, 142679, 4297, 4297]\n"
     ]
    }
   ],
   "source": [
    "# Now lets do inference where i will give the model one item and it will predict the next sequence upto 5 items(feed the output of the model as input to the model again)\n",
    "\n",
    "# Load the model checkpoint\n",
    "model = RNNModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('rnnmodel.ckpt'))\n",
    "model.eval()\n",
    "\n",
    "# Inference\n",
    "\n",
    "# Choose a random item from the dataset\n",
    "import random\n",
    "item = random.randint(1, input_size-1)\n",
    "print('Starting item:', item)\n",
    "\n",
    "# Initialize the input sequence with the chosen item\n",
    "input_seq = torch.tensor([[item]]).to(device)\n",
    "\n",
    "# Generate the next 5 items in the sequence\n",
    "with torch.no_grad():\n",
    "    for _ in range(5):\n",
    "        output = model(input_seq)\n",
    "        _, predicted = torch.max(output[:, -1, :], 1)\n",
    "        input_seq = torch.cat((input_seq, predicted.unsqueeze(1)), dim=1)\n",
    "        \n",
    "print('Generated sequence:', input_seq.squeeze().tolist())\n",
    "\n",
    "torch.save(model.state_dict(), 'rnnmodel.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now create the transformer decoder architecture and train it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.5698025226593018\n",
      "Epoch 2, Loss: 1.735397458076477\n",
      "Epoch 3, Loss: 1.7853801250457764\n",
      "Epoch 4, Loss: 1.8887933492660522\n",
      "Epoch 5, Loss: 0.8825941681861877\n"
     ]
    }
   ],
   "source": [
    "# create the transformer deocder model which takes the inout sequence one by one and predicts the next item in the sequence and train the model on gpu use teacher forcing technique\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_items, embed_size, num_layers, num_heads, hidden_dim):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.item_embedding = nn.Embedding(num_items, embed_size)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(embed_size, num_heads, hidden_dim),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, num_items)\n",
    "        \n",
    "    def forward(self, input_seqs):\n",
    "        embeddings = self.item_embedding(input_seqs)\n",
    "        output = self.transformer_decoder(embeddings, embeddings)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "# Hyperparameters\n",
    "num_items = 0\n",
    "with open('sequential_recommendation_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        sequence = list(map(int, parts[1:]))\n",
    "        num_items = max(num_items, max(sequence))\n",
    "        \n",
    "num_items += 1\n",
    "embed_size = 128\n",
    "num_layers = 1\n",
    "num_heads = 2\n",
    "hidden_dim = 256\n",
    "num_epochs = 5\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Initialize the model, loss function, and optimizer and train the model on gpu\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerDecoder(num_items, embed_size, num_layers, num_heads, hidden_dim)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(dataloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (user_ids, input_seqs, target_seqs) in enumerate(dataloader):\n",
    "        \n",
    "        model.train()\n",
    "        input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "        outputs = model(input_seqs)\n",
    "        loss = criterion(outputs.view(-1, num_items), target_seqs.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    \n",
    "    if loss.item() < 0.8:\n",
    "        break\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'transformermodel.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting item: 138329\n",
      "Generated sequence: [138329, 939, 939, 939, 939, 939, 939, 939, 939, 939, 939]\n"
     ]
    }
   ],
   "source": [
    "# Now lets do inference where i will give the model one item and it will predict the next sequence upto 5 items(feed the output of the model as input to the model again)\n",
    "\n",
    "# Load the model checkpoint\n",
    "model = TransformerDecoder(num_items, embed_size, num_layers, num_heads, hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load('transformermodel.ckpt'))\n",
    "model.eval()\n",
    "\n",
    "# Inference\n",
    "\n",
    "# Choose a random item from the dataset\n",
    "import random\n",
    "item = random.randint(1, num_items-1)\n",
    "print('Starting item:', item)\n",
    "\n",
    "# Initialize the input sequence with the chosen item\n",
    "input_seq = torch.tensor([[item]]).to(device)\n",
    "\n",
    "# Generate the next 5 items in the sequence\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        output = model(input_seq)\n",
    "        _, predicted = torch.max(output[:, -1, :], 1)\n",
    "        input_seq = torch.cat((input_seq, predicted.unsqueeze(1)), dim=1)\n",
    "        \n",
    "print('Generated sequence:', input_seq.squeeze().tolist())\n",
    "\n",
    "#save the model checkpoint\n",
    "torch.save(model.state_dict(), 'transformermodel.ckpt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: NDCG Score Calculation\n",
    "\n",
    "In this section, we will evaluate the performance of our trained models - the Transformer model and the Recurrent Neural Network (RNN) model. The evaluation metric we will use is the Normalized Discounted Cumulative Gain (NDCG) score.\n",
    "\n",
    "NDCG is a popular metric for evaluating recommendation systems, as it takes into account both the relevance of the recommended items and their ranking. Higher NDCG scores indicate better performance of the model.\n",
    "\n",
    "## Transformer Model Evaluation\n",
    "\n",
    "First, we will calculate the NDCG score for the Transformer model. We will use the test dataset to generate recommendations and then compare these recommendations with the actual items the users interacted with.\n",
    "\n",
    "## RNN Model Evaluation\n",
    "\n",
    "Next, we will calculate the NDCG score for the RNN model, following the same process as with the Transformer model. \n",
    "\n",
    "By comparing the NDCG scores of the two models, we can determine which model performs better at recommending items that are relevant to the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG Score Calculation\n",
    "\n",
    "Normalized Discounted Cumulative Gain (NDCG) is a popular metric used for evaluating the performance of recommendation systems. It measures the performance of a recommendation system based on the relevance of recommended items and their ranking.\n",
    "\n",
    "The formula for DCG (Discounted Cumulative Gain) is:\n",
    "\n",
    "DCG@k = Σ (2^relevance[i] - 1) / log2(i + 1) for i in range(1, k+1)\n",
    "\n",
    "Where:\n",
    "- `relevance[i]` is the relevance of the item at position `i` in the recommended list. In the context of recommendation systems, this could be the rating of the item.\n",
    "- `log2(i + 1)` is a discount factor that reduces the contribution of items as their position in the list increases. The `+1` in the log and range functions is to account for the fact that list positions start at 1, not 0.\n",
    "\n",
    "The formula for NDCG is:\n",
    "\n",
    "NDCG@k = DCG@k / IDCG@k\n",
    "\n",
    "Where:\n",
    "- `DCG@k` is the DCG score for the recommended list.\n",
    "- `IDCG@k` is the DCG score for the ideal list (a perfectly ranked list).\n",
    "\n",
    "The NDCG score is a value between 0 and 1. A score of 1 means that the recommended list is perfectly ranked, while a score of 0 means the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(162625, 128)\n",
       "  (rnn): RNN(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=162625, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load rnn and transformer model\n",
    "\n",
    "transformer_model = TransformerDecoder(num_items, embed_size, num_layers, num_heads, hidden_dim).to(device)\n",
    "transformer_model.load_state_dict(torch.load('transformermodel.ckpt'))\n",
    "transformer_model.eval()\n",
    "\n",
    "rnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "rnn_model.load_state_dict(torch.load('rnnmodel.ckpt'))\n",
    "rnn_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for RNN model: 0.4012138349681588\n",
      "NDCG@5 for Transformer model: 0.4549966632593519\n",
      "NDCG@10 for RNN model: 0.4290205564705818\n",
      "NDCG@10 for Transformer model: 0.5484319116045691\n"
     ]
    }
   ],
   "source": [
    "#Load both the models and compare the results of ndcg@5 and ndcg@10 for both the models by calculating dcg and idcg and then calculating ndcg\n",
    "\n",
    "# Load the dataset\n",
    "dataset = SequentialRecommendationDataset('sequential_recommendation_data.txt')\n",
    "\n",
    "# calculate NDCG for the RNN model and Transformer model\n",
    "\n",
    "def calculate_ndcg(model, dataset, device, k):\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    for user_ids, input_seqs, target_seqs in dataloader:\n",
    "        input_seqs = input_seqs.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seqs)\n",
    "            for i in range(len(user_ids)):\n",
    "                user_id = user_ids[i]\n",
    "                target_seq = target_seqs[i]\n",
    "                predicted_seq = output[i].argmax(dim=1)\n",
    "                dcg += calculate_dcg(target_seq, predicted_seq, k)\n",
    "                idcg += calculate_dcg(target_seq, target_seq, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "def calculate_dcg(target_seq, predicted_seq, k):\n",
    "    target_seq = target_seq.cpu().numpy()\n",
    "    predicted_seq = predicted_seq.cpu().numpy()\n",
    "    dcg = 0\n",
    "    for i in range(min(k, len(target_seq))):\n",
    "        item = predicted_seq[i]\n",
    "        if item in target_seq:\n",
    "            rank = np.where(target_seq == item)[0][0]\n",
    "            dcg += 1 / np.log2(rank + 2)\n",
    "    return dcg\n",
    "\n",
    "import numpy as np\n",
    "k = 5\n",
    "ndcg_rnn = calculate_ndcg(rnn_model, dataset, device, k)\n",
    "ndcg_transformer = calculate_ndcg(transformer_model, dataset, device, k)\n",
    "\n",
    "print(f'NDCG@{k} for RNN model: {ndcg_rnn}')\n",
    "print(f'NDCG@{k} for Transformer model: {ndcg_transformer}')\n",
    "\n",
    "k = 10\n",
    "ndcg_rnn = calculate_ndcg(rnn_model, dataset, device, k)\n",
    "ndcg_transformer = calculate_ndcg(transformer_model, dataset, device, k)\n",
    "\n",
    "print(f'NDCG@{k} for RNN model: {ndcg_rnn}')\n",
    "print(f'NDCG@{k} for Transformer model: {ndcg_transformer}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are inside the data directory , where along woth this file there are 4 folders named beauty,sports,toys and yelp\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_pickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_text(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "    \n",
    "# I want to mention a data directory where my data is stored\n",
    "# data is in ../../Data/data directory\n",
    "data_dir = \"../../../Data/data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317516 19850 19850\n"
     ]
    }
   ],
   "source": [
    "# data_splits = load_pickle('../../Data/data/beauty/rating_splits_augmented.pkl')\n",
    "# test_review_data = data_splits['test']\n",
    "\n",
    "# data_splits = load_pickle('../../../Data/data/beauty/rating_splits_augmented.pkl')\n",
    "# use data_dir now\n",
    "data_splits = load_pickle(os.path.join(data_dir,'beauty/rating_splits_augmented.pkl'))\n",
    "train_data = data_splits['train']\n",
    "valid_data = data_splits['val']\n",
    "test_data = data_splits['test']\n",
    "\n",
    "print(len(train_data),len(valid_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'val', 'test', 'train_indices', 'val_indices', 'test_indices'])\n"
     ]
    }
   ],
   "source": [
    "print(data_splits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reviewerID': 'A3G6XNM240RMWA', 'asin': '7806397051', 'reviewerName': 'Karen', 'helpful': [0, 1], 'reviewText': \"The texture of this concealer pallet is fantastic, it has great coverage and a wide variety of uses, I guess it's meant for professional makeup artists and a lot of the colours are of no use to me but I use at least two of them on a regular basis, and two more occasionally, which is the only reason I'm giving it for stars, I feel like the range of colors is kind of a waste for me, but the  product itself  is wonderful, it's not cakey, gives me a natural for and concealed my imperfections, therefore I highly recommend it :)\", 'overall': 4.0, 'summary': 'great quality', 'unixReviewTime': 1378425600, 'reviewTime': '09 6, 2013', 'explanation': 'great quality', 'feature': 'quality'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22363 12101\n"
     ]
    }
   ],
   "source": [
    "# Now i want user item interaction matrix for the train data\n",
    "# I will create a user item interaction matrix for the train data\n",
    "# I will create a user item interaction matrix for the valid data\n",
    "# I will create a user item interaction matrix for the test data\n",
    "\n",
    "# {'reviewerID': 'A1YJEY40YUW4SE', 'asin': '7806397051', 'reviewerName': 'Andrea', 'helpful': [3, 4], 'reviewText': 'Very oily and creamy. Not at all what I expected... ordered this to try to highlight and contour and it just looked awful!!! Plus, took FOREVER to arrive.', 'overall': 1.0, 'summary': \"Don't waste your money\", 'unixReviewTime': 1391040000, 'reviewTime': '01 30, 2014'\n",
    "# The above is the format of the data\n",
    "# I will convert this data into user item interaction matrix\n",
    "def user2item(data):\n",
    "    user2item = {}\n",
    "    for d in data:\n",
    "        user = d['reviewerID']\n",
    "        item = d['asin']\n",
    "        if user not in user2item:\n",
    "            user2item[user] = []\n",
    "        user2item[user].append(item)\n",
    "    return user2item\n",
    "\n",
    "def item2user(data):\n",
    "    item2user = {}\n",
    "    for d in data:\n",
    "        user = d['reviewerID']\n",
    "        item = d['asin']\n",
    "        if item not in item2user:\n",
    "            item2user[item] = []\n",
    "        item2user[item].append(user)\n",
    "    return item2user\n",
    "\n",
    "user2item_train = user2item(train_data)\n",
    "item2user_train = item2user(train_data)\n",
    "\n",
    "user2item_valid = user2item(valid_data)\n",
    "item2user_valid = item2user(valid_data)\n",
    "\n",
    "user2item_test = user2item(test_data)\n",
    "item2user_test = item2user(test_data)\n",
    "\n",
    "print(len(user2item_train),len(item2user_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7806397051', 'B0020YLEYK', 'B002WLWX82', 'B004756YJA', 'B004ZT0SSG', '7806397051', '7806397051', '7806397051', '7806397051', '7806397051', '7806397051', 'B002WLWX82', 'B002WLWX82', 'B002WLWX82', 'B002WLWX82', 'B002WLWX82', 'B002WLWX82', 'B002WLWX82']\n"
     ]
    }
   ],
   "source": [
    "print(user2item_train['A1YJEY40YUW4SE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270614663 317516 270297147\n",
      "89512220 19850 89492370\n",
      "90230956 19850 90211106\n",
      "0.9988266858991303 0.9997782425684448 0.99978000898051\n"
     ]
    }
   ],
   "source": [
    "# Now calculate the sparisty of the user item interaction matrix\n",
    "# Sparsity = (Number of missing values)/(Total values)\n",
    "# Total values = Number of users * Number of items\n",
    "# Number of missing values = Total values - Number of present values\n",
    "\n",
    "def sparsity(user2item,item2user):\n",
    "    total_values = len(user2item)*len(item2user)\n",
    "    present_values = 0\n",
    "    for user in user2item:\n",
    "        present_values += len(user2item[user])\n",
    "    missing_values = total_values - present_values\n",
    "    print(total_values,present_values,missing_values)\n",
    "    sparsity = missing_values/total_values\n",
    "    return sparsity\n",
    "\n",
    "sparsity_train = sparsity(user2item_train,item2user_train)\n",
    "sparsity_valid = sparsity(user2item_valid,item2user_valid)\n",
    "sparsity_test = sparsity(user2item_test,item2user_test)\n",
    "print(sparsity_train,sparsity_valid,sparsity_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis about the minimum sequence length and maximum sequence length and average sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Seq Len: 0, Max Seq Len: 204, Avg Seq Len: 8.875961366481846\n"
     ]
    }
   ],
   "source": [
    "sequential_data = load_text(os.path.join(data_dir,'beauty/sequential_data.txt'))\n",
    "# print(sequential_data)\n",
    "\n",
    "# Now in every line first element is the user and the rest are the items\n",
    "# check the minimum sequence length and maximum sequence length\n",
    "# check the average sequence length\n",
    "min_seq_len = 100000\n",
    "max_seq_len = 0\n",
    "total_seq_len = 0\n",
    "count = 0\n",
    "for line in sequential_data.split('\\n'):\n",
    "    items = line.split(' ')\n",
    "    seq_len = len(items) - 1\n",
    "    if seq_len < min_seq_len:\n",
    "        min_seq_len = seq_len\n",
    "    if seq_len > max_seq_len:\n",
    "        max_seq_len = seq_len\n",
    "    total_seq_len += seq_len\n",
    "    count += 1\n",
    "    \n",
    "print(f\"Min Seq Len: {min_seq_len}, Max Seq Len: {max_seq_len}, Avg Seq Len: {total_seq_len/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
